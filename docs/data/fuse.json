{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Semantic Flow Documentation","n":0.577},"1":{"v":"\n**Dereferenceable, versioned semantic meshes** will be the foundation for a new era of knowledge graphs.\n\n[[now]] | [[todo]] | [[principle]] | [[dev.contributor.djradon.dev-log]]\n\n## What Is Semantic Flow?\n\n**Semantic Flow** is a framework for managing and publishing resource identifiers, knowledge graphs and other semantic data by leveraging GitHub, Gitlab, and other free static hosting services. It enables a **dereferenceable Semantic Web** where every HTTP IRI returns meaningful content.\n\n[[concept.single-mesh-repo]] provide storage, [[concept.mesh]] provide resource management and publishing, and [[concept.semantic-flow-site]] support data discovery and explainability.\n\n## Benefits\n\n- own your own self-describing data and data schemas\n- complete version history when you want it\n- reliable persistence\n- truly FAIR (Findable, Accessible, Interoperable, and Reusable)\n\n## Features\n\n- seamlessly integrate other data sources anywhere in your mesh\n- generate and customize mini-sites or single-page applications for nodes in your mesh\n- see [[feature]] for a list of planned features\n","n":0.086}}},{"i":2,"$":{"0":{"v":"Template","n":1}}},{"i":3,"$":{"0":{"v":"Task","n":1},"1":{"v":"\n## Decisions\n\n\n## Prompt\n\n\n## TODO\n\n\n","n":0.5}}},{"i":4,"$":{"0":{"v":"use cases","n":0.707}}},{"i":5,"$":{"0":{"v":"Use Case: Radio Show Websiste","n":0.447},"1":{"v":"\nSuppose DJ Radon wants to publish a website about himself, his musical activities, and maybe some album reviews. Call it a wiki, call it a knowledgebase, call it a show site. \n\nTo get started, he wants to publish his top-5 most-played tracks of the week. \n\nPublishing them as plain-text might be adequate for this situation, but say he eventually wanted to make his site linked: you can click from a playlist, to an album, to a track, to an artist. \n\nBut to get started:\n\n* A **namespace** `/ns/`\n* A **thing** `/ns/djradon/`\n* A **dataset** `/ns/djradon/picks/`\n\nSee [[Example Mesh|concept.mesh#example-mesh]] for a mapping of resources types\n\n#### Mesh Node Directory Structure\n\n```file\ntest-ns                    # bare node\n   djradon                 # ref node (refering to a human dj)\n      bio                  # payload node\n      picks                # payload node \n      underbrush           # ref node\n         playlists         # data (series) node\n            1996-11-10     # payload node\n            1996-11-17     # payload node\n```\n\n#### Sample RDF (Turtle)\n\n1. **Namespace metadata**\n\n   ```turtle\n   # /ns/_id/ns_id.trig\n   <> a sflo:BareNode ;\n      dct:title \"Namespace Root\" ;\n      sf:contains <https://example.org/ns/djradon/> .\n   ```\n\n2. **Thing metadata**\n\n   ```turtle\n   # /ns/djradon/_id/djradon_id.trig\n   <> a sf:Thing ;\n      rdfs:label \"djradon\" ;\n      sf:backlink <https://example.org/ns/> .\n   ```\n\n3. **Dataset metadata**\n\n   ```turtle\n   # /ns/djradon/picks/_id/picks_id.trig\n   <> a sf:VersionedDataset ;\n      dct:title \"djradon picks\" ;\n      sf:backlink <https://example.org/ns/djradon/> .\n   ```\n\n4. **Current distribution**\n\n   ```turtle\n   # /ns/djradon/picks/picks.trig\n   <> dct:issued \"2025-06-22\"^^xsd:date ;\n      dct:creator <https://example.org/agents/bot> .\n   ```\n\n5. **Historical version**\n\n   ```turtle\n   # /ns/djradon/picks/_v-series/v1/picks.jsonld\n   <> dct:issued \"2025-06-01\"^^xsd:date .\n   ```\n\n---\n\nWith these few rules and a tiny folder-walking parser your **Semantic Mesh** is unambiguously self-describing, easy to validate, and ready for any RDF-aware tooling.\n","n":0.064}}},{"i":6,"$":{"0":{"v":"Todo","n":1},"1":{"v":"\n# General Project TODO List\n\nThis list captures general tasks, ideas, and items that have not yet been broken down into formal, actionable tasks (i.e., `tasks.YYYY-MM-DD-task-name.md`).\n\n## High Priority\n\n- [ ] set up a store adapter, store implementations, query catalog, and web UI\n- [ ] Implement the core Weave Process logic (versioning, promotion, link resolution).\n- [ ] Define and implement the two-flow configuration inheritance model.\n- [ ] Create initial unit and integration tests for `sflo-host`.\n- [ ] Design sflo-web server-rendered architecture (Fastify view engine + HTMX partials)\n- [ ] Logging Phase 2 (+ sentry mcp server?)\n- It seems like, similar to how Flows can have a weaveLabel for \"last woven\", Flows could have a sequenceNumber for \"last woven\"\n\n\n## Grooming / Future Tasks\n\n- [ ] Define a \"Documentation Grooming\" Roo Mode to periodically refine documentation for pithiness and consistency.\n- [ ] Implement the `sflo-web` plugin for a basic web UI.\n\n## Agent Maintenance Tasks\n\n- [ ] Ensure all memory bank files are consistently updated as work progresses.\n- [ ] Review and update [[dev.patterns]] as new architectural decisions are made.\n- [ ] Review and update [[dev.dependencies]] when major dependencies are added or removed.\n\n\n","n":0.073}}},{"i":7,"$":{"0":{"v":"Tasks","n":1}}},{"i":8,"$":{"0":{"v":"2025 11 28_refine Createnode","n":0.5},"1":{"v":"\n\n```md\n\n## Prompt\n\nWe already have an architecture draft in `architecture-plan-rdfsource-createnode.md`. Use it as a **structural reference**, but treat the rules below as **authoritative** where there is any conflict.\n\nThe goal of this task is to:\n\n1. Refine `createNode` so that it:\n   - scaffolds a mesh node’s directory structure,\n   - writes **_working** shots (where inputs are provided),\n   - does **not** write any snapshots (v1, _default) for any flow.\n2. Ensure `_meta` exists but contains **no RDF**; metadata snapshots are created only by later **weave** operations.\n3. Create default `index.html` resource pages for:\n   - the node itself,\n   - each flow directory that is created,\n   - and (optionally) each shot directory, using a platform default template.\n4. Make provenance defaults come exclusively from `_cfg-op` (operational config), not from a separate provenance bundle.\n5. Preserve the existing RdfSource / parsing semantics direction (file:/// base, mesh-native JSON-LD).\n\nThis sets us up so that:\n\n- `createNode` establishes a *reviewable, un-woven node* with optional `_working` data and HTML resource pages.\n- The first actual weave can be a **regular weave**—no special “initialization weave” concept is needed.\n\n---\n\n## Authoritative behavior for createNode\n\n### 1. Required vs optional flows\n\nFor a newly created node:\n\n- `_meta/` is **mandatory** (always created).\n- All other flow directories are **optional**:\n  - `_ref/`\n  - `_payload/`\n  - `_cfg-op/`\n  - `_cfg-inh/`\n\n`createNode`:\n\n- MUST always create `_meta/` with an empty `_meta/_default/` directory (no RDF content).\n- MUST NOT create `_meta/_working` at all.\n- MAY create other flow directories only when needed (inputs provided, or design choice to always scaffold them).\n\n### 2. Snapshots vs working shots\n\nGlobal rule to enforce:\n\n- **“Weaves write snapshots.”**\n- `createNode` is **not** a weave; it performs no snapshots.\n\nConcretely:\n\n- `createNode` MUST NOT write:\n  - any `v1` directories,\n  - any `_default` distributions for any flow,\n  - any provenance activities in `_meta`.\n\nInstead:\n\n- For any flow where input is provided (reference/payload/config), `createNode`:\n  - MUST create a `_working/` shot directory for that flow,\n  - MUST write a **mesh-native JSON-LD distribution** there,\n  - MUST use the filename conventions below.\n\n### 3. Flow dirs, snaps, filenames (Option 1 + exact flow slugs)\n\nFlow directory slugs (fixed):\n\n- `_meta`\n- `_ref`\n- `_payload`\n- `_cfg-op`\n- `_cfg-inh`\n\nNode slug:\n\n- Derived from the node folder name.\n\nFile naming:\n\n- Payload:\n  - `_payload/_working/<nodeSlug>.jsonld`\n  - (later, weaves will write `_payload/vN/<nodeSlug>.jsonld` and `_payload/_default/<nodeSlug>.jsonld`)\n- Other flows:\n  - `_ref/_working/<nodeSlug>_ref.jsonld`\n  - `_cfg-op/_working/<nodeSlug>_cfg-op.jsonld`\n  - `_cfg-inh/_working/<nodeSlug>_cfg-inh.jsonld`\n- `_meta`:\n  - For **this task**, `createNode` **does not** write any JSON-LD inside `_meta` at all.\n  - `_meta/_default/` is created as an empty directory; snapshots will be created by weaves later.\n\nNo `_meta/_working` directory.\n\n### 4. RdfSource and parsing semantics (must keep)\n\nContinue to implement/extend the previously agreed model:\n\n- `RdfSource` discriminated union:\n  - `kind: 'file' | 'url' | 'inline'` with `syntax` hint.\n- Parsing behavior:\n  - For `kind: 'file'`:\n    - `documentIri = file:///absolute/path/to/file`\n    - parse with `baseIri = documentIri`.\n  - For `kind: 'url'`:\n    - `documentIri = url`.\n  - For `kind: 'inline'`:\n    - use a synthetic `file:///virtual/...` base (document in comments).\n- Mesh-native JSON-LD serialization:\n  - no `@base` in context,\n  - local IRIs relative to the chosen target base,\n  - external IRIs absolute.\n\n`createNode` must use `RdfSource` + debasing logic to populate `_working` distributions when inputs are provided.\n\n### 5. Provenance defaults: only via operational config\n\nRemove or ignore `ProvenanceBundleInput` in `createNode`:\n\n- `createNode` options should look like:\n\n  ```ts\n  export interface CreateNodeOptions {\n    payloadDataset?: RdfSource;\n    referenceDataset?: RdfSource;\n    operationalConfig?: RdfSource;\n    inheritableConfig?: RdfSource;\n\n    allowNonEmpty?: boolean;\n\n    // Optional: node README content or path (see below)\n    readme?: string;      // inline markdown\n    readmePath?: string;  // file path for markdown\n  }\n````\n\n* If `operationalConfig` is provided:\n\n  * Parse it as RDF via `RdfSource` and write `_cfg-op/_working/<nodeSlug>_cfg-op.jsonld` in mesh-native form.\n  * This config MAY contain “provenance defaults” (rightsHolder, license, default agents, etc.), but **no actual weave events**.\n* Actual weave events and PROV activities will be created later by a generic weave operation consuming `_cfg-op`, `_working` datasets, etc.\n\n### 6. README and starter kits\n\nFor this task, support at least:\n\n* Optional README content for the node itself:\n\n  * `readme` (inline markdown) or `readmePath` (file).\n* Do **not** treat README as RDF; it’s normal content to be integrated into the node’s resource page (index.html).\n* You do *not* need to implement zipped starter kits now; just keep the architecture flexible enough that `createNode` could later accept a “starter node bundle” and unpack it before applying additional config.\n\n---\n\n## Resource pages (index.html) behavior\n\nWe want dereferenceability even before the first weave.\n\n### 7. index.html generation\n\n`createNode` should generate basic `index.html` files using a “platform default” template for:\n\n* The node root folder:\n\n  * `<nodePath>/index.html`\n* Each created flow directory:\n\n  * `<nodePath>/_meta/index.html`\n  * `<nodePath>/_payload/index.html` (if created)\n  * `<nodePath>/_ref/index.html` (if created)\n  * etc.\n* Optionally, each shot directory created at this stage:\n\n  * `<nodePath>/_payload/_working/index.html`\n  * `<nodePath>/_cfg-op/_working/index.html`\n  * etc.\n\nTemplate requirements (minimal for now):\n\n* Node `index.html`:\n\n  * Mentions the node slug.\n  * Links to any existing flow directories as “NamingResources” / “FileResources” (no need for full ontology terms, just human-readable for now).\n  * Optionally renders README content if provided (simple markdown → HTML is enough; can be naive).\n\n* Flow `index.html`:\n\n  * Identifies the flow by slug (`_payload`, `_ref`, `_cfg-op`, etc.).\n  * Links to any `_working` distributions present (plain file links).\n\nThese templates can be simple functions or static HTML skeletons with minimal logic; the goal is just to ensure:\n\n* every directory is dereferenceable over HTTP,\n* there’s enough structure that future weaves can build richer resource pages without breaking anything.\n\n---\n\n## What needs to be changed vs the original architecture plan\n\nWhen you implement this task, please:\n\n1. **Remove** snapshot generation from `createNode`:\n\n   * No `_meta/v1`, no `_meta/_default` distributions.\n   * No `v1` snapshots for `_payload`, `_ref`, `_cfg-*`.\n2. **Ensure `_meta` is mandatory**, but only the directory and `_default/` subdir exist; no RDF.\n3. **Add `_working` shots** for flows where inputs are provided, using the naming rules above.\n4. **Drop `ProvenanceBundleInput`** from `createNode`; provenance defaults belong in `_cfg-op/_working`.\n5. **Add support for optional Node README** (inline or from file) at `createNode` time, store it as nodename/README.md\n6. **Generate default `index.html`** in node root (using README.md if present, and linking to created flow and FlowShot dirs).\n\nYou can either refactor the existing `createNode` implementation/plan or restart it with these rules; given the amount of change, a **surgical refactor** is probably reasonable:\n\n* Keep file locations, error types, and basic directory scaffolding logic.\n* Remove snapshot-writing logic and provenance bundles.\n* Insert `_working` shot writing + `index.html` generation.\n\n---\n\n## Success criteria\n\nThis task is complete when:\n\n1. `createNode`:\n\n   * Creates `_meta/_default/` as an empty directory.\n   * Creates other flow dirs only as needed.\n   * Writes `_working` distributions (mesh-native JSON-LD) for any flows where `RdfSource` inputs are provided.\n   * Does not write any snapshots (`v1`, `_default` distributions) or metadata RDF.\n2. Optional `operationalConfig` is written to `_cfg-op/_working/<nodeSlug>_cfg-op.jsonld` and not used to generate any PROV events yet.\n3. Optional README is accepted and integrated into the node’s `index.html` page.\n4. `index.html` exists in:\n\n   * node root,\n   * `_meta/`,\n   * each non-empty flow dir,\n   * and each `_working` shot dir created by `createNode`.\n5. All affected tests are updated or added to reflect:\n\n   * `_meta` is mandatory but RDF-free after `createNode`,\n   * no snapshots exist after `createNode`,\n   * `_working` distributions appear correctly when inputs are provided,\n   * `index.html` gets created in the expected locations.\n\n```\n","n":0.03}}},{"i":9,"$":{"0":{"v":"2025 11 28 Rdfsource Debasing Parsing","n":0.408},"1":{"v":"\n## TODO\n\n- [x] Explore project structure and understand requirements\n- [x] Design RdfSource abstraction architecture\n- [x] Design createNode operation architecture\n- [ ] Document parsing semantics for SFLO spec\n- [ ] Implement RdfSource types and core abstractions\n- [ ] Implement RdfSource parsing/serialization helpers\n- [ ] Implement createNode core function\n- [ ] Implement filesystem scaffolding utilities\n- [ ] Implement metadata generation with provenance stubs\n- [ ] Create tests for RdfSource\n- [ ] Create tests for createNode\n- [ ] Create simple Node.js runner script\n- [ ] Update task files with progress\n\n## Prompt\n\nImplement and document a coherent **RDF loading and serialization model** for Semantic Flow, then **integrate it into `createNode`**.\n\nThis task has four tightly related parts:\n\n1. Define a **code-level `RdfSource` abstraction** and helpers.\n2. Write/update the **“Parsing Semantics”** sections of the SFLO spec (check [[concept.identifier.intramesh.relative]], [[concept.iri]], [[concept.implied-rdf-base]])\n3. Formalize the **debasing/rebasing algorithm** (in [[concept.debasing]]) for adopting external datasets into mesh-native form.\n4. Integrate all of the above into the  **`createNode`** task ([[task.2025-11-27-createNode]]), replacing any ad-hoc path/string handling.\n\nThe goal is to make RDF ingestion/emission:\n\n- explicit,\n- testable,\n- compatible with RDF/JS and JSON-LD tooling,\n- and aligned with the “relative, base-free JSON-LD on disk” rule.\n\n---\n\n## Context & Decisions (must be respected)\n\n### 1. Mesh-native JSON-LD style (file format)\n\nMesh-authored RDF files (the ones Semantic Flow writes into the node tree) MUST follow:\n\n- **No `@base`** in JSON-LD.\n- **Local identifiers are relative IRIs**:\n  - e.g. `\"@id\": \"foo\"`, `\"@id\": \"payload/v1\"`.\n- **External identifiers are absolute IRIs**:\n  - vocabularies, external links, etc.\n\nThese files must be “publishable anywhere” as-is:\n- When loaded from `file:///…`, the base is file path.\n- When served from HTTP (e.g. GitHub Pages), the base is the HTTP URL.\n\nNo additional rewrite step is required at publish time.\n\n### 2. Internal base semantics (parsing)\n\nInternally, RDF/JSON-LD tooling always works with **absolute IRIs**.\n\nFor **any mesh-native file read from disk**:\n\n- Compute **document IRI** as a `file:///` URL from the absolute path.\n- Always parse with **`baseIRI = file:///…`**:\n  - JSON-LD: `jsonld.expand(..., { base: documentIri })` or equivalent.\n  - Turtle/TriG: N3/rdf-parse with `baseIRI: documentIri`.\n\nThere is **no “base-less” in memory**; “base-free” is only a file-level serialization convention.\n\nFor external datasets (URL / file):\n\n- If they have an explicit base or document IRI (HTTP/file), use that as base.\n- If they have relative IRIs but no usable base, treat as an error or require explicit base (no silent guessing).\n\n### 3. Flow + filename conventions relevant to this task\n\nWe are going with **Option 1**:\n\n- **Flow directory slugs** (core flows, short names):\n  - `_meta`    → node metadata flow (old `_node-metadata-flow`)\n  - `_ref`     → reference flow\n  - `_payload` → payload/data flow\n  - `_cfg-op`  → operational config flow\n  - `_cfg-inh` → inheritable config flow\n\n- **Node slug**:\n  - Derived from the node folder name.\n  - Used to construct filenames.\n\n- **Distribution filenames**:\n\n  - Payload: `<nodeSlug>.jsonld` (no `_payload` in filename).\n  - Others: `<nodeSlug>-<flowShort>.jsonld`, e.g.:\n    - metadata: `<nodeSlug>_meta.jsonld`\n    - reference: `<nodeSlug>_ref.jsonld`\n    - cfg-op: `<nodeSlug>_cfg-op.jsonld`\n    - cfg-inh: `<nodeSlug>_cfg-inh.jsonld`\n\nExample for node at `/ontology/semantic-flow-ontology/`:\n\n```text\n/ontology/semantic-flow-ontology/\n  _payload/_default/semantic-flow-ontology.jsonld\n  _meta/_default/semantic-flow-ontology_meta.jsonld\n  _ref/_default/semantic-flow-ontology_ref.jsonld\n","n":0.046}}},{"i":10,"$":{"0":{"v":"Architecture Plan - RdfSource & createNode","n":0.408},"1":{"v":"\n# Architecture Plan: RdfSource Abstraction & createNode Operation\n\n## Overview\n\nThis document provides the architectural design for implementing:\n1. **RdfSource abstraction** - A coherent model for RDF loading/serialization\n2. **createNode operation** - Core mesh node initialization using RdfSource\n\n## 1. RdfSource Abstraction Design\n\n### 1.1 Core Principles\n\n- **Explicit base handling**: Always require a base IRI for parsing\n- **Mesh-native output**: Serialize with relative IRIs, no `@base`\n- **Format-agnostic**: Support JSON-LD (primary) and Turtle/TriG (future)\n- **Testable & composable**: Small, focused functions that compose well\n\n### 1.2 Type Definitions\n\nLocation: [`shared/core/src/rdf/types.ts`](../../shared/core/src/rdf/types.ts)\n\n```typescript\n/**\n * Source of RDF data - file path, URL, or in-memory content\n */\nexport type RdfSourceInput =\n  | { type: \"file\"; path: string }\n  | { type: \"url\"; url: string }\n  | { type: \"content\"; content: string; format: RdfFormat };\n\n/**\n * Supported RDF serialization formats\n */\nexport type RdfFormat = \"jsonld\" | \"turtle\" | \"trig\" | \"ntriples\" | \"nquads\";\n\n/**\n * Options for parsing RDF\n */\nexport interface RdfParseOptions {\n  /** Base IRI for resolving relative references (REQUIRED) */\n  baseIri: string;\n  /** Expected format (auto-detect if not specified) */\n  format?: RdfFormat;\n}\n\n/**\n * Options for serializing RDF\n */\nexport interface RdfSerializeOptions {\n  /** Output format */\n  format: RdfFormat;\n  /** Whether to include @base in output (default: false for mesh-native) */\n  includeBase?: boolean;\n  /** Whether to pretty-print (default: true) */\n  prettyPrint?: boolean;\n}\n\n/**\n * Metadata about the RDF source\n */\nexport interface RdfSourceMetadata {\n  /** Detected or provided base IRI */\n  baseIri: string;\n  /** Detected format */\n  format: RdfFormat;\n  /** Original source location (file path or URL) */\n  sourceLocation?: string;\n}\n\n/**\n * Loaded RDF dataset with metadata\n */\nexport interface RdfDataset {\n  /** The RDF quads (using @rdfjs/types) */\n  quads: Quad[];\n  /** Metadata about the source */\n  metadata: RdfSourceMetadata;\n}\n```\n\n### 1.3 Core Functions\n\nLocation: [`shared/core/src/rdf/source.ts`](../../shared/core/src/rdf/source.ts)\n\n```typescript\n/**\n * Parse RDF from a source into quads\n * \n * @param source - RDF source input\n * @param options - Parse options (baseIri REQUIRED)\n * @returns Loaded dataset with quads and metadata\n * \n * See: concept.implied-rdf-base, concept.identifier.intramesh.relative\n */\nexport async function parseRdf(\n  source: RdfSourceInput,\n  options: RdfParseOptions\n): Promise<RdfDataset>;\n\n/**\n * Serialize RDF quads to a string\n * \n * Mesh-native mode (default):\n * - No @base in output\n * - Local IRIs become relative IRIs\n * \n * @param dataset - RDF dataset to serialize\n * @param options - Serialization options\n * @returns Serialized RDF string\n * \n * See: concept.identifier.intramesh.relative\n */\nexport async function serializeRdf(\n  dataset: RdfDataset,\n  options: RdfSerializeOptions\n): Promise<string>;\n\n/**\n * Compute file:/// base IRI from absolute file path\n * \n * @param absolutePath - Absolute filesystem path\n * @returns file:/// URL suitable as RDF base\n */\nexport function filePathToBaseIri(absolutePath: string): string;\n\n/**\n * Extract base IRI from RDF content (if present)\n * \n * @param content - RDF content string\n * @param format - RDF format\n * @returns Detected base IRI or undefined\n */\nexport async function detectBaseIri(\n  content: string,\n  format: RdfFormat\n): Promise<string | undefined>;\n```\n\n### 1.4 Helper Functions\n\nLocation: [`shared/core/src/rdf/helpers.ts`](../../shared/core/src/rdf/helpers.ts)\n\n```typescript\n/**\n * Make IRIs relative to a base\n * \n * Used for creating mesh-native files from external datasets\n * \n * @param quads - RDF quads with absolute IRIs\n * @param baseIri - Base to make relative to\n * @returns Quads with relative IRIs where applicable\n * \n * See: concept.debasing\n */\nexport function makeIrisRelative(\n  quads: Quad[],\n  baseIri: string\n): Quad[];\n\n/**\n * Detect RDF format from file extension or content\n * \n * @param pathOrContent - File path or content string\n * @returns Detected format\n */\nexport function detectFormat(pathOrContent: string): RdfFormat;\n\n/**\n * Validate that a base IRI is suitable for mesh use\n * \n * @param baseIri - IRI to validate\n * @throws if invalid\n */\nexport function validateMeshBaseIri(baseIri: string): void;\n```\n\n### 1.5 Implementation Notes\n\n- Use `@rdfjs/types` for Quad interface\n- Use `jsonld` library for JSON-LD parsing/serialization\n- Use `n3` library for Turtle/TriG parsing/serialization\n- Use `@rdfjs/data-model` for creating quads\n- Handle errors gracefully with custom error types\n- All functions are pure and stateless\n\n### 1.6 Testing Strategy\n\nLocation: [`shared/core/src/rdf/__tests__/`](../../shared/core/src/rdf/__tests__/)\n\nTest files:\n- `source.test.ts` - Core parsing/serialization\n- `helpers.test.ts` - Helper functions\n- `integration.test.ts` - End-to-end workflows\n\nTest cases:\n- Parse JSON-LD with explicit base\n- Parse Turtle with base declaration\n- Serialize mesh-native JSON-LD (no @base, relative IRIs)\n- Round-trip: parse → serialize → parse\n- Error handling: missing base, invalid format\n- File path to base IRI conversion\n- Base IRI detection\n\n## 2. createNode Operation Design\n\n### 2.1 Core Function Signature\n\nLocation: [`shared/core/src/operations/create-node.ts`](../../shared/core/src/operations/create-node.ts)\n\n```typescript\n/**\n * Options for creating a new mesh node\n */\nexport interface CreateNodeOptions {\n  /** Path to payload dataset (optional) */\n  payloadDatasetPath?: string;\n  \n  /** Path to reference dataset (optional) */\n  referenceDatasetPath?: string;\n  \n  /** Path to operational config dataset (optional) */\n  operationalConfigPath?: string;\n  \n  /** Path to inheritable config dataset (optional) */\n  inheritableConfigPath?: string;\n  \n  /** Provenance information (optional) */\n  provenanceInput?: ProvenanceBundleInput;\n  \n  /** Allow creating in non-empty directory (default: false) */\n  allowNonEmpty?: boolean;\n  \n  /** Namespace root path for computing base IRIs */\n  namespaceRoot?: string;\n}\n\n/**\n * Provenance information for node creation\n */\nexport interface ProvenanceBundleInput {\n  /** Primary agent IRI (human or system) */\n  primaryAgentIri?: string;\n  \n  /** Organization IRI (rights holder org) */\n  organizationIri?: string;\n  \n  /** Additional contributor IRIs */\n  contributorsIri?: string[];\n  \n  /** Rights holder IRI */\n  rightsHolderIri?: string;\n  \n  /** License IRI (e.g., CC BY-SA URL) */\n  licenseIri?: string;\n}\n\n/**\n * Result of node creation\n */\nexport interface CreateNodeResult {\n  /** Absolute path to created node */\n  nodePath: string;\n  \n  /** Node slug (derived from folder name) */\n  nodeSlug: string;\n  \n  /** Base IRI for the node */\n  nodeBaseIri: string;\n  \n  /** Created flow paths */\n  flows: {\n    meta: string;\n    ref?: string;\n    payload?: string;\n    cfgOp?: string;\n    cfgInh?: string;\n  };\n}\n\n/**\n * Create a new mesh node\n * \n * Steps:\n * 1. Validate path (not already a node, handle allowNonEmpty)\n * 2. Create node directory structure\n * 3. Create _node-handle/ stub\n * 4. Create _meta/ v1 and _default/ with minimal metadata\n * 5. Optionally import reference/payload/config datasets\n * 6. Return result summary\n * \n * @param nodeTargetPath - Path where node should be created\n * @param options - Creation options\n * @returns Creation result\n * @throws {NodeAlreadyExistsError} if node already initialized\n * @throws {DirectoryNotEmptyError} if directory not empty and !allowNonEmpty\n * \n * See: task.2025-11-27-createNode, concept.weave-process\n */\nexport async function createNode(\n  nodeTargetPath: string,\n  options?: CreateNodeOptions\n): Promise<CreateNodeResult>;\n```\n\n### 2.2 Internal Functions\n\n```typescript\n/**\n * Validate that path is suitable for node creation\n */\nasync function validateNodePath(\n  path: string,\n  allowNonEmpty: boolean\n): Promise<void>;\n\n/**\n * Create directory structure for a node\n */\nasync function createNodeScaffolding(\n  nodePath: string,\n  nodeSlug: string\n): Promise<void>;\n\n/**\n * Generate minimal v1 metadata for a new node\n */\nfunction generateNodeMetadata(\n  nodeSlug: string,\n  nodeBaseIri: string,\n  provenance?: ProvenanceBundleInput\n): RdfDataset;\n\n/**\n * Import and normalize a dataset into a flow\n */\nasync function importDatasetToFlow(\n  sourcePath: string,\n  targetFlowPath: string,\n  nodeSlug: string,\n  flowSlug: string,\n  nodeBaseIri: string\n): Promise<void>;\n\n/**\n * Derive node slug from folder name\n */\nfunction deriveNodeSlug(nodePath: string): string;\n\n/**\n * Compute node base IRI from path and namespace root\n */\nfunction computeNodeBaseIri(\n  nodePath: string,\n  namespaceRoot?: string\n): string;\n```\n\n### 2.3 Flow Slug Constants\n\nLocation: [`shared/core/src/constants/flows.ts`](../../shared/core/src/constants/flows.ts)\n\n```typescript\n/**\n * Flow slug constants (from semantic-flow ontology)\n * \n * See: ontology/semantic-flow/_payload-flow/_working/semantic-flow-ontology.jsonld\n */\nexport const FLOW_SLUGS = {\n  META: \"_meta\",\n  REF: \"_ref\",\n  PAYLOAD: \"_payload\",\n  CFG_OP: \"_cfg-op\",\n  CFG_INH: \"_cfg-inh\",\n} as const;\n\n/**\n * Snapshot folder names\n */\nexport const SNAPSHOT_FOLDERS = {\n  WORKING: \"_working\",\n  DEFAULT: \"_default\",\n} as const;\n\n/**\n * System folder names\n */\nexport const SYSTEM_FOLDERS = {\n  HANDLE: \"_node-handle\",\n} as const;\n```\n\n### 2.4 Filename Generation\n\n```typescript\n/**\n * Generate distribution filename for a flow\n * \n * Payload: <nodeSlug>.jsonld\n * Others: <nodeSlug>_<flowSlug>.jsonld\n * \n * @param nodeSlug - Node slug\n * @param flowSlug - Flow slug (undefined for payload)\n * @returns Filename\n */\nfunction generateDistributionFilename(\n  nodeSlug: string,\n  flowSlug?: string\n): string {\n  if (!flowSlug || flowSlug === FLOW_SLUGS.PAYLOAD) {\n    return `${nodeSlug}.jsonld`;\n  }\n  return `${nodeSlug}${flowSlug}.jsonld`;\n}\n```\n\n### 2.5 Metadata Generation Strategy\n\nThe v1 metadata will include:\n\n1. **Node Description**\n   - Node IRI: `<>` (relative, resolves to node)\n   - Type: Placeholder or inferred from flows\n   - Label/description: Minimal stub\n\n2. **Flow Metadata**\n   - Meta flow IRI: `_meta/` (relative)\n   - Flow type: `sflo:NodeMetadataFlow`\n   - Weave label: Initial (e.g., \"2025-11-28_1112_00\")\n   - Sequence number: 1\n\n3. **Provenance (Minimal)**\n   - NodeCreation activity stub\n   - Agent reference (if provided)\n   - Rights/license (if provided)\n   - No full DelegationChain yet\n\n4. **Relative IRIs**\n   - All local references use relative IRIs\n   - External vocabularies use absolute IRIs\n   - No `@base` in serialized output\n\nExample metadata structure:\n```jsonld\n{\n  \"@context\": {\n    \"sflo\": \"https://semantic-flow.github.io/ontology/semantic-flow/\",\n    \"dcat\": \"http://www.w3.org/ns/dcat#\",\n    \"dcterms\": \"http://purl.org/dc/terms/\",\n    \"prov\": \"http://www.w3.org/ns/prov#\"\n  },\n  \"@graph\": [\n    {\n      \"@id\": \"\",\n      \"@type\": \"sflo:BareNode\",\n      \"dcterms:title\": \"Node Title\"\n    },\n    {\n      \"@id\": \"_meta/\",\n      \"@type\": \"sflo:NodeMetadataFlow\",\n      \"sflo:weaveLabel\": \"2025-11-28_1112_00\",\n      \"sflo:versioningState\": { \"@id\": \"sflo:VersioningState/versioned/\" }\n    },\n    {\n      \"@id\": \"_meta/_default/\",\n      \"@type\": \"sflo:DefaultShot\",\n      \"sflo:sequenceNumber\": 1,\n      \"prov:wasGeneratedBy\": {\n        \"@id\": \"_meta/_default/#creation-activity\"\n      }\n    }\n  ]\n}\n```\n\n### 2.6 Directory Structure Created\n\n```\n<nodeTargetPath>/\n├── _node-handle/\n│   └── README.md          # Stub explaining the handle concept\n├── _meta/\n│   ├── v1/\n│   │   └── <slug>_meta.jsonld\n│   └── _default/\n│       └── <slug>_meta.jsonld\n[optional flows if datasets provided:]\n├── _ref/\n│   ├── v1/\n│   │   └── <slug>_ref.jsonld\n│   └── _default/\n│       └── <slug>_ref.jsonld\n├── _payload/\n│   ├── v1/\n│   │   └── <slug>.jsonld\n│   └── _default/\n│       └── <slug>.jsonld\n├── _cfg-op/\n│   ├── v1/\n│   │   └── <slug>_cfg-op.jsonld\n│   └── _default/\n│       └── <slug>_cfg-op.jsonld\n└── _cfg-inh/\n    ├── v1/\n    │   └── <slug>_cfg-inh.jsonld\n    └── _default/\n        └── <slug>_cfg-inh.jsonld\n```\n\n### 2.7 Error Handling\n\nCustom error types in [`shared/core/src/errors/node-errors.ts`](../../shared/core/src/errors/node-errors.ts):\n\n```typescript\nexport class NodeAlreadyExistsError extends Error {\n  constructor(path: string) {\n    super(`Node already exists at: ${path}`);\n    this.name = \"NodeAlreadyExistsError\";\n  }\n}\n\nexport class DirectoryNotEmptyError extends Error {\n  constructor(path: string) {\n    super(`Directory not empty: ${path}. Use allowNonEmpty option to override.`);\n    this.name = \"DirectoryNotEmptyError\";\n  }\n}\n\nexport class InvalidNodePathError extends Error {\n  constructor(path: string, reason: string) {\n    super(`Invalid node path: ${path}. Reason: ${reason}`);\n    this.name = \"InvalidNodePathError\";\n  }\n}\n```\n\n## 3. Simple Node Runner\n\nLocation: [`scripts/create-node.ts`](../../scripts/create-node.ts)\n\n```typescript\n#!/usr/bin/env tsx\n/**\n * Simple CLI runner for createNode operation\n * \n * Usage:\n *   npx tsx scripts/create-node.ts <nodePath> [--allow-non-empty]\n */\n\nimport { createNode } from \"@semantic-flow/core\";\nimport { parseArgs } from \"node:util\";\n\nasync function main() {\n  const { values, positionals } = parseArgs({\n    options: {\n      \"allow-non-empty\": { type: \"boolean\", default: false },\n    },\n    allowPositionals: true,\n  });\n\n  if (positionals.length === 0) {\n    console.error(\"Usage: create-node <nodePath> [--allow-non-empty]\");\n    process.exit(1);\n  }\n\n  const nodePath = positionals[0];\n\n  try {\n    console.log(`Creating node at: ${nodePath}`);\n    \n    const result = await createNode(nodePath, {\n      allowNonEmpty: values[\"allow-non-empty\"] as boolean,\n    });\n\n    console.log(\"\\n✅ Node created successfully!\");\n    console.log(`  Path: ${result.nodePath}`);\n    console.log(`  Slug: ${result.nodeSlug}`);\n    console.log(`  Base IRI: ${result.nodeBaseIri}`);\n    console.log(`\\nFlows created:`);\n    console.log(`  Metadata: ${result.flows.meta}`);\n    if (result.flows.ref) console.log(`  Reference: ${result.flows.ref}`);\n    if (result.flows.payload) console.log(`  Payload: ${result.flows.payload}`);\n    if (result.flows.cfgOp) console.log(`  Config (Op): ${result.flows.cfgOp}`);\n    if (result.flows.cfgInh) console.log(`  Config (Inh): ${result.flows.cfgInh}`);\n    \n  } catch (error) {\n    console.error(\"\\n❌ Error creating node:\");\n    console.error(error instanceof Error ? error.message : String(error));\n    process.exit(1);\n  }\n}\n\nmain();\n```\n\n## 4. Testing Strategy\n\n### 4.1 RdfSource Tests\n\n- Unit tests for each function\n- Format detection tests\n- Base IRI computation tests\n- Round-trip parse/serialize tests\n- Error handling tests\n\n### 4.2 createNode Tests\n\nLocation: [`shared/core/src/operations/__tests__/create-node.test.ts`](../../shared/core/src/operations/__tests__/create-node.test.ts)\n\nTest cases:\n1. Create node in empty directory\n2. Fail on already-initialized node\n3. Fail on non-empty directory without flag\n4. Succeed on non-empty with allowNonEmpty\n5. Create node with payload dataset\n6. Create node with reference dataset\n7. Create node with all optional datasets\n8. Validate metadata content\n9. Validate filename conventions\n10. Validate directory structure\n\n### 4.3 Integration Tests\n\nEnd-to-end scenarios:\n1. Create node → read metadata → validate structure\n2. Create node → import dataset → verify relative IRIs\n3. Multiple nodes in hierarchy\n\n## 5. Documentation Updates\n\n### 5.1 Parsing Semantics Spec\n\nUpdate the following documentation files:\n\n1. **[`concept.identifier.intramesh.relative.md`](../../documentation/concept.identifier.intramesh.relative.md)**\n   - Add section on parsing with base IRI\n   - Add examples of relative IRI resolution\n\n2. **[`concept.iri.md`](../../documentation/concept.iri.md)**\n   - Add section on base IRI computation\n   - Add file:/// URL examples\n\n3. **[`concept.implied-rdf-base.md`](../../documentation/concept.implied-rdf-base.md)**\n   - Add detailed parsing rules\n   - Add serialization rules\n   - Add examples\n\n4. **[`concept.debasing.md`](../../documentation/concept.debasing.md)**\n   - Formalize the algorithm\n   - Add step-by-step examples\n   - Reference RdfSource functions\n\n5. **New file: [`concept.rdf-source.md`](../../documentation/concept.rdf-source.md)**\n   - Document the RdfSource abstraction\n   - API reference\n   - Usage examples\n\n## 6. Dependencies to Add\n\nUpdate [`shared/core/package.json`](../../shared/core/package.json):\n\n```json\n{\n  \"dependencies\": {\n    \"@rdfjs/types\": \"^1.1.0\",\n    \"@rdfjs/data-model\": \"^2.0.0\",\n    \"jsonld\": \"^8.3.2\",\n    \"n3\": \"^1.17.2\"\n  },\n  \"devDependencies\": {\n    \"@types/jsonld\": \"^1.5.13\",\n    \"@types/n3\": \"^1.16.4\"\n  }\n}\n```\n\n## 7. Implementation Order\n\n1. **Phase 1: RdfSource Foundation** (1-2 days)\n   - Install dependencies\n   - Create type definitions\n   - Implement core parsing/serialization\n   - Implement helper functions\n   - Write unit tests\n\n2. **Phase 2: createNode Core** (2-3 days)\n   - Implement path validation\n   - Implement scaffolding functions\n   - Implement metadata generation\n   - Implement optional dataset import\n   - Write unit tests\n\n3. **Phase 3: Integration** (1 day)\n   - Create runner script\n   - Write integration tests\n   - Manual testing\n\n4. **Phase 4: Documentation** (1 day)\n   - Update concept docs\n   - Add API documentation\n   - Update task files\n\n**Total estimated time: 5-7 days**\n\n## 8. Future Enhancements (Out of Scope)\n\nThese are explicitly **not** part of this task:\n\n- Full debasing algorithm with IRI rewriting\n- Payload unpacking into child nodes\n- Config inheritance from parent nodes\n- Full PROV/DelegationChain modeling\n- SHACL validation\n- RDF store integration\n- CLI framework (Oclif) integration\n- Interactive mode with prompts\n\n## 9. Success Criteria\n\nThe implementation is complete when:\n\n1. ✅ RdfSource can parse JSON-LD and Turtle with explicit base\n2. ✅ RdfSource can serialize mesh-native JSON-LD (no @base, relative IRIs)\n3. ✅ createNode creates correct directory structure\n4. ✅ createNode generates valid v1 metadata\n5. ✅ createNode can import optional datasets\n6. ✅ All tests pass (unit + integration)\n7. ✅ Documentation is updated\n8. ✅ Simple runner script works\n9. ✅ Code follows project conventions\n10. ✅ Task files are updated with progress\n\n## References\n\n- [task.2025-11-28-rdfsource-debasing-parsing](./task.2025-11-28-rdfsource-debasing-parsing.md)\n- [task.2025-11-27-createNode](./task.2025-11-27-createNode.md)\n- [concept.identifier.intramesh.relative](./concept.identifier.intramesh.relative.md)\n- [concept.implied-rdf-base](./concept.implied-rdf-base.md)\n- [concept.debasing](./concept.debasing.md)\n- [concept.weave-process](./concept.weave-process.md)\n- [dev.general-guidance](./dev.general-guidance.md)\n","n":0.022}}},{"i":11,"$":{"0":{"v":"2025-11-27-createNode","n":1},"1":{"v":"\n## TODO\n\n- [x] Explore project structure and understand requirements\n- [x] Design RdfSource abstraction architecture\n- [x] Design createNode operation architecture\n- [ ] Document parsing semantics for SFLO spec\n- [ ] Implement RdfSource types and core abstractions\n- [ ] Implement RdfSource parsing/serialization helpers\n- [ ] Implement createNode core function\n- [ ] Implement filesystem scaffolding utilities\n- [ ] Implement metadata generation with provenance stubs\n- [ ] Create tests for RdfSource\n- [ ] Create tests for createNode\n- [ ] Create simple Node.js runner script\n- [ ] Update task files with progress\n\nSee detailed architecture plan in [[task.2025-11-28-architecture-plan]].\n\n## Prompt\n\nImplement a core `createNode` operation that:\n\n- Takes a filesystem path and options.\n- Initializes that path as a **mesh node**.\n- Creates the minimal **handle** and **metadata** folders .\n- Optionally attaches initial payload/reference/config inputs.\n- Enforces basic safety invariants (no double-init, non-empty dir warnings).\n- executes the [[weave process|concept.weave-process]]\n\nThis should be a **pure Node.js library function** plus a minimal Node-based entry point for manual use. Do **not** integrate Oclif yet (future task; likely with `@inquirer/prompts` for interactivity).\n\n---\n\n## Decisions\n\n1. **Name and scope**\n   - Operation name: `createNode`.\n   - Lives in core library (e.g. `@semantic-flow/core`), not tied to any specific CLI framework.\n   - This task provides:\n     - `createNode(nodeTargetPath, options)` implementation.\n     - A thin, non-interactive Node entry point (e.g. `scripts/create-node.ts`) for manual testing.\n\n2. **Metadata flow invariants**\n   - Metadata is **only** written as part of weaves (including this one).\n   - The **metadata flow is always versioned**:\n     - A `v1` FlowShot is created for `_meta`.\n     - A `_default` distribution is a copy of `v1` with `@base` implicitly equal to the file’s future URL (no in-file `@base`).\n   - There is **no `_working` metadata** in this v1.\n\n3. **Mesh-native RDF style**\n   - For all SF-authored RDF files:\n     - No `@base` in the file.\n     - Local identifiers use **relative IRIs** (see `concept.identifier.intramesh.relative`).\n     - Tooling must supply the base IRI when parsing/serializing (based on node path + namespace root).\n   - This applies to metadata and any SF-authored reference/payload flows in this task.\n\n4. **Reference vs payload**\n   - **Exactly one ReferenceFlow per node** (or none).\n   - PayloadFlow is optional; a node having a PayloadFlow makes it a **Payload Node**.\n   - Enforceable difference (to be wired into SHACL later, not enforced in this task’s runtime logic):\n     - If a node has a PayloadFlow, its ReferenceFlow **must** type the node as a `dcat:Dataset`:\n       - In reference data: `<> a dcat:Dataset`.\n\n5. **Safety and idempotence**\n   - `createNode(nodeTargetPath, options)`:\n     - MUST **error** if the path already “looks like a node” (presence of `_node-handle` or `_meta`).\n     - MUST **warn** if the directory exists and is non-empty.\n     - SHOULD be idempotent only in the trivial “fails-fast on already-initialized” sense; no attempt to merge with existing node structure in this task.\n\n6. **Parent topology**\n   - This task does **not** persist any parent/child relationships in metadata.\n   - Optional runtime inspection of parent directories (e.g. to warn about creating “orphan” nodes) is out of scope for v1 and should **not** block this task.\n   - Mesh composability/transposability is preserved by not writing parent references into RDF.\n\n7. **Payload/reference/config handling (v1 scope)**\n   - `createNode` **may** accept:\n     - `payloadDatasetPath?: string`\n     - `referenceDatasetPath?: string`\n     - `operationalConfigPath?: string`\n     - `inheritableConfigPath?: string`\n     - `provenanceInput?: ProvenanceBundleInput` (see below)\n   - In this task:\n     - If `referenceDatasetPath` is provided:\n       - Copy or normalize it into `_ref/_working/` (no unpack, no de-basing logic yet).\n     - If `payloadDatasetPath` is provided:\n       - Copy or normalize it into `_payload/_working/` (no de-basing, no unpack).\n     - Config files:\n       - If provided, copy/normalize into `_cfg-op/_working/` and/or `_cfg-inh/_working/`.\n     - in all cases, the target filename in general will be \"node name\" + \"flow slug\" + \".jsonld\". \n       - the flow slugs are defined in ../ontology/semantic-flow/\n     - \n   - **No de-basing or unpacking** in this task:\n     - Imported payload/reference/config are treated as already “good enough” or mesh-native.\n     - Debasing and unpacking are separate follow-up tasks.\n\n8. **Provenance input (structure, not full PROV)**\n   - `createNode` should accept a **structured provenance input**, *not* a full `ProvenanceContext`.\n   - Example shape (TS, conceptual):\n\n     ```ts\n     type ProvenanceBundleInput = {\n       primaryAgentIri?: string;     // human or system\n       organizationIri?: string;     // rights holder org, optional\n       contributorsIri?: string[];   // additional agents, optional\n       rightsHolderIri?: string;     // explicit rights holder; if absent, may default from org/primary\n       licenseIri?: string;          // e.g., CC BY-SA URL\n     };\n     ```\n\n   - In this task:\n     - You do **not** need to fully implement DelegationChain / ProvenanceContext, but:\n       - `createNode` should **reserve space** in the v1 metadata shot for a future PROV model (e.g., stub nodes with IRIs based on your fragment scheme).\n       - At minimum, write the node’s `rightsHolder` and `license` if provided.\n\n9. **RDF format expectations**\n   - Code should be written so that metadata and reference/payload flows can be emitted as **JSON-LD** datasets (primary), with internal APIs abstract enough to allow future TriG distributions.\n   - Do not hard-wire a specific RDF library beyond what’s needed to:\n     - create simple graphs,\n     - serialize JSON-LD with relative IRIs.\n\n10. **CLI framework**\n    - This task **does not** integrate Oclif or any CLI framework.\n    - A minimal Node entry point (e.g. `node scripts/create-node.js <nodeTargetPath>`) is enough.\n    - Future CLI work:\n      - Will likely use `@inquirer/prompts` (modern Inquirer) for interactivity instead of Enquirer.\n      - Should wrap this `createNode` core function, not replace it.\n\n---\n\n## Requirements\n\n### Functional\n\n- Implement:\n\n  ```ts\n  async function createNode(\n    nodeTargetPath: string,\n    options?: {\n      payloadDatasetPath?: string;\n      referenceDatasetPath?: string;\n      operationalConfigPath?: string;\n      inheritableConfigPath?: string;\n      provenanceInput?: ProvenanceBundleInput;\n      allowNonEmpty?: boolean;\n    }\n  ): Promise<void>;\n````\n\n* Behavior:\n\n  1. **Path handling**\n\n     * If `nodeTargetPath` does not exist: create directory.\n     * If it exists and:\n\n       * contains `_node-handle` or `_meta` → throw an error (“Node already initialized”).\n       * is non-empty and `allowNonEmpty` is not set → throw an error or require explicit override.\n\n  2. **Node scaffolding**\n\n     * Create `_node-handle/` folder (stub content; minimal files per existing ontology conventions).\n\n     * Create `_meta/` with structure:\n\n       ```\n       _meta/\n         v1/\n           <metadata>.jsonld\n         _default/\n           <metadata>.jsonld   # content identical to v1, but treated as “current”\n       ```\n\n     * Metadata content for v1 must include at least:\n\n       * A minimal node description stub (node IRI, type placeholder).\n       * Rights/licensing if present in `provenanceInput` (at dataset or node level).\n       * A placeholder or minimal structure for the NodeCreation Activity and provenance model (no need to fully flesh out DelegationChain yet, but leave room).\n\n  3. **Optional flows**\n\n     * If `referenceDatasetPath` provided:\n\n       * Create `_ref/v1/…` and copy/normalize the file there.\n       * (No validation that `<> a dcat:Dataset` is present yet; that’s SHACL-level, not runtime.)\n     * If `payloadDatasetPath` provided:\n\n       * Create `_payload/v1/…` and copy/normalize the file.\n     * If config paths provided:\n\n       * Create `_cfg-op/v1/…` and/or `_cfg-inh/v1/…` and copy/normalize.\n\n  4. **No advanced behaviors**\n\n     * No de-basing (no IRI rewriting).\n     * No unpacking payload into child nodes.\n     * No config inheritance from parent nodes.\n\n* Implement a simple Node-based runner:\n\n  ```bash\n  node scripts/create-node.js <nodeTargetPath> [--allow-nonempty]\n  ```\n\n  that:\n\n  * resolves `nodeTargetPath`,\n  * calls `createNode(nodeTargetPath, { allowNonEmpty: true/false })`,\n  * logs structured success/failure.\n\n### Non-Functional\n\n* Clear, structured errors for:\n\n  * already-initialized node,\n  * non-empty directory without `allowNonEmpty`,\n  * filesystem failures.\n* Logging should integrate with your existing logging abstraction if available; otherwise, stub logging with a thin wrapper that can be replaced later.\n* Keep the core function testable without side effects beyond filesystem writes:\n\n  * All external interactions (e.g., resolving base IRIs, future namespace info) should be parameterizable or stubbed for now.\n\n---\n\n## Deliverables\n\n1. **Core implementation**\n\n   * `createNode` function in the core Node library.\n   * Supporting types (`ProvenanceBundleInput`, options interface).\n\n2. **Filesystem layout tests**\n\n   * Tests that:\n\n     * Starting from an empty directory, `createNode` produces the expected folder/file structure.\n     * Running `createNode` again fails with “already initialized.”\n     * Non-empty directory handling behaves correctly.\n\n3. **Minimal metadata content tests**\n\n   * Sanity checks that:\n\n     * `_meta/v1/*.jsonld` and `_default/*.jsonld` both exist.\n     * `rightsHolder` and `license` triples are present when `provenanceInput` provides them.\n     * Local IRIs in metadata are relative (no `@base` in the file).\n\n4. **Simple Node runner**\n\n   * `scripts/create-node.(ts|js)` or equivalent, wired to `createNode` with basic CLI arg parsing.\n\n---\n\n## Out of Scope / Follow-ups\n\n* Oclif-based CLI wrapping `createNode` (with `@inquirer/prompts`).\n* De-basing (namespace adoption) of imported datasets.\n* Payload unpacking into child nodes.\n* Config inheritance (from parent nodes).\n* Full PROV/DelegationChain/ProvenanceContext modeling in metadata (beyond minimal stubs).\n* RDF store integration or SHACL validation wiring (e.g., the `PayloadNode ⇒ dcat:Dataset` rule).\n","n":0.028}}},{"i":12,"$":{"0":{"v":"Replace Monotonic V N Snapshots with Human Readable Weave Labels","n":0.316},"1":{"v":"\n\n## **CURRENT STATE**\n\nThe ontology groundwork is complete:\n- `sflo:weaveLabel` property exists for human-readable labels\n- `sflo:sequenceNumber` property exists for monotonic ordering\n- `sflo:previousSnapshot` property exists for snapshot linking\n- FlowShot terminology (Snapshot/DefaultShot/WorkingShot) is established in the ontology\n\nThis task focuses on **documentation consistency** and **ontology verification**.\n\n---\n\n## **GOAL**\n\nDocument the enhanced snapshot folder naming that combines human-readable weave labels with sequence numbers:\n\n```\nYYYY-MM-DD_HHMM_SS_vN\n```\n\nExamples:\n* `2025-11-24_0142_07_v1`\n* `2025-11-24_0142_08_v2`\n* `2025-11-24_0142_15_v3`\n\nWhile preserving:\n* Immutable snapshots (FlowShots)\n* Consistent `_default` semantics (DefaultShot)\n* A linear `previousSnapshot` chain for metadata flows\n* The folder name as part of the canonical IRI identity\n\n---\n\n## **MOTIVATION**\n\n1. `_vN` alone gives ordinality but no human context\n2. Full millisecond timestamps (`20251109181158123`) are too visually dense for humans\n3. Weaves are slow, human-mediated operations, so:\n   * Second-resolution labels are adequate, and\n   * Same-second multiple runs would be extremely rare\n4. Human-friendly labels improve UX without affecting machine identity\n\n---\n\n## **SPECIFICATION**\n\n### **1. Flowshot (Snapshot) Folder Format**\n\nEach snapshot folder uses:\n\n```\n<weaveLabel>_v<sequenceNumber>\n```\n\nWhere:\n\n1. **weaveLabel:** a human-oriented identifier, generated once per weave run:\n   ```\n   YYYY-MM-DD_HHMM_SS\n   ```\n   (timestamp with second-level precision)\n\n2. **sequenceNumber:**\n   A strictly monotonic integer *per flow*, stored inside snapshot metadata and reflected in folder name:\n   ```\n   sflo:sequenceNumber 17 .\n   ```\n\nSequence number provides ordering; weave label provides readability.\n\n---\n\n### **2. Weave Label Allocation**\n\n* **Only one process should weave a mesh/sub-mesh at a time.**\n\n* Advisory soft lock (see [[concept.weave-process.weave-lock]]):\n  * At start of weave: create `.weave-lock` in the mesh root\n  * Lock file contains: PID, host, timestamp\n  * Stale locks may be broken with `--force`\n\n* Within a single weave run:\n  * Core computes `weaveLabel` once and reuses it for all flows\n  * The timestamp is captured at weave start and includes seconds precision\n\n---\n\n### **3. FlowShot Structure (per flow)**\n\nEach flow's directory structure:\n\n```\n/<node>/<flow>/\n  YYYY-MM-DD_HHMM_SS_vN/    # Snapshot (immutable FlowShot)\n  _default/                 # DefaultShot\n  _working/                 # WorkingShot\n```\n\nA Snapshot dataset includes at minimum:\n\n```turtle\n:ThisSnapshot a sflo:Snapshot ;\n    sflo:weaveRunId \"urn:uuid:…\" ;\n    sflo:weaveLabel \"2025-11-24_0142_55\" ;\n    sflo:sequenceNumber 7 ;\n    prov:generatedAtTime \"2025-11-24T01:42:55Z\"^^xsd:dateTime ;\n    sflo:previousSnapshot :PriorSnapshot .   # optional if no prior\n```\n\n---\n\n### **4. DefaultShot Semantics**\n\n`_default/` for any flow is:\n\n* A **byte-for-byte mirror** of the latest snapshot dataset\n* Contains **no additional metadata** not present in the snapshot itself\n* Updated on weave by **replacing** its contents atomically\n\nTherefore:\n* DefaultShot = \"current snapshot content\"\n* Snapshot = \"immutable dataset with history metadata\"\n* All historical links live **inside the snapshot**, not in `_default`\n\n---\n\n### **5. Canonical Identity**\n\nThe canonical identity of a snapshot IS its IRI, which includes the folder name:\n\n```\nhttps://example.org/my-node/_payload/2025-11-24_0142_07_v1/\n```\n\nSince the folder name is part of the IRI path, it directly contributes to the identity. The weave label and sequence number in the folder name become permanent parts of the snapshot's identity.\n\n---\n\n\n## **TERMINOLOGY & REPLACEMENTS**\n\n### **Key Terms**\n\nTo maintain consistency across documentation, use these preferred terms:\n\n**FlowShot-related:**\n- **FlowShot** - The abstract concept of a flow realization (Snapshot, DefaultShot, or WorkingShot)\n- **Snapshot** (or **Snapshot FlowShot**) - An immutable, versioned FlowShot (formerly called \"snapshot\" or \"flow snapshot\")\n- **DefaultShot** - The current/latest FlowShot content (the `_default/` folder)\n- **WorkingShot** - The mutable staging FlowShot (the `_working/` folder)\n\n**Folder-related:**\n- **snapshot folder** - Replace references to `_vN` folders with \"snapshot folder\" or \"Snapshot FlowShot folder\"\n- **flowshot folder** - Generic term for any FlowShot folder (`_default/`, `_working/`, or snapshot folders)\n\n### **Replacement Guidelines**\n\nWhen updating documentation, apply these substitutions:\n\n| **Old Term**           | **New Term**                  | **Context**                             |\n| ---------------------- | ----------------------------- | --------------------------------------- |\n| `_vN` (as a concept)   | snapshot folder               | When referring to the folder itself     |\n| `_vN/` (in paths)      | `YYYY-MM-DD_HHMM_SS_vN/`      | When showing folder paths               |\n| \"snapshot\"             | Snapshot or Snapshot FlowShot | When referring to the concept           |\n| \"flow snapshot\"        | FlowShot (usually)            | Generic references to flow realizations |\n| \"snapshot\" (ambiguous) | Specify: Snapshot FlowShot    | When the Snapshot type is meant         |\n| \"_default snapshot\"    | DefaultShot                   | The current/latest realization          |\n| \"_working snapshot\"    | WorkingShot                   | The mutable staging realization         |\n\n### **Common Ambiguities**\n\n**\"Flow snapshots\" vs \"Snapshot FlowShots\":**\n- **\"Flow snapshots\"** (legacy term) → Usually means **FlowShots** (all types: Snapshot, DefaultShot, WorkingShot)\n- **\"Snapshot FlowShot\"** or **\"Snapshot\"** → Specifically means the immutable, versioned type\n\n**When documenting:**\n- Use \"FlowShot\" for the general concept\n- Use \"Snapshot\" or \"Snapshot FlowShot\" when specifically referring to versioned, immutable realizations\n- Use \"DefaultShot\" and \"WorkingShot\" for their specific folder types\n\n---\n\n## **DELIVERABLES**\n\n### **1. Ontology Verification**\n\nVerify these properties exist and are correctly defined:\n- ✅ `sflo:weaveLabel` (exists in semantic-flow ontology)\n- ✅ `sflo:sequenceNumber` (exists in semantic-flow ontology)\n- ✅ `sflo:previousSnapshot` (exists in semantic-flow ontology)\n- ✅ FlowShot/Snapshot/DefaultShot/WorkingShot classes (exist in semantic-flow ontology)\n\n### **2. Documentation Updates Required**\n\nUpdate all references from simple `_vN` to the new format in:\n\n**Core Concept Documentation:**\n* [`concept.weave-label.md`](documentation/concept.weave-label.md) - Update format from YYYYMMDD.HHMMSS to YYYY-MM-DD_HHMM_SS\n* [`concept.flow-version.md`](documentation/concept.flow-version.md) - Clarify relationship with flowshot naming\n* [`folder.flowshot.md`](documentation/folder.flowshot.md) - Update to show concatenation format\n\n**High Priority Updates (Multiple `_vN` references):**\n* [`concept.summary.md`](documentation/concept.summary.md) - 8 occurrences\n* [`guide.product-brief.md`](documentation/guide.product-brief.md) - Snapshot descriptions\n* [`concept.weave-process.md`](documentation/concept.weave-process.md) - Weave process descriptions\n\n**FlowShot Terminology Alignment:**\n* [`mesh-resource.node-component.flow-snapshot.version.md`](documentation/mesh-resource.node-component.flow-snapshot.version.md)\n* [`mesh-resource.node-component.flow-shot.default.md`](documentation/mesh-resource.node-component.flow-shot.default.md)\n* [`mesh-resource.node-component.flow-shot.working.md`](documentation/mesh-resource.node-component.flow-shot.working.md)\n\n**Other Documentation:**\n* [`facet.flow.versioned.md`](documentation/facet.flow.versioned.md)\n* [`mesh-resource.node-component.md`](documentation/mesh-resource.node-component.md)\n* [`concept.metadata.provenance.md`](documentation/concept.metadata.provenance.md)\n* [`mesh-resource.node-component.flow.payload.md`](documentation/mesh-resource.node-component.flow.payload.md)\n* [`mesh-resource.node-component.node-config-defaults.md`](documentation/mesh-resource.node-component.node-config-defaults.md)\n\n**Check for existence and update if present:**\n* `concept.namespace.segment.system.md`\n* `folder.node.md`\n* `facet.filesystem.folder.md`\n\n---\n\n## **TODO**\n\n### **Completed Documentation Updates (2025-11-27)**\n\n- [x] Update `concept.weave-label.md` format description - **COMPLETED** (changed to YYYY-MM-DD_HHMM_SS)\n- [x] Update all `_vN` references in `concept.summary.md` - **COMPLETED** (8+ occurrences updated)\n- [x] Update `guide.product-brief.md` snapshot descriptions - **COMPLETED** (FlowShots reference updated)\n- [x] Update `concept.flow-version.md` - **COMPLETED** (added format details with sequence numbers)\n- [x] Update `folder.snapshot.md` - **COMPLETED** (format specification with examples)\n- [x] Update `concept.metadata.provenance.md` - **COMPLETED** (RDF turtle examples updated)\n- [x] Update `concept.git.md` - **COMPLETED** (5 sections updated with new format)\n- [x] Update `facet.filesystem.folder.md` - **COMPLETED** (snapshot folder section rewritten)\n- [x] Update `mesh-resource.node-component.flow.payload.md` - **COMPLETED** (snapshot references updated)\n- [x] Update `mesh-resource.node-component.flow-shot.default-shot.md` - **COMPLETED** (example updated)\n- [x] Update `mesh-resource.node-component.flow.md` - **COMPLETED** (code example updated)\n- [x] Update `concept.mesh.md` - **COMPLETED** (table examples updated)\n- [x] Update `folder.node.md` - **COMPLETED** (distribution folder reference updated)\n- [x] Update `mesh-resource.node.payload.md` - **COMPLETED** (version 3 example updated)\n- [x] Review and update `concept.weave-process.md` - **COMPLETED** (terminology and format fixed)\n\n### **Remaining Tasks**\n\n- [ ] Verify all ontology properties are consistent in semantic-flow-ontology.jsonld\n- [ ] Ensure all documentation cross-references are consistent\n- [ ] Perform final search for any remaining `_vN` patterns in documentation\n\n---\n\n## **DECISIONS**\n\n* **2025-11-27**: Confirmed terminology - using `sflo:Snapshot` (not SnapshotShot) and `sflo:previousSnapshot` throughout\n* **2025-11-27**: Changed format to `YYYY-MM-DD_HHMM_SS_vN` using seconds (with underscore separator) instead of suffix approach\n* **2025-11-26**: Confirmed folder format keeping the `_vN` suffix for sequence numbers\n* **2025-11-26**: Aligned with FlowShot terminology (Snapshot/DefaultShot/WorkingShot)\n* **2025-11-26**: Clarified that folder name IS part of the canonical IRI identity\n* **2025-11-26**: Focused on documentation and ontology verification only (no implementation)\n\n---\n\n## **NOTES**\n\n* The sequence number (_vN) remains in folder names for clear ordinality\n* Weave labels provide human-readable temporal context\n* The folder name becomes part of the permanent IRI identity\n* No implementation work is needed at this stage\n* Documentation consistency is the primary goal\n* The [`concept.weave-process.md`](documentation/concept.weave-process.md) has been reviewed and updated for consistency\n\n","n":0.031}}},{"i":13,"$":{"0":{"v":"2025 11 06 Embed Rdf Store","n":0.408},"1":{"v":"\n## Decisions\n\n\n## Prompt\n\nLet's add the latest Quadstore, Comunica, @rdfjs/types, rdf-parse, memory-level, and rdf-data-factory to dependencies, and implement singleton quadstore and single queryEngine for the application's RDF storage and querying needs. \n\n\n## TODO\n\n\n","n":0.177}}},{"i":14,"$":{"0":{"v":"2025 11 04 Shared Logging and Errorhandling","n":0.378},"1":{"v":"\n## Prompt\n# Semantic Flow Logging & Error Handling System Specification\n## Node.js Platform Rewrite\n\n**Version:** 1.0  \n**Date:** November 4, 2025  \n**Status:** Working Specification\n\n---\n\n## Overview\n\nThis specification defines a streamlined, production-ready logging and error handling system for the Semantic Flow platform's Node.js rewrite. It consolidates the best features from the current Deno implementation while addressing identified redundancies and adding improvements for both CLI and service usage.\n\n### Design Goals\n\n1. **Unified Architecture**: Single system supporting both CLI tools and long-running services\n2. **Simplified API**: Eliminate redundant functions and streamline the interface  \n3. **Enhanced Performance**: Optimized for high-throughput service operations\n4. **Better Developer Experience**: Clear patterns, comprehensive documentation, and testing utilities\n5. **Production Ready**: Robust error recovery, monitoring, and observability features\n\n---\n\n## Core Architecture\n\n### Module Structure\n```\nsrc/logging/\n├── core/\n│   ├── logger.ts              # Main logger implementation\n│   ├── types.ts               # Type definitions and interfaces\n│   ├── formatters.ts          # Message formatting utilities\n│   └── context.ts             # Context management and merging\n├── channels/\n│   ├── console.ts             # Console output channel\n│   ├── file.ts                # File logging with rotation\n│   └── monitoring.ts          # External monitoring (Sentry, etc.)\n├── errors/\n│   ├── capture.ts             # Error capture and logging\n│   ├── recovery.ts            # Error recovery strategies  \n│   ├── types.ts               # Error type definitions\n│   └── factory.ts             # Error class factory utilities\n├── config/\n│   ├── loader.ts              # Configuration loading and validation\n│   ├── schema.ts              # Configuration schema definitions\n│   └── defaults.ts            # Default configurations\n├── utils/\n│   ├── context.ts             # AsyncLocalStorage context management\n│   ├── performance.ts         # Performance monitoring utilities\n│   ├── testing.ts             # Testing utilities and mocks\n│   └── validation.ts          # Input validation helpers\n└── index.ts                   # Main exports\n```\n\n---\n\n## Type System\n\n### Core Types\n\n```typescript\n// Enhanced log levels with numeric values for easy comparison\nexport enum LogLevel {\n  TRACE = 0,\n  DEBUG = 10,\n  INFO = 20,\n  WARN = 30,\n  ERROR = 40,\n  FATAL = 50\n}\n\n// String literals for configuration mapping\nexport const LogLevelStrings = ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] as const;\nexport type LogLevelString = typeof LogLevelStrings[number];\n\n// Core log entry structure used throughout the system\nexport interface LogEntry {\n  timestamp: number;                 // Date.now()\n  level: LogLevel;\n  message: string;\n  context?: LogContext;\n  error?: {\n    name: string;\n    message: string;\n    stack?: string[];\n    code?: string;\n  };\n  service: { \n    name: string; \n    version?: string; \n    instanceId?: string \n  };\n  pid: number;\n  hostname?: string;\n}\n\n// Comprehensive but streamlined log context\nexport interface LogContext {\n  // Core identification\n  operation?: string;\n  operationId?: string;\n  component?: string;\n  \n  // Semantic Flow specific context\n  meshId?: string;\n  nodeId?: string;\n  meshName?: string;\n  nodeName?: string;\n  \n  // Performance tracking\n  startTime?: number;\n  duration?: number;\n  memoryUsage?: number;\n  \n  // Request context (for service mode)\n  requestId?: string;\n  userId?: string;\n  sessionId?: string;\n  \n  // Error context\n  errorCode?: string;\n  errorType?: string;\n  stackTrace?: string[];\n  errorCause?: unknown;\n  \n  // Flexible metadata\n  tags?: Record<string, string>;\n  metadata?: Record<string, unknown>;\n}\n\n// Error capture options (for logging/reporting)\nexport interface ErrorCaptureOptions {\n  message?: string;\n  context?: LogContext;\n  logLevel?: LogLevel;\n  includeStackTrace?: boolean;\n  reportToMonitoring?: boolean;\n}\n\n// Error recovery options (for control flow)\nexport interface ErrorRecoveryOptions<T = unknown> {\n  strategy: ErrorRecoveryStrategy;\n  fallbackValue?: T;\n  retryCount?: number;\n  retryDelay?: number;\n  shouldRetry?: (error: unknown, attempt: number) => boolean;\n}\n\n// Base log channel interface for extensibility\nexport interface LogChannel {\n  write(entry: LogEntry): Promise<void> | void;\n  flush(): Promise<void>;\n  close(): Promise<void>;\n  readonly minLevel: LogLevel;\n}\n\n// Channel configuration\nexport interface ChannelConfig {\n  enabled: boolean;\n  level: LogLevel;\n  format: 'json' | 'pretty' | 'compact';\n  \n  // Channel-specific options\n  console?: {\n    colors?: boolean;\n    timestamps?: boolean;\n  };\n  file?: {\n    path?: string;\n    maxSize?: number;\n    maxFiles?: number;\n    rotationStrategy?: 'time' | 'size';\n  };\n  monitoring?: {\n    provider: 'sentry' | 'datadog' | 'newrelic';\n    dsn?: string;\n    environment?: string;\n    sampleRate?: number;\n  };\n}\n\n// Main logger configuration\nexport interface LoggerConfig {\n  serviceName: string;\n  serviceVersion: string;\n  environment: 'development' | 'staging' | 'production';\n  instanceId?: string;\n  \n  // Channel configurations\n  console: ChannelConfig;\n  file: ChannelConfig;\n  monitoring: ChannelConfig;\n  \n  // Performance settings\n  async: boolean;        // true: buffered writes + sync stderr for ERROR/FATAL\n                        // false: synchronous writes where possible\n  bufferSize: number;\n  flushInterval: number;\n  \n  // Context settings\n  autoContext: {\n    includeTimestamp: boolean;\n    includeHostname: boolean;\n    includeProcessInfo: boolean;\n  };\n}\n```\n\n### Error Types\n\n```typescript\n// Base error class with enhanced context\nexport class SemanticFlowError extends Error {\n  public readonly code: string;\n  public readonly context: Record<string, unknown>;\n  public readonly timestamp: Date;\n  public readonly recoverable: boolean;\n  \n  constructor(\n    message: string,\n    code: string,\n    context: Record<string, unknown> = {},\n    recoverable = true\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n    this.code = code;\n    this.context = context;\n    this.timestamp = new Date();\n    this.recoverable = recoverable;\n  }\n}\n\n// Error factory to reduce boilerplate\nexport function createErrorType(\n  name: string,\n  code: string,\n  recoverable = true\n): new (message: string, context?: Record<string, unknown>) => SemanticFlowError {\n  return class extends SemanticFlowError {\n    constructor(message: string, context: Record<string, unknown> = {}) {\n      super(message, code, context, recoverable);\n      this.name = name;\n    }\n  };\n}\n\n// Specific error types using factory\nexport const ValidationError = createErrorType('ValidationError', 'VALIDATION_ERROR');\nexport const ConfigurationError = createErrorType('ConfigurationError', 'CONFIG_ERROR', false);\nexport const MeshProcessingError = createErrorType('MeshProcessingError', 'MESH_PROCESSING_ERROR');\n\nexport class ApiError extends SemanticFlowError {\n  public readonly statusCode: number;\n  \n  constructor(message: string, statusCode: number, context?: Record<string, unknown>) {\n    super(message, 'API_ERROR', context);\n    this.statusCode = statusCode;\n  }\n}\n```\n\n---\n\n## Core Logger Interface\n\n### Simplified Logger API\n\n```typescript\nexport interface Logger {\n  // Core logging methods (synchronous with async buffering)\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  // Context management (returns immutable context wrappers)\n  withContext(context: LogContext): Logger;\n  withOperation(operation: string, operationId?: string): Logger;\n  withComponent(component: string): Logger;\n  child(context: LogContext): Logger; // Alias for withContext\n  \n  // Performance tracking\n  startTimer(operation: string): Timer;\n  \n  // Error capture (for logging/reporting only)\n  captureError(error: unknown, options?: ErrorCaptureOptions): void;\n  \n  // Lifecycle management\n  flush(): Promise<void>;\n  close(): Promise<void>;\n}\n\n// Child logger semantics documentation:\n// - child() returns a thin wrapper sharing transports and buffer with parent\n// - context is frozen and shallow-merged per call\n// - no mutation or context bleed between child instances\n\nexport interface Timer {\n  end(context?: LogContext): void;\n  checkpoint(label: string, context?: LogContext): void;\n}\n```\n\n### Factory Functions\n\n```typescript\n// Singleton pattern with dependency injection support\nexport function initLogger(config?: Partial<LoggerConfig>): Logger;\nexport function getLogger(): Logger;\nexport function __resetLoggerForTests(): void;\n\n// Factory functions for initialization\nexport function createLogger(config?: Partial<LoggerConfig>): Logger;\nexport function createCliLogger(options?: {\n  verbose?: boolean;\n  quiet?: boolean;\n  format?: 'pretty' | 'json';\n}): Logger;\nexport function createServiceLogger(serviceName: string, options?: {\n  enableFileLogging?: boolean;\n  enableMonitoring?: boolean;\n  environment?: string;\n}): Logger;\n\n// Component-scoped logger (pure ESM)\nexport function getComponentLogger(sourceUrl: string /* import.meta.url */): Logger {\n  const file = new URL(sourceUrl);\n  const base = file.pathname.split(\"/\").pop() ?? \"unknown\";\n  const component = base.replace(/\\.(m|c)?js|ts$/, \"\");\n  return getLogger().child({ component });\n}\n```\n\n---\n\n## Separated Error Handling\n\n### Error Capture (Logging/Reporting)\n\n```typescript\n// Pure error capture - only logs and reports, no control flow\nexport function captureError(error: unknown, options: ErrorCaptureOptions = {}): void {\n  // Synchronous logging for ERROR/FATAL levels\n  // Async buffering for full reporting\n}\n```\n\n### Error Recovery (Control Flow)\n\n```typescript\n// Error recovery strategies (simplified)\nexport enum ErrorRecoveryStrategy {\n  CONTINUE = 'continue',        // Don't throw, continue execution\n  RETHROW = 'rethrow',         // Log then rethrow (default)\n  FALLBACK = 'fallback',       // Return fallback value\n  RETRY = 'retry'              // Retry operation with backoff\n}\n\n// Apply recovery strategy to an error\nexport async function applyErrorRecovery<T>(\n  error: unknown,\n  options: ErrorRecoveryOptions<T>\n): Promise<T | never> {\n  // Implementation handles retry logic, fallbacks, etc.\n}\n\n// Convenience wrapper combining capture + recovery\nexport async function withErrorHandling<T>(\n  operation: () => Promise<T>,\n  options: {\n    capture?: ErrorCaptureOptions;\n    recovery?: ErrorRecoveryOptions<T>;\n  } = {}\n): Promise<T | undefined> {\n  try {\n    return await operation();\n  } catch (error) {\n    if (options.capture) {\n      captureError(error, options.capture);\n    }\n    if (options.recovery) {\n      return await applyErrorRecovery(error, options.recovery);\n    }\n    throw error; // Default: rethrow\n  }\n}\n```\n\n### Error Classification\n\n```typescript\nexport class ErrorClassifier {\n  static classify(error: unknown): {\n    type: string;\n    severity: LogLevel;\n    recoverable: boolean;\n    category: 'system' | 'business' | 'validation' | 'network' | 'unknown';\n  };\n  \n  static shouldReport(error: unknown, threshold: LogLevel): boolean;\n  static extractContext(error: unknown): Record<string, unknown>;\n}\n```\n\n---\n\n## Channel Implementations\n\n### Console Channel\n\n```typescript\nexport class ConsoleChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  \n  constructor(private config: ChannelConfig['console']) {\n    this.minLevel = config.level;\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Always synchronous for console - push async work to buffer\n    if (entry.level >= LogLevel.ERROR) {\n      this.writeSynchronous(entry);\n    } else {\n      this.writeStandard(entry);\n    }\n  }\n  \n  flush(): Promise<void> {\n    // Console doesn't buffer, so flush is a no-op\n    return Promise.resolve();\n  }\n  \n  close(): Promise<void> {\n    return Promise.resolve();\n  }\n  \n  // Synchronous critical path for errors\n  private writeSynchronous(entry: LogEntry): void {\n    const line = this.formatCritical(entry);\n    try {\n      process.stderr.write(line);\n    } catch {\n      // Best effort - never throw from logging\n    }\n  }\n  \n  // Standard output for non-critical levels\n  private writeStandard(entry: LogEntry): void {\n    const line = process.stdout.isTTY \n      ? this.formatForTTY(entry) \n      : this.formatForPipe(entry);\n    try {\n      process.stdout.write(line);\n    } catch {\n      // Best effort - never throw from logging\n    }\n  }\n  \n  // Smart formatting based on environment\n  private formatForTTY(entry: LogEntry): string;\n  private formatForPipe(entry: LogEntry): string;\n  private formatCritical(entry: LogEntry): string;\n}\n```\n\n### File Channel\n\n```typescript\nexport class FileChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  private buffer: LogEntry[] = [];\n  private flushTimer: NodeJS.Timer | null = null;\n  \n  constructor(private config: ChannelConfig['file']) {\n    this.minLevel = config.level;\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Always async for file channel - add to buffer\n    this.buffer.push(entry);\n    this.scheduleFlush();\n  }\n  \n  // Single-flight flush to prevent concurrent flushes\n  private inflight?: Promise<void>;\n  \n  flush(): Promise<void> {\n    return this.inflight ?? (this.inflight = this.flushImpl().finally(() => this.inflight = undefined));\n  }\n  \n  close(): Promise<void> {\n    if (this.flushTimer) {\n      clearTimeout(this.flushTimer);\n      this.flushTimer = null;\n    }\n    return this.flushBuffer();\n  }\n  \n  // Enhanced rotation with compression and atomic operations\n  private rotateIfNeeded(): Promise<void>; // Uses fs.rename for atomicity\n  private compressOldLogs(): Promise<void>;\n  private scheduleFlush(): void;\n  private flushImpl(): Promise<void>; // Uses fs.writev for batched writes\n  \n  // File opened with O_APPEND for safe concurrent writes\n  // Reopen file descriptor after rotation\n}\n```\n\n### Monitoring Channel\n\n```typescript\nexport class MonitoringChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  private buffer: LogEntry[] = [];\n  private rateLimiter: RateLimiter;\n  \n  constructor(private config: ChannelConfig['monitoring']) {\n    this.minLevel = config.level;\n    this.rateLimiter = new RateLimiter(config.sampleRate || 1.0);\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Apply sampling and rate limiting\n    if (!this.shouldSample(entry)) return;\n    \n    // Always async for monitoring - add to buffer with timeout protection\n    this.buffer.push(entry);\n    this.scheduleFlush();\n  }\n  \n  // Single-flight flush to prevent concurrent monitoring flushes\n  private inflight?: Promise<void>;\n  \n  flush(): Promise<void> {\n    return this.inflight ?? (this.inflight = this.flushImpl().finally(() => this.inflight = undefined));\n  }\n  \n  close(): Promise<void> {\n    return this.flushWithTimeout();\n  }\n  \n  // Provider-specific implementations\n  private sentryAdapter: SentryAdapter;\n  private datadogAdapter?: DatadogAdapter;\n  \n  // Smart sampling and rate limiting with timeout protection\n  private shouldSample(entry: LogEntry): boolean;\n  private scheduleFlush(): void;\n  private flushImpl(): Promise<void>; // Bounds batch size and applies per-entry deadlines\n  \n  // Drop counter for monitoring timeouts/failures\n  private droppedCount = 0;\n  public getDroppedCount(): number { return this.droppedCount; }\n}\n\n// Missing primitive definitions\nclass RateLimiter {\n  constructor(private rate: number) {}\n  allow(): boolean { return Math.random() < this.rate; }\n}\n\ninterface SentryAdapter {\n  send(entry: LogEntry): Promise<void>;\n}\n\ninterface DatadogAdapter {\n  send(entry: LogEntry): Promise<void>;\n}\n```\n\n---\n\n## Configuration System\n\n### Async Flag Semantics\n\nThe `async` configuration flag controls write behavior:\n\n- **`async: true`** (default): Non-blocking buffered writes for all channels. ERROR/FATAL levels also emit synchronously to `process.stderr` for immediate visibility.\n- **`async: false`**: Console and file channels use `writeSync` on ERROR/FATAL and best-effort synchronous writes for other levels. Monitoring channel remains buffered with best-effort flush. **Warning**: Synchronous I/O can impact performance significantly under load.\n\n### Schema-Based Configuration\n\n```typescript\n// JSON Schema for logger configuration\nexport const LoggerConfigSchema = {\n  type: 'object',\n  properties: {\n    serviceName: { type: 'string', minLength: 1 },\n    serviceVersion: { type: 'string' },\n    environment: { \n      type: 'string', \n      enum: ['development', 'staging', 'production'] \n    },\n    console: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: true },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        format: { type: 'string', enum: ['json', 'pretty', 'compact'] }\n      }\n    },\n    file: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: false },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        format: { type: 'string', enum: ['json', 'pretty', 'compact'] },\n        path: { type: 'string' },\n        maxSize: { type: 'number', minimum: 1024 },\n        maxFiles: { type: 'number', minimum: 1 },\n        rotationStrategy: { type: 'string', enum: ['time', 'size'] }\n      }\n    },\n    monitoring: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: false },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        provider: { type: 'string', enum: ['sentry', 'datadog', 'newrelic'] },\n        dsn: { type: 'string' },\n        environment: { type: 'string' },\n        sampleRate: { type: 'number', minimum: 0, maximum: 1 }\n      }\n    }\n  },\n  required: ['serviceName']\n};\n\n// Configuration loader with multiple sources\nexport class ConfigLoader {\n  static load(sources: {\n    defaults?: Partial<LoggerConfig>;\n    configFile?: string;\n    environment?: Record<string, string>;\n    cliArgs?: Record<string, unknown>;\n  }): LoggerConfig;\n  \n  static validate(config: unknown): LoggerConfig;\n  static merge(...configs: Partial<LoggerConfig>[]): LoggerConfig;\n  \n  // Map string level names to enum values\n  static parseLogLevel(level: string): LogLevel {\n    const index = LogLevelStrings.indexOf(level as LogLevelString);\n    return index >= 0 ? (index * 10) as LogLevel : LogLevel.INFO;\n  }\n}\n```\n\n### Environment Variable Mapping\n\n```bash\n# Service identification\nSF_SERVICE_NAME=semantic-flow-service\nSF_SERVICE_VERSION=2.0.0\nSF_ENVIRONMENT=production\n\n# Console logging\nSF_LOG_CONSOLE_ENABLED=true\nSF_LOG_CONSOLE_LEVEL=info\nSF_LOG_CONSOLE_FORMAT=pretty\n\n# File logging  \nSF_LOG_FILE_ENABLED=true\nSF_LOG_FILE_PATH=./logs/sf-service.log\nSF_LOG_FILE_LEVEL=debug\nSF_LOG_FILE_MAX_SIZE=10485760\nSF_LOG_FILE_MAX_FILES=5\n\n# Monitoring\nSF_LOG_MONITORING_ENABLED=true\nSF_LOG_MONITORING_PROVIDER=sentry\nSF_LOG_MONITORING_DSN=https://...\nSF_LOG_MONITORING_SAMPLE_RATE=0.1\n```\n\n---\n\n## Context Management\n\n### AsyncLocalStorage Integration\n\n```typescript\n// Automatic context propagation using Node.js AsyncLocalStorage\nexport class ContextManager {\n  private static als = new AsyncLocalStorage<LogContext>();\n  \n  static run<T>(context: LogContext, fn: () => T): T {\n    return this.als.run(context, fn);\n  }\n  \n  static runAsync<T>(context: LogContext, fn: () => Promise<T>): Promise<T> {\n    return this.als.run(context, fn);\n  }\n  \n  static current(): LogContext | undefined {\n    return this.als.getStore();\n  }\n  \n  static merge(context: LogContext): LogContext {\n    const current = this.current();\n    return current ? { ...current, ...context } : context;\n  }\n}\n\n// HTTP middleware integration (framework-agnostic example)\nimport { randomUUID } from 'node:crypto';\n\nexport function createRequestLogger(\n  req: { id?: string; method: string; path: string; get(header: string): string | undefined },\n  res: unknown,\n  next: () => void\n) {\n  const requestContext: LogContext = {\n    operation: 'http-request',\n    requestId: req.id || randomUUID(),\n    metadata: {\n      method: req.method,\n      path: req.path,\n      userAgent: req.get('User-Agent')\n    }\n  };\n  \n  ContextManager.run(requestContext, () => {\n    (req as any).logger = getLogger().child({ component: 'http-handler' });\n    next();\n  });\n}\n```\n\n---\n\n## Performance Enhancements\n\n### Async Logging with Buffering\n\n```typescript\nexport class AsyncLogBuffer {\n  private buffer: LogEntry[] = [];\n  private flushTimer: NodeJS.Timer | null = null;\n  \n  constructor(\n    private maxSize: number = 1000,\n    private flushInterval: number = 5000\n  ) {}\n  \n  add(entry: LogEntry): void;\n  flush(): Promise<void>;\n  private autoFlush(): void;\n}\n```\n\n### Performance Monitoring\n\n```typescript\nexport class PerformanceTracker {\n  static trackOperation<T>(\n    operation: string,\n    fn: () => Promise<T>,\n    logger: Logger\n  ): Promise<T>;\n  \n  static createTimer(operation: string, logger: Logger): Timer;\n  \n  // Memory usage tracking\n  static trackMemory(logger: Logger): void;\n  \n  // Log volume metrics  \n  static getLogMetrics(): {\n    totalLogs: number;\n    logsByLevel: Record<LogLevel, number>;\n    errorRate: number;\n    avgResponseTime: number;\n  };\n}\n```\n\n---\n\n## Testing Utilities\n\n### Mock Logger\n\n```typescript\nexport class MockLogger implements Logger {\n  public logs: LogEntry[] = [];\n  public capturedErrors: Array<{ error: unknown; options?: ErrorCaptureOptions }> = [];\n  \n  // Implement all Logger methods with recording\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  captureError(error: unknown, options?: ErrorCaptureOptions): void {\n    this.capturedErrors.push({ error, options });\n    // Also create a log entry for the error\n    const entry: LogEntry = {\n      timestamp: Date.now(),\n      level: options?.logLevel || LogLevel.ERROR,\n      message: options?.message || (error instanceof Error ? error.message : String(error)),\n      context: options?.context,\n      error: error instanceof Error ? {\n        name: error.name,\n        message: error.message,\n        stack: error.stack?.split('\\n'),\n        code: (error as any).code\n      } : undefined,\n      service: { name: 'test-service' },\n      pid: process.pid\n    };\n    this.logs.push(entry);\n  }\n  \n  // Test utilities\n  findLogsByLevel(level: LogLevel): LogEntry[];\n  findLogsByComponent(component: string): LogEntry[];\n  findLogsByOperation(operation: string): LogEntry[];\n  hasErrorWithCode(code: string): boolean;\n  clearLogs(): void {\n    this.logs.length = 0;\n    this.capturedErrors.length = 0;\n  }\n}\n\nexport class LoggerTestUtils {\n  static createMockLogger(): MockLogger;\n  static createTestConfig(): LoggerConfig;\n  static waitForLogs(logger: MockLogger, count: number, timeout?: number): Promise<void>;\n}\n```\n\n### Integration Test Helpers\n\n```typescript\nexport class LoggerIntegrationTests {\n  static async testFileRotation(config: LoggerConfig): Promise<TestResult>;\n  static async testErrorHandling(config: LoggerConfig): Promise<TestResult>;\n  static async testPerformance(config: LoggerConfig): Promise<PerformanceResult>;\n  static async testMonitoringIntegration(config: LoggerConfig): Promise<TestResult>;\n}\n```\n\n---\n\n## Usage Examples\n\n### CLI Tool Usage\n\n```typescript\nimport { createCliLogger, getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({ \n  verbose: process.argv.includes('--verbose'),\n  format: 'pretty' \n});\n\n// Component-scoped logger using import.meta.url\nconst componentLogger = getComponentLogger(import.meta.url);\n\nasync function processFiles(files: string[]) {\n  const timer = componentLogger.startTimer('process-files');\n  \n  try {\n    componentLogger.info(`Processing ${files.length} files`);\n    \n    for (const file of files) {\n      const fileLogger = componentLogger.withContext({ \n        operation: 'process-file',\n        metadata: { filename: file }\n      });\n      \n      await processFile(file, fileLogger);\n    }\n    \n    timer.end({ metadata: { filesProcessed: files.length } });\n  } catch (error) {\n    captureError(error, {\n      message: 'Failed to process files',\n      context: { metadata: { files } },\n      reportToMonitoring: true\n    });\n    throw error;\n  }\n}\n```\n\n### Service Usage\n\n```typescript\nimport { createServiceLogger, getComponentLogger, ContextManager } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('semantic-flow-api', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: process.env.NODE_ENV\n});\n\n// Component-specific logger for this module\nconst componentLogger = getComponentLogger(import.meta.url);\n\napp.use((req, res, next) => {\n  const requestContext = {\n    operation: 'api-request',\n    requestId: req.id,\n    metadata: {\n      method: req.method,\n      path: req.path,\n      userAgent: req.get('User-Agent')\n    }\n  };\n  \n  ContextManager.run(requestContext, () => {\n    req.logger = componentLogger.child({ component: 'http-handler' });\n    next();\n  });\n});\n```\n\n### Error Handling\n\n```typescript\nimport { captureError, withErrorHandling, ErrorRecoveryStrategy } from '@semantic-flow/logging';\n\n// Simple error capture (logging only)\nasync function riskyOperation() {\n  try {\n    await doSomethingRisky();\n  } catch (error) {\n    captureError(error, {\n      message: 'Risky operation failed',\n      context: { operation: 'risky-op' },\n      includeStackTrace: true\n    });\n    // Continue with fallback logic\n  }\n}\n\n// Combined error handling with recovery\nconst result = await withErrorHandling(\n  () => callExternalAPI(),\n  {\n    capture: {\n      context: { operation: 'external-api-call' },\n      reportToMonitoring: true\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.RETRY,\n      retryCount: 3,\n      fallbackValue: null,\n      retryDelay: 1000\n    }\n  }\n);\n\n// AsyncLocalStorage context propagation with ESM\nconst componentLogger = getComponentLogger(import.meta.url);\n\nContextManager.runAsync(\n  { operation: 'batch-process', userId: 'user123' },\n  async () => {\n    // All logging within this scope automatically includes the context\n    componentLogger.info('Starting batch process'); // Automatically includes operation + userId + component\n    \n    await processItems();\n    \n    componentLogger.info('Batch process completed');\n  }\n);\n\n// Dynamic import example for configuration loading\nasync function loadConfig() {\n  try {\n    const configModule = await import('./config.js');\n    return configModule.default;\n  } catch (error) {\n    captureError(error, {\n      message: 'Failed to load configuration module',\n      context: { operation: 'config-load' }\n    });\n    throw error;\n  }\n}\n```\n\n---\n\n## Migration Guide\n\n### From Deno Implementation\n\n1. **Logger Initialization**: Use `initLogger()` at startup, then `getLogger()` or `getComponentLogger(import.meta.url)` anywhere\n2. **Error Handling**: Replace both `handleError` and `handleCaughtError` with `captureError()` + optional recovery\n3. **Configuration**: Use schema-based config with singleton pattern\n4. **Context Management**: Use `AsyncLocalStorage` for automatic context propagation\n5. **Async Methods**: All logging methods are synchronous (with internal async buffering)\n6. **Context Creation**: Use immutable `child()` loggers instead of mutable context updates\n\n### Breaking Changes\n\n- Log levels changed to enum with numeric values\n- Context merging behavior simplified\n- File rotation configuration consolidated\n- Sentry integration moved to generic monitoring channel\n\n---\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure (Week 1-2)\n- [ ] Type definitions and interfaces\n- [ ] Basic logger implementation\n- [ ] Console channel implementation\n- [ ] Configuration system\n- [ ] Unit tests for core functionality\n\n### Phase 2: Advanced Features (Week 3-4)\n- [ ] File channel with rotation\n- [ ] Monitoring channel (Sentry)\n- [ ] Unified error handling\n- [ ] Performance tracking\n- [ ] Integration tests\n\n### Phase 3: CLI/Service Integration (Week 5)\n- [ ] CLI logger factory\n- [ ] Service logger factory  \n- [ ] Context management utilities\n- [ ] Documentation and examples\n\n### Phase 4: Testing & Documentation (Week 6)\n- [ ] Comprehensive test suite\n- [ ] Performance benchmarks\n- [ ] Migration documentation\n- [ ] API documentation\n\n#### Detailed Test Plan\n\n**Core Functionality Tests:**\n- **Deterministic clocks**: Inject `now()` function into logger; use Vitest fake timers for predictable timestamps\n- **Rotation tests**: Force size-based rotation with tiny `maxSize`; assert new file created and file descriptor reopened\n- **Signal tests**: Simulate SIGINT/SIGTERM with child process; assert `flush()` called before exit\n- **No-throw contract**: Intentionally throw inside a channel's `write` method; ensure logger falls back to console and does not crash caller\n- **ESM component detection**: Test `getComponentLogger(import.meta.url)` with various file paths and extensions\n\n**Advanced Feature Tests:**\n- **Monitoring timeouts**: Stub adapter with delayed promise; assert drop counters increment when deadlines exceeded\n- **ALS propagation**: Assert `ContextManager.current()` context merged into log entries across `await` points\n- **Recursive logging protection**: Trigger error within logging code; verify fallback to console.error without infinite loops\n- **Back-pressure handling**: Fill buffers beyond capacity; verify graceful degradation and dropped message counts\n- **Pure ESM integrity**: Verify no `require()` calls in bundled output; test dynamic import usage\n\n---\n\n## Success Criteria\n\n1. **Performance**: 50% faster than current Deno implementation\n2. **Memory Usage**: 30% lower memory footprint\n3. **API Simplicity**: 40% fewer public methods/functions\n4. **Test Coverage**: 95% code coverage\n5. **Documentation**: Complete API docs and usage examples\n6. **Migration**: Clear migration path from Deno version\n\n---\n\n## Production Considerations\n\n### JSON Lines Output Format\n\nAll structured log output follows the JSON Lines format (newline-delimited JSON) for ingestion by log processors like FluentBit, Loki, or Elasticsearch:\n\n```typescript\n// Each log entry is a single JSON object terminated by \\n\n{\"timestamp\":1699027200000,\"level\":20,\"message\":\"Server started\",\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n{\"timestamp\":1699027201000,\"level\":40,\"message\":\"Database connection failed\",\"error\":{\"name\":\"Error\",\"message\":\"Connection timeout\"},\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n```\n\n### Flush Guarantees\n\nThe `flush()` method provides the following guarantees:\n- Drains buffers of all enabled channels within a bounded time (default 5s timeout)\n- Never throws - errors are logged to console as fallback\n- Returns when all pending entries are written or timeout is reached\n- Safe to call multiple times concurrently\n\n### Critical Path Reliability\n\n```typescript\n// Synchronous logging for ERROR/FATAL ensures immediate output\nfunction logSyncCritical(entry: LogEntry): void {\n  const line = JSON.stringify({\n    t: entry.timestamp,\n    lvl: entry.level,\n    msg: entry.message,\n    rid: entry.context?.requestId\n  }) + '\\n';\n  \n  try { \n    process.stderr.write(line); \n  } catch {\n    // Never throw from logging - best effort only\n  }\n  \n  // Optional: Synchronous file write for critical errors\n  // if (criticalFd) fs.writeSync(criticalFd, line);\n}\n\n// Safe JSON serialization with limits and circular protection\nconst MAX_CTX = 20_000, MAX_MSG = 2_000, MAX_STACK = 50;\n\nfunction safeStringify(value: any, maxLength = MAX_CTX): string {\n  try {\n    const seen = new WeakSet();\n    const result = JSON.stringify(value, (key, val) => {\n      if (typeof val === 'object' && val !== null) {\n        if (seen.has(val)) return '[Circular]';\n        seen.add(val);\n      }\n      if (typeof val === 'string' && val.length > MAX_MSG) {\n        return val.slice(0, MAX_MSG) + '...[truncated]';\n      }\n      return val;\n    });\n    return result.length > maxLength ? result.slice(0, maxLength) + '...[truncated]' : result;\n  } catch {\n    return '[Unserializable]';\n  }\n}\n\n// Strip ANSI escape codes for non-TTY output\nfunction stripAnsi(text: string): string {\n  return text.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// PII redaction hook (optional per channel)\ntype RedactFn = (key: string, value: any) => any;\ntype RedactConfig = string[] | RedactFn;\n\nfunction applyRedaction(entry: LogEntry, redact?: RedactConfig): LogEntry {\n  if (!redact) return entry;\n  \n  if (Array.isArray(redact)) {\n    // Field-based redaction\n    const redactFields = new Set(redact);\n    return JSON.parse(JSON.stringify(entry, (key, value) => \n      redactFields.has(key) ? '[REDACTED]' : value\n    ));\n  } else {\n    // Custom redaction function\n    return JSON.parse(JSON.stringify(entry, redact));\n  }\n}\n\n// Ensure JSON lines contain no ANSI codes\nfunction formatJsonLine(entry: LogEntry, redact?: RedactConfig): string {\n  const redacted = applyRedaction(entry, redact);\n  const serialized = safeStringify(redacted);\n  return stripAnsi(serialized) + '\\n';\n}\n```\n\n### Graceful Shutdown\n\n```typescript\nconst SAFE_CLOSE_MS = 2000;\n\n// Comprehensive process termination hooks\nprocess.on('beforeExit', async () => {\n  await getLogger().flush();\n});\n\nprocess.on('SIGTERM', () => void gracefulExit(0));\nprocess.on('SIGINT', () => void gracefulExit(130));\n\nprocess.on('uncaughtException', (error) => {\n  // Sync-log minimal line to stderr first\n  const line = JSON.stringify({\n    t: Date.now(),\n    lvl: LogLevel.FATAL,\n    msg: 'Uncaught exception - process terminating',\n    err: error.message\n  }) + '\\n';\n  try { process.stderr.write(line); } catch {}\n  \n  // Then capture full error context\n  captureError(error, { \n    logLevel: LogLevel.FATAL,\n    message: 'Uncaught exception - process terminating',\n    includeStackTrace: true \n  });\n  void gracefulExit(1);\n});\n\nprocess.on('unhandledRejection', (reason) => {\n  captureError(reason as unknown, { \n    logLevel: LogLevel.ERROR,\n    message: 'Unhandled promise rejection',\n    includeStackTrace: true \n  });\n});\n\nfunction safeGetLogger(): Logger | undefined {\n  try {\n    return getLogger();\n  } catch {\n    return undefined;\n  }\n}\n\nasync function gracefulExit(code: number) {\n  try {\n    const logger = safeGetLogger();\n    if (logger) {\n      // Race between proper close and timeout\n      await Promise.race([\n        logger.close(),\n        new Promise(resolve => setTimeout(resolve, SAFE_CLOSE_MS))\n      ]);\n    }\n  } catch {\n    // Best effort - don't block exit\n  } finally {\n    process.exit(code);\n  }\n}\n```\n\n### Guard Against Recursive Logging\n\n```typescript\n// Use AsyncLocalStorage to prevent recursive logging per async context\nconst recursionGuard = new AsyncLocalStorage<boolean>();\n\nclass LoggerImpl implements Logger {\n  private log(level: LogLevel, message: string, error?: Error, context?: LogContext): void {\n    if (recursionGuard.getStore()) {\n      // Prevent infinite loops in error handling - use direct console\n      console.error('Recursive logging detected:', message);\n      return;\n    }\n    \n    recursionGuard.run(true, () => {\n      this.writeToChannels(level, message, error, context);\n    });\n  }\n  \n  trace(message: string, context?: LogContext): void {\n    this.log(LogLevel.TRACE, message, undefined, context);\n  }\n  \n  debug(message: string, context?: LogContext): void {\n    this.log(LogLevel.DEBUG, message, undefined, context);\n  }\n  \n  info(message: string, context?: LogContext): void {\n    this.log(LogLevel.INFO, message, undefined, context);\n  }\n  \n  warn(message: string, context?: LogContext): void {\n    this.log(LogLevel.WARN, message, undefined, context);\n  }\n  \n  error(message: string, error?: Error, context?: LogContext): void {\n    this.log(LogLevel.ERROR, message, error, context);\n  }\n  \n  fatal(message: string, error?: Error, context?: LogContext): void {\n    this.log(LogLevel.FATAL, message, error, context);\n  }\n  \n  private writeToChannels(level: LogLevel, message: string, error?: Error, context?: LogContext): void {\n    // Implementation that might trigger errors and recursive logging\n  }\n}\n```\n\n---\n\n## Understanding Error Recovery Strategies\n\nError recovery strategies control **what happens after an error is logged**, not the logging itself:\n\n```typescript\n// Example: Processing a batch of files\nfor (const file of files) {\n  await withErrorHandling(\n    () => processFile(file),\n    {\n      capture: { \n        message: `Failed to process ${file}`,\n        context: { operation: 'file-processing', filename: file }\n      },\n      recovery: {\n        strategy: ErrorRecoveryStrategy.CONTINUE, // Keep processing other files\n        // Don't let one bad file stop the whole batch\n      }\n    }\n  );\n}\n\n// Example: Critical database connection\nconst db = await withErrorHandling(\n  () => connectToDatabase(),\n  {\n    capture: {\n      message: 'Database connection failed',\n      reportToMonitoring: true\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.RETRY,\n      retryCount: 5,\n      retryDelay: 2000,\n      // Keep trying - app can't work without DB\n    }\n  }\n);\n\n// Example: Optional feature that might fail\nconst userPreferences = await withErrorHandling(\n  () => loadUserPreferences(userId),\n  {\n    capture: {\n      message: 'Failed to load user preferences',\n      context: { userId }\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.FALLBACK,\n      fallbackValue: DEFAULT_PREFERENCES,\n      // App works fine with defaults\n    }\n  }\n);\n```\n\n---\n\n## Questions for Review\n\n1. Should error recovery strategies be pluggable?\n\n---\n\n## Future Enhancements (v2+)\n\nItems suggested in review but deferred for initial implementation:\n\n1. **Plugin Architecture**: `logger.use(plugin)` for extensible middleware\n2. **Circuit Breaker**: Automatic monitoring channel fallback during outages  \n3. **ErrorClassifier Registry**: Pluggable error classification rules\n4. **Advanced Performance Tracking**: Built-in metrics collection and analysis\n5. **Log Event Streaming**: Global event emitter for real-time log processing\n6. **Pluggable Formatters**: Custom format registration system\n7. **Advanced Context Features**: Structured event fields, correlation IDs\n8. **Log Compression**: Automatic compression of rotated log files\n9. **Source Location Capture**: Optional `file`, `line`, `col` fields from stack traces in debug builds\n10. **Custom Formatters**: `registerFormatter(name, fn)` for pluggable output formats beyond json/pretty/compact\n\nThese features can be added incrementally based on usage patterns and requirements.\n\n### Nice-to-Have Before v1\n\n- **Monitoring Timeouts**: Per-entry deadlines and token-bucket rate limiting for monitoring channels\n- **Enhanced Error Context**: Automatic source location capture in development mode\n- **Advanced Sampling**: Intelligent sampling based on error patterns and frequency\n\n---\n\n## Critical Review Summary\n\n**ChatGPT's feedback addressed key architectural issues:**\n\n✅ **Fixed**: Async/sync API inconsistency  \n✅ **Fixed**: Split error handling into capture + recovery concerns  \n✅ **Fixed**: Added LogChannel interface for extensibility  \n✅ **Fixed**: Added synchronous critical path for ERROR/FATAL  \n✅ **Fixed**: Added AsyncLocalStorage for context propagation  \n✅ **Fixed**: Added singleton pattern with DI support  \n✅ **Added**: Error class factory to reduce boilerplate  \n✅ **Added**: Context immutability clarification  \n✅ **Added**: Production reliability safeguards  \n\n📋 **Deferred**: Plugin architecture, circuit breakers, advanced features\n\nThe specification now provides a solid foundation that can be extended incrementally.\n\n---\n\n---\n\n## Packaging and Runtime Requirements\n\n### Node.js Support\n- **Minimum version**: Node.js >=24 (requires `AsyncLocalStorage` and `fs.writev`)\n- **Module format**: **Pure ESM** only. All published JS is ES Modules. No CommonJS build.\n- **Package metadata**:\n  ```json\n  {\n    \"name\": \"@semantic-flow/logging\",\n    \"version\": \"1.0.0\",\n    \"type\": \"module\",\n    \"main\": \"./dist/index.js\",\n    \"module\": \"./dist/index.js\",\n    \"types\": \"./dist/index.d.ts\",\n    \"exports\": {\n      \".\": {\n        \"types\": \"./dist/index.d.ts\",\n        \"import\": \"./dist/index.js\"\n      },\n      \"./package.json\": \"./package.json\"\n    },\n    \"engines\": { \"node\": \">=18.17\" }\n  }\n  ```\n\n### TypeScript Configuration\n- **Compile ESM-only types**:\n  ```json\n  // tsconfig.build.json\n  {\n    \"compilerOptions\": {\n      \"module\": \"ES2022\",\n      \"target\": \"ES2022\",\n      \"moduleResolution\": \"bundler\",\n      \"declaration\": true,\n      \"declarationMap\": true,\n      \"emitDeclarationOnly\": false,\n      \"outDir\": \"dist\",\n      \"sourceMap\": true,\n      \"inlineSources\": true,\n      \"verbatimModuleSyntax\": true,\n      \"exactOptionalPropertyTypes\": true,\n      \"lib\": [\"ES2022\"]\n    },\n    \"include\": [\"src\"]\n  }\n  ```\n\n### Runtime ESM Hygiene\n- **No `require()`**: Use `import`/`import()` everywhere\n- **Replace `__dirname/__filename`**:\n  ```typescript\n  import { fileURLToPath } from \"node:url\";\n  import { dirname } from \"node:path\";\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = dirname(__filename);\n  ```\n- **JSON imports**:\n  ```typescript\n  import schema from \"./schema.json\" with { type: \"json\" };\n  ```\n- **CLI entry point**:\n  ```javascript\n  #!/usr/bin/env node\n  import { run } from '../dist/cli.js';\n  run();\n  ```\n\n### Consumer Requirements\n- **Pure ESM consumers**: Standard `import` statements\n- **CommonJS consumers**: Must use dynamic `import('@semantic-flow/logging')`\n- **Tree-shaking**: Channels kept in separate files for optimal bundling\n\n### Testing (Vitest)\n```typescript\n// vitest.config.ts\nimport { defineConfig } from 'vitest/config';\nexport default defineConfig({\n  test: {\n    environment: 'node',\n    isolate: true\n  }\n});\n```\n\n### CI/CD Requirements\n- **JSON Lines validation**: Lint commit examples to ensure valid JSONL with newline at end\n- **Console hygiene**: Enforce no `console.log` in src except inside `ConsoleChannel` or emergency fallback\n- **ESM purity check**: Block accidental CJS fields in package.json or `*.cjs` files in dist/\n- **Type checking**: Ensure all examples compile without errors\n- **Performance benchmarks**: Automated benchmarks comparing against previous versions\n\n### Build Tool Configuration\n\n**TypeScript Compiler (tsc)**:\n```bash\ntsc --project tsconfig.build.json\n```\n\n**tsup**:\n```bash\ntsup src/index.ts --format esm --dts --sourcemap\n```\n\n**Rollup**:\n```javascript\nexport default {\n  input: 'src/index.ts',\n  output: {\n    file: 'dist/index.js',\n    format: 'esm',\n    sourcemap: true\n  }\n};\n```\n\n**esbuild**:\n```javascript\nesbuild --bundle src/index.ts --format=esm --outfile=dist/index.js --sourcemap\n```\n\n### Optional Runtime Guardrails\n```typescript\n// Add once at package init to catch CJS usage\nif (typeof require !== 'undefined') {\n  throw new Error('This package is ESM-only. Use import().');\n}\n```\n\n## TODO\n\n### Phase 1: Core Infrastructure ✅ COMPLETE\n[x] Create the `@semantic-flow/logging` package structure (directories, `package.json`, `tsconfig.json`).\n[x] Implement type definitions and interfaces in `shared/logging/src/core/types.ts`.\n[x] Implement error types and factory in `shared/logging/src/errors/types.ts` and `shared/logging/src/errors/factory.ts`.\n[x] Implement the `ConsoleChannel` and `LogChannel` interface.\n[x] Implement the core `Logger` interface and basic `LoggerImpl`.\n[x] Implement the configuration system.\n[x] Implement main exports and factory functions.\n[x] Implement unit tests for core functionality.\n[x] Developer documentation created in `documentation/dev.logging-and-error-handling.md`.\n\n### Phase 1 Enhancement: Function Name Capture ✅ COMPLETE\n[x] Implement automatic function/method name capture in log context.\n[x] Add configuration option to enable/disable function name capture.\n[x] Update developer documentation with function name capture feature.\n[x] Add unit tests for function name capture.\n\n**Implementation Details:**\n- Added `function?: string` field to [`LogContext`](shared/logging/src/core/types.ts:22) interface\n- Added `captureFunctionName: boolean` to `autoContext` configuration (default: `NODE_ENV !== 'production'`)\n- Implemented stack trace parsing in [`shared/logging/src/utils/stack-trace.ts`](shared/logging/src/utils/stack-trace.ts:1)\n- Integrated capture into [`LoggerImpl.log()`](shared/logging/src/core/logger.ts:137) method\n- Updated [`documentation/dev.logging-and-error-handling.md`](documentation/dev.logging-and-error-handling.md:1) with comprehensive documentation\n- Added environment variable `SF_LOG_AUTO_CONTEXT_CAPTURE_FUNCTION_NAME`\n- All 11 unit tests passing in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1)\n\n**Performance Considerations:**\n- Function name capture is environment-aware (disabled in production by default)\n- Uses stack trace parsing which has a measurable performance cost\n- Can be explicitly enabled/disabled via configuration\n- Skips correct number of stack frames to capture the actual calling function\n\n### Phase 2: Advanced Features (Planned)\n[ ] File channel with rotation\n[ ] Monitoring channel (Sentry)\n[ ] Unified error handling (capture + recovery)\n[ ] Performance tracking enhancements\n[ ] Integration tests\n\n\n## Decision\n\n- modern ESM only, targeting NodeJS >=24\n\n\n## Summary\n\nPhase 1: Core Infrastructure for the Shared Logging and Error Handling System (`@semantic-flow/logging`) is complete, following the specification in [`documentation/task.2025-11-04-shared-logging-and-errorhandling.md`](documentation/task.2025-11-04-shared-logging-and-errorhandling.md:1).\n\n## Phase 1 Accomplishments\n\n1.  **Package Setup:** Created the `shared/logging/` package structure, including `package.json` and `tsconfig.json` configured for Pure ESM and Node.js >=18.17.\n2.  **Core Types:** Implemented all core types and interfaces in [`shared/logging/src/core/types.ts`](shared/logging/src/core/types.ts:1), including `LogLevel`, `LogEntry`, `LogContext`, `LogChannel`, and `LoggerConfig`.\n3.  **Error Handling:** Implemented the base [`SemanticFlowError`](shared/logging/src/errors/types.ts:4) class and the `createErrorType` factory function in [`shared/logging/src/errors/types.ts`](shared/logging/src/errors/types.ts:1).\n4.  **Configuration:** Implemented the configuration schema, default values, and the `ConfigLoader` class with deep merging and log level parsing in `shared/logging/src/config/`.\n5.  **Console Channel:** Implemented the synchronous [`ConsoleChannel`](shared/logging/src/channels/console.ts:9) and necessary formatting utilities (`safeStringify`, `stripAnsi`, `formatCritical`) in [`shared/logging/src/channels/console.ts`](shared/logging/src/channels/console.ts:1) and [`shared/logging/src/core/formatters.ts`](shared/logging/src/core/formatters.ts:1).\n6.  **Core Logger:** Implemented the [`Logger`](shared/logging/src/core/logger.ts:23) interface and [`LoggerImpl`](shared/logging/src/core/logger.ts:91) class, including auto-context generation, channel dispatch, and basic timer functionality.\n7.  **Context Management:** Implemented the [`ContextManager`](shared/logging/src/core/context.ts:12) using `AsyncLocalStorage` for context propagation.\n8.  **Factory Functions:** Implemented the singleton pattern (`initLogger`, `getLogger`) and specialized factory functions (`createCliLogger`, `createServiceLogger`) in [`shared/logging/src/core/logger.ts`](shared/logging/src/core/logger.ts:1).\n9.  **Exports:** Consolidated all public exports in [`shared/logging/src/index.ts`](shared/logging/src/index.ts:1).\n10. **Unit Tests:** Implemented core unit tests in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1) using a `MockLogger` utility, covering singleton behavior, context propagation, and error handling.\n\n## function/method name capture, addressed all CodeRabbit suggestions, and integrated the logging system into sflo-host\n\n### 1. Function Name Capture Feature ✅\n- Added [`function?: string`](shared/logging/src/core/types.ts:22) field to `LogContext`\n- Implemented stack trace parsing in [`shared/logging/src/utils/stack-trace.ts`](shared/logging/src/utils/stack-trace.ts:1)\n- Integrated into [`LoggerImpl.log()`](shared/logging/src/core/logger.ts:139) with environment-aware defaults\n- Comprehensive documentation in [`documentation/dev.logging-and-error-handling.md`](documentation/dev.logging-and-error-handling.md:108)\n\n### 2. CodeRabbit Critical Fixes ✅\n\n1. **Channel Sharing** - Fixed resource leak where child loggers created duplicate channels\n2. **Async Logger Cleanup** - Made `initLogger()` async to properly await channel closure\n3. **Test Reset Cleanup** - Made `__resetLoggerForTests()` async with proper channel cleanup\n4. **CLI Tool Name Safety** - Added try-catch for `fileURLToPath()` edge cases\n5. **Test Config Isolation** - Fixed `createTestConfig()` to return deep copy\n6. **Race Condition Fix** - Fixed `waitForLogs()` with proper timeout cleanup\n7. **Redaction Logic Fix** - Fixed critical bug using `JSON.stringify` replacer correctly\n8. **DoS Protection** - Added `MAX_CTX * 2` upper bound for log entries\n9. **Schema Validation** - Added minimum constraints for `bufferSize`/`flushInterval`\n10. **Comment Clarity** - Fixed misleading async buffering comment\n11. **Code Cleanup** - Removed unused `logs` property from `MockLogger`\n\n### 3. SFLO Host Integration ✅\n\n**Updated Files:**\n- [`sflo-host/package.json`](sflo-host/package.json:15) - Added `@semantic-flow/logging` dependency\n- [`sflo-host/src/index.ts`](sflo-host/src/index.ts:1) - Integrated logging system:\n  - Replaced Fastify's built-in logger with `@semantic-flow/logging`\n  - Added component-scoped logger using `getComponentLogger(import.meta.url)`\n  - Initialized logger at startup with environment-aware configuration\n  - Added graceful shutdown handlers for SIGTERM/SIGINT\n  - Replaced all `app.log` calls with structured logging using `metadata` field\n  - Added proper error handling with `logger.fatal()`\n\n**Integration Features:**\n- Environment-aware log levels (DEBUG in dev, INFO in prod)\n- Pretty formatting in development, JSON in production\n- Component name automatically captured as 'index'\n- Graceful shutdown with log flushing\n- Structured logging with metadata for all application events\n\n### Test Results ✅\nAll 11 unit tests passing in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1)\n\n### TypeScript Compilation ✅\n- Logging package builds successfully\n- SFLO Host compiles without errors\n- All workspace dependencies linked correctly\n\nThe `@semantic-flow/logging` package is now production-ready and fully integrated into the SFLO Host application!\n","n":0.014}}},{"i":15,"$":{"0":{"v":"Progress","n":1},"1":{"v":"\n\n- [[completed.2025-11-03-optimizing-for-agents]] - Initial Memory Bank Structure\n","n":0.378}}},{"i":16,"$":{"0":{"v":"products","n":1}}},{"i":17,"$":{"0":{"v":"sflo-host","n":1},"1":{"v":"\n- supports the official clients ([[product.cli]] and [[product.plugins.sflo-web]]) and third-party clients by\n  - providing an [[product.plugins.sflo-api]]\n  - serving meshes via a contained static http service\n  - keeping central control of mesh operations.\n    - e.g. locking sub-meshes during weave.\n    - enables parallelism, which can speed up the weave and ensure processes don't clobber each others' work. It can also provide locking to allow multi-user or multi-client modification.\n\n## Functions\n\n- file watchers\n- file locking\n\n","n":0.119}}},{"i":18,"$":{"0":{"v":"Plugins","n":1}}},{"i":19,"$":{"0":{"v":"Sparql Upd","n":0.707}}},{"i":20,"$":{"0":{"v":"Sparql Readonly","n":0.707}}},{"i":21,"$":{"0":{"v":"Sparql Editor","n":0.707}}},{"i":22,"$":{"0":{"v":"sflo-web","n":1},"1":{"v":"\n\n## Unexamined Technical Architecture\n\n- server-rendered + HTMX approach\n\n- Development workflow:\n  - Use `pnpm` to install dependencies\n  - Run `vite` dev server for local development with hot module replacement\n  - Build production assets with `vite build`\n  - Output static assets to `/sflo-web/dist` for deployment or serving \n- Recommended dependencies:\n  - `fetch` for API calls to `sflo-api`\n\n- Testing:\n  - Use `vitest` for unit and integration tests\n  - Use `cypress` or `playwright` for end-to-end testing\n\n- Linting and formatting:\n  - Use `eslint` with Vue 3 plugin\n  - Use `prettier` for code formatting\n","n":0.107}}},{"i":23,"$":{"0":{"v":"sflo-api plugin","n":0.707},"1":{"v":"\n\nUse noun URLs that mirror the mesh's filesystem. Bytes go to `_working`. Versioning and \"current flips\" happen on weave. All flows except [[mesh-resource.node-component.flow.node-metadata]] support arbitrary PATCH. System-only fields in `_node_metadata_flow` are rejected on write.\n\n## Conventions\n\n* Base: `/api/{mesh}/{nodePath}/…` where `{resourcePath}` is greedy and slash-separated.\n* Reserved flow directories under a node:\n\n  * `_node_metadata_flow/` (required)\n  * `_cfg-op/`\n  * `_cfg-inh/`\n  * `_ref/`\n  * `_payload/`  ← payload dataset for payload nodes\n* Snapshot layout under any flow:\n\n  * `_snapshots/{vN}/_dist/{files…}`\n  * `_default/` → pointer to a snapshot (folder or file)\n  * `_working/`   → working content before weave\n* Headers:\n\n  * `Idempotency-Key` on PUT/POST of bytes or creators\n  * `ETag` on reads; `If-Match` on pointer changes\n  * Optional `Content-Digest: sha-256=:…:` on bytes\n* Media types:\n\n  * JSON-LD bytes: `application/ld+json`\n  * TriG bytes: `application/trig`\n  * Merge patch: `application/merge-patch+json` (RFC 7396) over **framed JSON-LD**\n* Validation:\n\n  * PUT/PATCH: **syntactic only** (parse). SHACL runs during weave.\n* Errors: `application/problem+json` or `…+json+ld` with `type`, `detail`, `instance`.\n\n## Events (SSE)\n\n`GET /api/{mesh}/events`\n\n* `fs.change { paths:[], iris:[] }`\n* `job.progress { job, pct, msg }`\n* `job.done { job, artifacts:[…], changed:{ paths, iris } }`\n\n## Mesh registry\n\n* `POST /api/meshes`\n\n  * Body: `{ \"name\": \"test-ns\", \"path\": \"./test-ns\" }`\n  * `201 Created` + `Location: /api/test-ns/`\n* `GET /api/meshes` → list\n\n## Resources\n\n### Nodes\n\n#### `GET /api/{mesh}/{nodePath}/`\n\nProbably Returns:\n\n- A list of its components (including flows) — This is important to understand what building blocks or sub-resources the node contains.\n- A list and count of its child nodes — Useful for navigation and understanding the node hierarchy.\n- Its node type (probably computed) — Helps clients understand the nature or classification of the node.\n- HATEOAS links to related resources like flows, snapshots, jobs, resource pages and other documentation resources, asset trees — Enables discoverability and navigation.\n\n\nMaybe returns:\n- backlinks (references to this node from other places in the mesh)\n- some metadata, especially non-semantic metadata like filesystem creation/modification timestamps, filesystem permissions\n- status flags like whether _working has diverged\n\n* **Dataset upload (bytes to `_working`)**\n\n  * `PUT /api/{mesh}/{nodePath}/_payload/_working/{nodeName}.jsonld`\n\n    * Body: JSON-LD (or TriG variant if you standardize a filename)\n    * Effects:\n\n      * Bare → becomes payload node\n      * Reference → becomes Reference+Dataset\n      * Dataset → replaces `_working`\n    * `201 Created` (new content) or `200/204` (duplicate); `Content-Location` echoes the `_working` URL\n* **List current distributions**\n\n  * `GET /api/{mesh}/{nodePath}/_payload/_default/` → array of files\n* **Fetch a current distribution**\n\n  * `GET /api/{mesh}/{nodePath}/_payload/_default/{filename}` → bytes\n* **List snapshots**\n\n  * `GET /api/{mesh}/{nodePath}/_payload/_snapshots/`\n* **Snapshot metadata**\n\n  * `GET /api/{mesh}/{nodePath}/_payload/_snapshots/{vN}` → JSON-LD summary\n* **Snapshot distributions**\n\n  * `GET /api/{mesh}/{nodePath}/_payload/_snapshots/{vN}/_dist/`\n  * `GET /api/{mesh}/{nodePath}/_payload/_snapshots/{vN}/_dist/{filename}`\n\n## Flows (common to all five kinds)\n\n* **Flow summary**\n\n  * `GET /api/{mesh}/{nodePath}/_{flowKind}/`\n    `flowKind ∈ { metadata-flow, op_config_flow, inheritable_config_flow, reference_flow, payload-flow }`\n* **Create snapshot from `_working` (server constructs version)**\n\n  * `POST /api/{mesh}/{nodePath}/_{flowKind}/_snapshots/`\n\n    * Body: `{ \"source\": \"next\", \"note\": \"…\" }` (optional)\n    * Fast: `201 Location: …/_snapshots/{vN}`; Long: `202 Location: /api/{mesh}/jobs/{id}`\n\n### PATCH (config flows)\n\nGoal: “make a couple changes without re-uploading a full file.” We merge **current** with patch → write result to `_working`.\n\n* `PATCH /api/{mesh}/{nodePath}/_{flowKind}/_working/`\n\n  * Allowed `flowKind`: `op_config_flow`, `inheritable_config_flow` (and optionally others with JSON-LD content)\n  * `Content-Type: application/merge-patch+json`\n  * Semantics:\n\n    1. Server reads `_default` distribution (JSON-LD framed DTO)\n    2. Applies RFC 7396 merge patch\n    3. Writes the merged document to `_working/` as JSON-LD\n  * Response:\n\n    * `201 Created` with `Content-Location: …/_{flowKind}/_working/`\n    * Emits `fs.change`\n  * **System-only fields** (especially in `_node_metadata_flow`): writes to these keys are **rejected** with `403` (or `422`), response lists offending JSON Pointers\n\n#### Optional PUT for entire JSON-LD next\n\n* `PUT /api/{mesh}/{nodePath}/_{flowKind}/_working/{filename}`\n\n  * Replace `_working` fully with a new JSON-LD file\n\n## Pointer management (promote current)\n\n* `PUT /api/{mesh}/{nodePath}/_{flowKind}/_default/`\n\n  * Body: `{ \"snapshot\": \"vN\" }`\n  * Headers: `If-Match: \"<etag-of-current-pointer>\"`\n  * `200/204` and `fs.change`\n\n## Jobs (noun URLs; body declares type)\n\n* `POST /api/{mesh}/jobs`\n\n  * Example weave:\n\n    ```json\n    {\n      \"@type\": \"sflo:WeaveJob\",\n      \"targets\": [ \"/{nodePath}/\" ],\n      \"flows\": [\"payload-flow\",\"metadata-flow\",\"reference_flow\"],\n      \"promote\": true\n    }\n    ```\n  * `202 Accepted` + `Location: /api/{mesh}/jobs/{id}`\n* `GET /api/{mesh}/jobs/{id}` → status (HTML or JSON-LD)\n* SSE emits progress and completion; on success weave:\n\n  * Validates (SHACL if enabled)\n  * Creates `…/_snapshots/{vN}` from `_working` for addressed flows\n  * Optionally flips `…/_default/` when `promote:true`\n  * Emits `fs.change` with `paths` and `iris`\n\n## HATEOAS (every JSON-LD/HTML response)\n\nMinimum links on a node:\n\n```json\n\"links\": [\n  { \"rel\":\"self\", \"href\":\"/api/{mesh}/{nodePath}/\" },\n  { \"rel\":\"flow\", \"kind\":\"payload-flow\", \"href\":\"/api/{mesh}/{nodePath}/_payload/\" },\n  { \"rel\":\"dataset.uploadNext\", \"href\":\"/api/{mesh}/{nodePath}/_payload/_working/{nodeName}.jsonld\", \"method\":\"PUT\" },\n  { \"rel\":\"flow.patchNext\", \"kind\":\"op_config_flow\", \"href\":\"/api/{mesh}/{nodePath}/_cfg-op/_working/\", \"method\":\"PATCH\", \"type\":\"application/merge-patch+json\" },\n  { \"rel\":\"flow.createSnapshot\", \"kind\":\"payload-flow\", \"href\":\"/api/{mesh}/{nodePath}/_payload/_snapshots/\", \"method\":\"POST\" },\n  { \"rel\":\"job.start\", \"href\":\"/api/{mesh}/jobs\", \"method\":\"POST\", \"expects\":\"sflo:WeaveJob\" }\n]\n```\n\n## Permissions\n\n* System-only properties (esp. `_node_metadata_flow`) are enforced server-side:\n\n  * Attempted write → `403` with list of blocked JSON Pointers\n  * Server may augment or overwrite these during weave\n* Per-mesh RBAC: viewer/editor/admin; enforced on all writes\n\n## Notes and constraints\n\n* No multi-file uploads: `_working` is a single JSON-LD (or TriG) file for payload-flow. \n* PATCH is supported for flows whose `_default` is JSON-LD. Not supported for TriG distributions.\n* All URLs are nouns. No `?op=`. Jobs model compute.\n* API ↔ site symmetry: replacing `/api` with the site host yields the same resource for GETs that return files.\n\n## Open flags to decide (defaults in parentheses)\n\n* Allow PATCH for `reference_flow` and `metadata-flow`? (default: **disallow** for `_node_metadata_flow`, **allow** for `reference_flow` JSON-LD)\n* Enforce `Idempotency-Key` as **required** or **optional** on PUT/PATCH? (recommended: **required**)\n* Return `application/problem+json` or `…+json+ld` for errors? (recommended: **…+json+ld**)\n\nThis spec matches your rules: nouns only, `_working` for bytes, weave does validation + versioning + promotion, and PATCH merges “current → next” for the two config flows (and optionally others) while blocking system-only fields.\n","n":0.034}}},{"i":24,"$":{"0":{"v":"Mesh Server","n":0.707}}},{"i":25,"$":{"0":{"v":"API Docs plugin","n":0.577}}},{"i":26,"$":{"0":{"v":"Core","n":1},"1":{"v":"\n- Used by sflo-host/api and sflo-cli\n- contains cross-cutting functionality\n","n":0.333}}},{"i":27,"$":{"0":{"v":"sflo CLI","n":0.707},"1":{"v":"\n## Stand-alone or Service-backed\n\n- the CLI can function alone, but when used in tandem with the [[product.sflo-host]], it becomes multi-user and multi-threaded\n\n## Commands\n\n","n":0.209}}},{"i":28,"$":{"0":{"v":"Product Ideas","n":0.707}}},{"i":29,"$":{"0":{"v":"Sparql Sync","n":0.707},"1":{"v":"\n- detects changes in a [[concept.semantic-flow-site]] and updates a triple-store as appropriate\n- on demand, scans a triple-store and updates a site as appropriate.","n":0.209}}},{"i":30,"$":{"0":{"v":"Per Node and per Submesh Logging","n":0.408},"1":{"v":"\n- it'd be nice if you could feed changelogs for certain flows to arbitray listeners out there. \n","n":0.236}}},{"i":31,"$":{"0":{"v":"Hateoas Driven API Recipe","n":0.5},"1":{"v":"\n## Your Use Case\n\nYou're not just documenting the API — you're using it:\n\n-   To **operate a mesh manually**, before clients exist.\n    \n-   You need an **executable interface**, not just static documentation.\n    \n-   You want **structured examples** (i.e. **recipes**) that can act as proto-clients or decision-paths.\n    \n-   You lean toward **HATEOAS** style: i.e., user follows links (or rels), maybe inputting small bits along the way.\n    \n\n___\n\n## 🔥 This is _not_ what Scalar is designed for\n\nScalar is a **docs tool**, great for:\n\n-   Rendering OpenAPI reference UIs.\n    \n-   Hosting and styling spec-based endpoints.\n    \n-   **Not** for chaining calls, dynamic flows, or acting like a human-in-the-loop client.\n    \n\nEven Stoplight or RapiDoc start to feel clunky in your context, because what you're actually describing is closer to:\n\n### \\> ✅ A human-usable API client with structured, composable, inspectable **API macros or flows**\n\n___\n\n## 🚨 So yes, you probably need to build something.\n\nBut here's how to think about it critically, so you **don't throw away everything** or reinvent everything either.\n\n___\n\n## 🧠 What You Actually Want: A \"HATEOAS Recipe Runner\"\n\nYou're describing a system that does the following:\n\n| Feature                        | Description                                                    |\n| ------------------------------ | -------------------------------------------------------------- |\n| 🔧 **Embeds real OpenAPI**      | So you get type safety, endpoint listings, schemas, validation |\n| 🧪 **Can execute requests**     | Full HTTP interaction, possibly with state/cookies/token       |\n| 🪜 **Supports recipes/flows**   | Sequence of calls, possibly branching via hypermedia           |\n| 🧵 **Has local state/input**    | To reuse values from previous steps                            |\n| 📎 **HATEOAS link traversal**   | e.g. follow `\"next\"` or `\"create\"` link relations dynamically  |\n| 🧰 **Deno/TS-native**           | So it integrates with your mesh, Weave, etc.                   |\n| 🧭 **Interactive + replayable** | You can try things, backtrack, debug                           |\n","n":0.06}}},{"i":32,"$":{"0":{"v":"Fluree Connector","n":0.707},"1":{"v":"\n## User Story\n\nAs a user of semantic meshes, I would like to be able to query an RDF database\nthat is synced to my [[concept.single-mesh-repo]]. Fluree is one option -- it has\na cloud version for easy data hosting\n","n":0.164}}},{"i":33,"$":{"0":{"v":"Principles","n":1}}},{"i":34,"$":{"0":{"v":"transposability","n":1},"1":{"v":"\n## Overview\n\nThere are two types of mesh transposability:\n\n- **[[Host transposability|principle.transposability.host]]** is the ability to move a [[concept.mesh]] to different serving locations without breaking its internal structure; i.e., A transposable mesh works correctly regardless of which [[concept.namespace.context]] contains it.\n- **[[Intramesh transposability|principle.transposability.intramesh]]** is the ability to move a [[concept.mesh.sub]] to a different part of the mesh\n\nBoth types of transposability rely on the use of an [[concept.implied-rdf-base]] and the use of [[relative identifier|concept.identifier.intramesh.relative]]s for intramesh references.\n\n## Key Principles\n\n### 1. No Hardcoded BASE URIs\n\nSemantic Flow never specifies BASE URIs in RDF distribution files. Instead, it relies on the RDF specification's defined behavior for situations where \"no base URI is embedded and the representation is not encapsulated within some other entity\": parsers use the document's retrieval IRI as the base URI. \n\n### 2. URI Reference Strategies\n\n- use [[concept.identifier.intramesh]] identifiers for internal references, see [[faq.reference-iri-choices]]\n\n\n## Transposition Scenarios\n\n### Moving Complete Meshes\n\nA complete mesh can be moved between repos, accounts, or hosting providers:\n\n```bash\n# Original location\nhttps://djradon.github.io/mesh/\n\n# New location after moving repo\nhttps://myorganization.github.io/data-mesh/\n\n# Or new hosting provider\nhttps://mysite.com/semantic-data/\n```\n\nAll internal relationships continue to work because they resolve relative to the new serving location.\n\n### Moving Submeshes Within Hierarchy\n\nWhile technically possible, moving parts of a mesh to different [[concept.namespace]]s is **discouraged** as it breaks the permanence principle of semantic identifiers. IRIs should remain stable over time.\n\nExample of what to avoid:\n```bash\n# Discouraged: moving bio from one parent to another\nns/djradon/projects/bio/ → ns/djradon/bio/\n```\n\nThis changes the permanent identifier for the bio resource and may break external references.\n\n## Implementation Benefits\n\n### No Build Step Required\n\nMeshes work directly when served from any static file server:\n- GitHub Pages\n- Netlify  \n- Apache/Nginx\n- Local file system\n\n### Standards Compliance\n\nTransposability leverages standard RDF parsing behavior rather than custom mechanisms, ensuring compatibility with existing RDF tools and libraries.\n\n## Best Practices\n\n1. **Use relative URIs** for all intra-mesh references\n2. **Avoid reorganizing internal structure**: because mesh structure determines namespaces, to maintain stable namespaces and preserve identifier permanence, nodes should not be moved around once published\n3. **Test transposition** by serving from different locations\n4. **Validate RDF** after moving to ensure parser compatibility\n\n","n":0.055}}},{"i":35,"$":{"0":{"v":"Intramesh Transposability","n":0.707}}},{"i":36,"$":{"0":{"v":"host transposability","n":0.707},"1":{"v":"\nHost transposability ensures that Semantic Flow meshes remain portable and can be deployed flexibly across different hosting environments while maintaining their semantic integrity.\n","n":0.209}}},{"i":37,"$":{"0":{"v":"Single Referent Principle","n":0.577},"1":{"v":"\n- An IRI should refer to only one thing\n- this is NOT the \"single name\" principle; multiple IRIs can refer to the same thing\n\n## Issues\n\n- meaning evolves over time. An IRI that once meant one thing, will come to be something different. Perhaps with versioning readily available, specifying the version helps uphold this principle.\n","n":0.136}}},{"i":38,"$":{"0":{"v":"pseudo-immutability","n":1},"1":{"v":"\nIn a filesystem-based structure like a [[concept.mesh]], you can't really prevent changes. But some things in a mesh should be treated as immutable, like [[mesh-resource.node-component.flow-shot.snapshot]] and [[concept.identifier.intramesh]].\n\n**Pseudo-immutability** acknowledges that things might be changed, for various reasons:\n\n- accidental changes\n- \"cleaning up\" of data for legal reasons, e.g.: personally-identifiable information (PII) or \"the right to be forgotten.\"\n- fixing of typos or other errors\n- re-organizing namespaces\n\nApplications should deal gracefully, and optionally alert users to improperly mutated data. \n\n\n**Pseudo-immutability** also acknowledges that:\n\n- for \"draft data\" especially, \"the working version\" is going to keep changing until a \"weave\" happens (i.e., a new version is minted). \n- sometimes you want the \"latest\" data for a given resource. Typically, \"current\" would be a pointer, redirect, or symlink. But given our goal of static hosting, we've decided just to have duplicate files for the \"current\" flow and the \"most recent version\" flow. \n\n\n## Mitigations\n\n- metadata can track changes and supply reasons for mutation\n- hashes can be used to detect mutations\n","n":0.079}}},{"i":39,"$":{"0":{"v":"Dereferencability for Humans","n":0.577},"1":{"v":"\nIf you put any [[mesh-resource]]'s IRI in a browser, it should return some useful context. \n\n \n\n\n## References\n\n- https://ld4pe.dublincore.org/learning_resource/making-uris-published-on-data-web-rdf-dereferencable/\n","n":0.229}}},{"i":40,"$":{"0":{"v":"composability","n":1},"1":{"v":"\n## Overview\n\nComposability is the ability to combine meshes. Semantic Flow enables flexible mesh composition by allowing any mesh node to contain other nodes, and by not specifying an absolute path anywhere. \n\n## Key Concepts\n\n### Mesh Boundaries\n\nA mesh is identified as a folder that looks like a [[folder.node]], i.e., has (at least) these two subfolders:\n\n\n\n### Upward Reference Problem\n\nWhen extracting submeshes, upward references can break. For example, if something within `ns/djradon/bio/` references `../../` (pointing to `ns/djradon/`), that reference will break if only the `bio/` subtree is copied elsewhere.\n\n### Weaving Process Solution\n\nDuring weaving, tools:\n1. **Scan for broken relatives**: Check all relative IRIs in the mesh\n2. **Convert broken ones**: Replace with absolute IRIs using canonical publication data\n3. **Leave working relatives alone**: Preserve transposability where possible\n\nAfter weaving, submeshes are semantically complete and can be composed using standard file operations.\n\n## Incorporating External Meshes\n\n### Importing (No External Connection)\n\nImport meshes or submeshes as permanent copies with no ongoing connection to the source:\n\n```bash\n# Method 1: Git archive (clean, can target specific paths)\ngit archive --remote=https://github.com/djradon/mesh.git main ns/djradon/ | tar -x -C collaborators/\ngit add collaborators/djradon/\ngit commit -m \"Import djradon's mesh\"\n\n# Method 2: Download and copy\ncIRI -L https://github.com/djradon/mesh/archive/main.zip -o mesh.zip\nunzip mesh.zip\ncp -r mesh-main/ns/djradon/ collaborators/djradon/\ngit add collaborators/djradon/\ngit commit -m \"Import djradon's mesh\"\n```\n\nThe imported content becomes permanently part of your repository with no external dependencies.\n\n### Embedding (Maintains External Connection)\n\nEmbed external meshes while maintaining a connection for updates:\n\n```bash\n# Import with ongoing connection to source repo\ngit subtree add --prefix=collaborators/djradon/ https://github.com/djradon/mesh.git main --squash\n\n# Update embedded mesh later\ngit subtree pull --prefix=collaborators/djradon/ https://github.com/djradon/mesh.git main --squash\n```\n\nThe embedded content becomes part of your repository and site, but you can pull updates from the source repository.\n\n### Directory Structure After Incorporation\n\n```\nyour-mesh/\n├── _flow/                           # Your mesh metadata\n├── _node-handle/\n├── ns/\n│   └── yourdata/\n└── collaborators/\n    └── djradon/                     # Imported/embedded mesh - served as static files\n        ├── _flow/                   # Their mesh metadata\n        ├── _node-handle/\n        └── ns/\n            └── djradon/\n```\n\nAll files are served directly as static content when the repository is published (e.g., via GitHub Pages).\n\n## Extracting Submeshes\n\n### Post-Weave Extraction\n\nAfter weaving resolves broken references, any subtree becomes a semantically complete mesh that can be copied using standard file operations:\n\n```bash\n# Copy submesh to create standalone mesh\ncp -r ns/djradon/ ../standalone-mesh/\n\n# Copy submesh to another location (e.g., for backup)\nrsync -av collaborators/alice/ backup/alice/\n```\n\n### Pre-Weave Considerations\n\nBefore weaving, analyze upward dependencies:\n- Identify references that point outside the intended extraction boundary\n- Determine if the extracted submesh will be semantically complete\n- Consider whether broken references should become absolute IRIs\n\n## Cross-Mesh References\n\n### Between Independent Meshes\n\nReferences between separate mesh repositories use absolute URIs:\n\n```turtle\n# Reference to external mesh\n<> foaf:knows <https://alice.github.io/mesh/ns/alice/> .\n```\n\n### Discovery Patterns\n\n**TBD**: Standardized mechanisms for:\n- Mesh discovery and registration\n- Stable cross-mesh URI patterns\n- Handling moved or unavailable external meshes\n\n## Composition Patterns\n\n### Collaborative Collection\n\nMultiple researchers contributing to a shared mesh:\n\n```bash\n# Add each contributor's mesh\ngit subtree add --prefix=contributors/alice/ https://alice.example/mesh.git main\ngit subtree add --prefix=contributors/bob/ https://bob.example/mesh.git main\n```\n\n### Organizational Hierarchy\n\nDepartment-level meshes within institutional mesh:\n\n```bash\n# Add department submeshes\ngit subtree add --prefix=departments/cs/ https://github.com/cs-dept/mesh.git main\ngit subtree add --prefix=departments/bio/ https://github.com/bio-dept/mesh.git main\n```\n\n### Temporal Snapshots\n\nPreserving historical versions of external meshes:\n\n```bash\n# Import specific version\ngit subtree add --prefix=snapshots/2024/djradon/ https://github.com/djradon/mesh.git v2024.1\n```\n\n## Best Practices\n\n### For Mesh Designers\n\n1. **Minimize upward references** in submesh boundaries to reduce weaving complexity\n2. **Design clear extraction points** - consider which subtrees should be independently viable\n3. **Use semantic boundaries** - align mesh structure with logical domain boundaries\n\n### For Mesh Composers\n\n1. **Choose import vs embed** based on maintenance needs - import for permanent copies, embed for ongoing updates\n2. **Import entire meshes** rather than attempting partial extraction from external repos\n3. **Weave before extraction** to ensure semantic completeness\n4. **Maintain incorporation metadata** - track source repositories and versions\n5. **Test extracted submeshes** independently before distribution\n\n### For Cross-Mesh References\n\n1. **Use absolute URIs** for references to external meshes\n2. **Prefer stable, canonical URIs** over temporary or redirect-based IRIs\n3. **Document external dependencies** for mesh consumers\n4. **Consider fallback strategies** for unavailable external resources\n\n## Workflow Integration\n\n### Development Workflow\n1. Incorporate external meshes using import (permanent) or embed (updateable) as needed\n2. Work with relative references for local development\n3. Weave before sharing to resolve broken dependencies\n4. Test extracted submeshes independently\n\n### Maintenance Workflow\n1. For embedded meshes: periodically update with `git subtree pull`\n2. For imported meshes: manually re-import if updates are needed\n3. Re-weave after updates to handle any new broken references\n4. Validate that composition still functions correctly\n5. Update documentation of external dependencies\n\n## TBD Items\n\n- **Cross-mesh reference protocols**: Standardized discovery and resolution mechanisms\n- **Version compatibility**: Handling version mismatches between composed meshes\n- **Dependency management**: Tools for tracking and updating external mesh dependencies\n- **Conflict resolution**: Handling namespace or identifier conflicts between composed meshes\n- **Performance optimization**: Efficient composition strategies for large meshes\n\nComposability enables Semantic Flow meshes to be combined and extracted flexibly while maintaining semantic integrity through intelligent tooling and clear design patterns.\n","n":0.036}}},{"i":41,"$":{"0":{"v":" Now","n":1},"1":{"v":"\n# Current Work Focus (Big Picture)\n\nThis document captures the high-level, current focus of development. It should be updated frequently to reflect the immediate priorities.\n\n## Current Priority\n\n- ontology update and conversion to jsonld\n\n## Active Tasks\n\n- [[task.2025-11-06-embed-rdf-store]]\n- figure out provenance - separate flow?\n\n## Blockers / Risks\n\n- Full implementation of the Weave Process remains a large, pending task.\n\n## Related Status\n\n- [[guide.status]] - Overall project health and component status.\n- [[todo]] - General list of pending tasks.\n","n":0.118}}},{"i":42,"$":{"0":{"v":"mesh resources","n":0.707},"1":{"v":"\n## Overview\n\nA **mesh resource** is any addressable component within a [[semantic mesh|concept.mesh]]. Every mesh resource has a unique [[Intramesh|concept.identifier.intramesh]] based on its path and locally unique name, making it dereferenceable via IRI.\n\nIn RDF terms, a resource is any node in an RDF graph that can be represented with an IRI (the other kinds of RDF graph nodes are literals and blank nodes). So theoretically, files and folders in [[mesh-resource.node-component.asset-tree]] could be considered RDF resources. But they are not considered **mesh** resources\n\n## Types of Mesh Resources\n\nThe structure of a semantic mesh is built on a fundamental distinction between **extensible** and **terminal** resources:\n\n- **[[Mesh nodes|mesh-resource.node]]** are extensible namespace containers:\n- **[[Mesh node components|mesh-resource.node-component]]** are terminal mesh resources:\n  - Can be physically represented as folders or files\n    - Folder [[concept.identifier.intramesh]] are part of the namespace but cannot be extended beyond their own internal structure\n  - All files and folders within a component folder are considered to be part of the parent node\n\n**Folder-based components:**\n\n\n- **[[metadata flows|mesh-resource.node-component.flow.node-metadata]]**: Administrative metadata (in `_meta/` folders)\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: File collections (in `_assets/` folders)\n- **[[Version datasets|mesh-resource.node-component.flow-shot.snapshot]]**: Versioned snapshots\n- **[[working snapshots|mesh-resource.node-component.flow-shot.working-shot]]**: Draft workspaces\n\n**File-based components:**\n- **Documentation files**: \n  - [[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]] are index.html files that provide de-referencability for their containing [[concept.identifier.intramesh]] [[facet.filesystem.folder]]\n  - **README.md and CHANGELOG.md**: unstructured documentation\n- **[[snapshot distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in RDF formats\n\n## Physical vs Logical Structure\n\n**Physical Representation:**\n- Mesh nodes and components are represented as folders in the filesystem\n- File resources are represented as individual files\n- Folder names become namespace segments and IRI path components\n\n**Logical Function:**\n- All mesh resources are addressable via their IRI path\n- IRIs must return meaningful content when dereferenced\n- Resources maintain semantic relationships through containment and cross-references\n\n## Asset Tree Special Case\n\n[[Asset trees|mesh-resource.node-component.asset-tree]] represent a special category where:\n- The asset tree itself (with its [[mesh-resource.node-component.flow.node-metadata]]) is part of the mesh structure\n- The files and folders contained within asset trees are \"attached to\" but not \"contained in\" the mesh\n- Asset tree contents are addressable but are not considered semantic flow resources\n\nThis distinction maintains clean separation between semantic mesh structure and arbitrary file attachments while preserving addressability.\n","n":0.054}}},{"i":43,"$":{"0":{"v":"mesh node","n":0.707},"1":{"v":"\n## Overview\n\nThe primary constituents of a semantic mesh are **mesh nodes**. A node's IRI refers to the its referent, i.e. the real-world or imaginary \"thing\" which the IRI names.\n\nNodes are represented \"on disk\" as [[mesh folders|facet.filesystem.folder]].\n\nMesh nodes establish conceptual [[namespace segments|concept.namespace.segment]] and can be holonic containers of other mesh nodes. They may also contain [[node components|mesh-resource.node-component]], which are supporting files and conceptual structures.\n\n## Node Types\n\n- [[bare node|mesh-resource.node.bare]] : containers\n- [[mesh-resource.node.reference]] : refering containers\n- [[payload node|mesh-resource.node.payload]] : dataset containers that refer to their datasets \n\n## Filesystem Structure\n\nWhen stored on disk, all mesh nodes:\n- are physically represented as folders in the filesystem\n- extend the identifier namespace with their folder name\n- contain any of their own mesh resources\n- may contain other nodes\n\n## Mandatory Components\n\nEvery mesh node has these components:\n\n- **[[mesh-resource.node-component.flow.node-metadata]]** ([[folder._meta]]): Centralized metadata for the node\n- **[[mesh-resource.node-component.node-handle]]** (`_node-handle/`): Universal marker folder that refers to the parent \"as a mesh node\", as opposed to \"as the name, dataset, or other thing\" to which it normally refers; a handle resource page should explain this distinction\n\n","n":0.077}}},{"i":44,"$":{"0":{"v":"reference node","n":0.707},"1":{"v":"\n## Definition\n\nA **reference node** is a [[mesh-resource.node]] that represents the (non-dataset) **referent** of the node — i.e., the thing in the world that the node stands for.\n\n## Purpose\n\n* To describe what the node *refers to* (person, place, concept, dataset, etc.).\n* To supply human/machine labels, identifiers, and minimal provenance about the referent.\n* To differentiate between metadata about the **node itself** (`_meta`) and metadata about the **referent**.\n\n## Typical Contents\n\n### Core Description\n\nWhat is the node’s referent? This is what the HTML resource page uses to say:\n\n“This is an Organization / Ontology / Rate Plan / Dataset / Whatever.”\n\n* `rdfs:label` (human-readable name of the referent).\n* `rdf:type` (classifying what kind of thing the referent is).\n* maybe  skos:prefLabel, dcterms:description, owl:sameAs, etc.\n\n### Dataset-level description (if the thing is a dataset)\n\n* Optional provenance (creator, source, temporal scope).\n* Optional identifiers (sameAs links, external URIs).\n* dcat:Dataset, dcat:distribution, dcterms:issued, dcterms:modified, dcterms:license, etc.\n\nThis describes the dataset-as-thing.\n\n### Context / External Perspectives\n\nLinks to registries, catalogs, external provenance, ie. alternate descriptions imported from elsewhere.\n\n\n## Example Snapshot Distribution\n\n```trig\n# The referent of the node (the actual person)\n<djradon>\n    rdf:type foaf:Person ;\n    rdfs:label \"dj radon\" ;\n    foaf:mbox <mailto:djradon@example.org> ;\n```\n","n":0.075}}},{"i":45,"$":{"0":{"v":"paylod node","n":0.707},"1":{"v":"\n**Payload nodes** are nodes with datasets conained in them via [[mesh-resource.node-component.flow.payload]]. They [[denote|concept.denotation]] their contained payload dataset, and must have a [[mesh-resource.node-component.flow.reference]] to describe that dataset.\n\n## Overview\n\n**payload nodes** are [[mesh-resource.node.reference]]s that represent and contain an evolvable \"payload\" dataset in the form of a [[mesh-resource.node-component.flow.payload]]. \n\nThe payload dataset is kept in the payload node's [[mesh-resource.node-component.flow.payload]].\n\nLike all [[mesh-resource.node-component.flow]]s, because it is evolvable it gets typed as a [DatasetSeries](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset_Series). Its snapshots are [[datasets|https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset]].\n\nUnlike [[flow snapshots|mesh-resource.node-component.flow-shot]] which contain concrete data distributions, payload nodes serve as conceptual containers that organize and provide identity for datasets without containing the data directly. I.e., payload nodes only contain concrete datasets by virtue of containing a [[mesh-resource.node-component.flow.payload]] (also abstract) and its snapshots, which have concrete distributions.\n\nIf a node has a PayloadFlow (i.e., it’s a “payload node”), then in its ReferenceFlow the node’s IRI MUST have rdf:type dcat:Dataset.\n\n## Abstract vs Concrete Datasets\n\n### payload node (highly abstract)\n\nA payload node represents a dataset as a concept, e.g.:\n\n- `/ns/djradon/bio/` = a biographical dataset about the person djradon\n- `/ns/census/` =  the results of a census \n- `/ns/weather-stations/` = a weather stations dataset\n\nThe payload node provides **stable identity**: The dataset persists conceptually even as concrete data changes.\n  \n### payload flow (abstract)\n\n[[mesh-resource.node-component.flow.payload]] is the single user payload flow for a node, realized by snapshots:\n\n- `/ns/monsters/_payload/_default/` = the default dataset snapshot\n- `/ns/weather-stations/_payload/2025-11-24_0142_07_v3/` = version 3 dataset snapshot\n\nSnapshots contain **distribution files**: the actual data in various formats (e.g., .trig, .jsonld)\n\n### payload flow Snapshot (slightly concrete)\n\nFlow snapshots are still not actual data, but they denote a specific version of an evolving dataset\n\n### payload flow Snapshot Distributions (concrete)\n\nThese are \"concrete information resources\", i.e. files.\n\n\n## Required Components\n\nEvery payload node must contain:\n\n- **[[mesh-resource.node-component.flow.node-metadata]]** (`_meta/`): Administrative metadata about the data concept\n- **[[mesh-resource.node-component.flow.payload]]** (`_payload/`): dataset data\n- **[[Node handle|mesh-resource.node-component.node-handle]]** (`_node-handle/`): Referential indirection for the node\n\n## Optional Components\n\n- [[mesh-resource.node-component.flow.reference]]: metadata about the dataset\n- [[Asset trees|mesh-resource.node-component.asset-tree]] (`_assets/`): Attached file collections\n- [[mesh-resource.node-component.documentation-resource.changelog]] and [[mesh-resource.node-component.documentation-resource.readme]]\n- [[mesh-resource.node-component.node-config-defaults]]\n\n## Key Characteristics\n\n### Not a Dataset\n\n**Important**: A payload node is **not itself a (concrete) dataset**. It represents the abstract concept of a dataset that may evolve over time:\n- payload nodes are never versioned (only their component flows are)\n- payload nodes serve as stable conceptual anchors\n\n### Extensible Container\n\nLike all mesh nodes, payload nodes can contain other mesh nodes and components, making them extensible namespace containers.\n\n## Examples\n\n### Unversioned payload node\n```\nns/monsters/\n├── _meta/                 # metadata about the \"monsters\" payload node\n├── _node-handle/               # handle for the payload node\n└── _payload/                 # single payload flow\n    └── _default/               # default dataset snapshot\n        ├── monsters.jsonld     # concrete distribution of the default snapshot\n        └── monsters.trig\n```\n","n":0.049}}},{"i":46,"$":{"0":{"v":"bare node","n":0.707},"1":{"v":"\n**Bare nodes** are [[mesh-resource.node]]s that contain other mesh nodes. Their [[concept.identifier]]\n\n## Function\n\n- namespace extenders and perhaps organizational containers\n\n**Mandatory Components**: `_meta/` + `_node-handle/`\n**Optional Components**: [[mesh-resource.node-component.flow.node-config]], [[mesh-resource.node-component.documentation-resource]]\n\n\nThey are physically represented by [[folder.node]].\n\n## Purpose\n\n- scaffolding, grouping, deferred semantics\n- a secondary, optional function is as \"semantic contextualizers\", but bare nodes don't have any definitive [[concept.referent]] of their own. \n","n":0.135}}},{"i":47,"$":{"0":{"v":"node component","n":0.707},"1":{"v":"\n## Overview\n\n**Node components** are mesh resources that support and define the mesh structure. Unlike [[mesh nodes|mesh-resource.node]] which can contain other mesh nodes, components cannot be extended beyond their own internal structure.\n\nComponents can be physically represented as folders or files, and all files and folders within a component folder are considered to be part of that component.\n\n## Component Categories\n\nComponents are categorized by their facets, including:\n  - typical creation and maintenance patterns (user vs system)\n  - versioning status\n  - folder vs. file\n  - node role (meta and data [[mesh-resource.node-component.flow]])\n\n### User Components\n\nUser components are primarily created and maintained by users or their software agents and services, and represent domain knowledge:\n\n**Folder-based user components:**\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: Collections of arbitrary files attached to the mesh (in `_assets/` folders)\n- **[[mesh-resource.node-component.flow-shot.working-shot]]**: Draft workspaces for ongoing changes to [[mesh-resource.node-component.flow]] (in `_working/` folders)\n\n**File-based user components:**\n- **README.md files**: User documentation providing context\n- **CHANGELOG.md files**: Version history documentation\n\n### System Components\n\nSystem components are usually created or altered by the [[Weave Process|concept.weave-process]] process rather than direct user modification:\n\n**Folder-based system components:**\n- **[[metadataset flows|mesh-resource.node-component.flow.node-metadata]]**: Administrative and structural metadata for mesh nodes (in `_meta/` folders)\n- **[[snapshot|mesh-resource.node-component.flow-shot.snapshot]]**: Versioned snapshots of datasets (in `_vN/` folders) are created on weave\n- **[[mesh-resource.node-component.flow-shot.default-shot]]**: updated on weave\n- **[[Node handles|mesh-resource.node-component.node-handle]]**: Components providing referential indirection for nodes as mesh resources (in `_node-handle/` folders)\n\n**File-based system components:**\n- **[[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]]**: Generated index.html files for human-readable access\n- **[[Distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in various RDF formats\n\n## Physical vs Logical Structure\n\n**Physical Representation:**\n- Folder-based components are represented as folders with underscore prefixes (like `_meta/`, `_assets/`)\n- File-based components are individual files within mesh nodes or other components\n- Component folders contain all files and folders that belong to that component\n\n**Logical Function:**\n- Components extend the namespace but are terminal (cannot contain other mesh nodes or components)\n- Components provide specialized functionality: metadata, versioning, referential data, or file attachments\n- Components maintain the semantic structure and operational capabilities of the mesh\n\n## Integration with Nodes\n\nComponents work in conjunction with mesh nodes to create the complete mesh structure:\n- Every mesh node contains at least two components: [[mesh-resource.node-component.flow.node-metadata]] and [[mesh-resource.node-component.node-handle]]\n- [[mesh-resource.node.payload]] contain a single [[mesh-resource.node-component.flow.payload]] \n- Any node may contain asset trees (user files) for bundling or [[concept.weave-process.resource-page-generation]]\n","n":0.054}}},{"i":48,"$":{"0":{"v":"snapshot distribution","n":0.707},"1":{"v":"\n- each [[mesh-resource.node-component.flow-shot]] should have one or more [[mesh-resource.node-component.snapshot-distribution]] [[file]].\n- a snapshot's distributions should all contain the same data, just in different syntaxes \n\n## Distribution Filenaming Per Flow\n\n-  [[mesh-resource.node-component.flow.reference]], [[mesh-resource.node-component.flow.node-metadata]], [[mesh-resource.node-component.flow.node-config.operational]] and [[mesh-resource.node-component.flow.node-config.inheritable]] have their distributions named with `_ref`, `_meta`, `_config` and `_inheritable-config` respectively\n- [[mesh-resource.node-component.flow.payload]] distributions use the node slug as the base filename (no \"_data\" or \"_payload\" suffix):\n\n","n":0.13}}},{"i":49,"$":{"0":{"v":"working distribution","n":0.707},"1":{"v":"\nUnlike most other [[mesh-resource.node-component.flow-shot]], the [[mesh-resource.node-component.flow-shot.working-shot]] is [[user-facing|facet.user]] (directly modifiable), and so there shouldn't be any [[concept.sibling-distribution]]. Just the one distribution that may be updated, and if multiple-syntax weaving is turned-on, is the source for the siblings.\n\nThe [[product.cli]] should have functionality for converting the working distribution between formats, in case you want to change the syntax you're using.\n","n":0.131}}},{"i":50,"$":{"0":{"v":"version distribution","n":0.707},"1":{"v":"\nEvery [[versioned|facet.flow.versioned]] or [[deversioned|facet.flow.deversioned]] [[mesh-resource.node-component.flow]] has at least one [[mesh-resource.node-component.flow-shot.snapshot]] with a corresponding **version distribution**. \n","n":0.25}}},{"i":51,"$":{"0":{"v":"current distribution","n":0.707}}},{"i":52,"$":{"0":{"v":"node handle","n":0.707}}},{"i":53,"$":{"0":{"v":"handle resource page","n":0.577},"1":{"v":"\n- provides an accessible description of its containing [[mesh-resource.node]], which is identified with a [[mesh-resource.node-component.node-handle]]. \n- by default it could be something simple like \"For Semantic Web purposes, this IRI should be considered to connote the Semantic Mesh node itself, not the node's referent.\"\n- essentially, a [[mesh-resource.node-component.documentation-resource.resource-page]]\n","n":0.146}}},{"i":54,"$":{"0":{"v":"Node Config Defaults","n":0.577},"1":{"v":"\n## Overview\n\nNode config defaults are inheritable settings that provide baseline behavior for nodes. They are supplied by ancestors (and service/platform) and are resolved by a mechanism similar to that of service config config.\n\n- Inheritance mechanism: see [[mesh-resource.node-component.flow.node-config.inheritable]]\n- Operational (final) config: see [[mesh-resource.node-component.flow.node-config.operational]]\n- Folder overview for config flows: see [[mesh-resource.node-component.flow.node-config]]\n\n## Common default settings (examples)\n\n- Flow versioning: on/off (whether abstract flows create `_vN/` snapshots on weave)\n- Distribution syntaxes: preferred serializations (e.g., TriG, JSON‑LD)\n- Resource pages and fragments: enable page/fragment generation; template and stylesheet selection\n- Aggregated distributions: on/off for generating top-level rollups\n- Rights & provenance defaults: copyright/licensing/attribution/delegation policies (applied at snapshot time)\n\nThese defaults apply when a node does not specify the setting in its [[mesh-resource.node-component.flow.node-config.operational]]; “most specific wins” from parent → service → platform (see [[mesh-resource.node-component.flow.node-config.inheritable]] for precedence).\n\n## Minimal guidance\n\n- Keep defaults lightweight; override at the node only when needed\n- Prefer repository‑level templates/css in `_assets/` for consistency (see [[mesh-resource.node-component.asset-tree]])\n- Review defaults when moving/embedding meshes to ensure expected publication behavior\n","n":0.08}}},{"i":55,"$":{"0":{"v":"node flow","n":0.707},"1":{"v":"\n[[Nodes|mesh-resource.node]] are primarily constituted by their semantic flows: evolvable datasets about their node's data, metadata, configuration, or referent. They exist through time, independent of any specific version or realization, and can evolve semi-independently.\n\nThere are five types of node flows.\n\n- [[mesh-resource.node-component.flow.node-metadata]] (required)\n- [[mesh-resource.node-component.flow.node-config.operational]] (optional)\n- [[mesh-resource.node-component.flow.node-config.inheritable]] (optional)\n- [[mesh-resource.node-component.flow.reference]] (optional)\n- [[mesh-resource.node-component.flow.payload]] (for payload nodes)\n\n\n## Relationship to snapshots\n\nAs DatasetSeries, node flows are realized through [[mesh-resource.node-component.flow-snapshot]] datasets, which are temporal slices of the flow. To borrow a phrase from the PROV model, we say that a snapshot is a specialization of the node flow.\n\n### Relationship pattern:\n\nEvery node flow has at least two concrete snapshots: [[mesh-resource.node-component.flow-shot.default-shot]] and [[mesh-resource.node-component.flow-shot.working]].\n\nThe node flow is a [DatasetSeries](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset_Series) and may have multiple [[mesh-resource.node-component.flow-snapshot.version]]s.\n\n\n### Ontology Example\n\n- node flow: \"My ontology definitions\" (persistent concept)\n- flow snapshots: v1, v2, current version, working draft of working version (specific realizations)\n\n\n```file\n/my-ontology/\n└── _payload/                  ← node flow (ontology definitions)\n    ├── _default/           ← flow snapshot\n    ├── _working/           ← flow snapshot\n    ├── 2025-11-24_0142_07_v1/           ← flow snapshot\n    └── 2025-11-24_0142_08_v2/                ← flow snapshot\n```\n\nIn this example:\n\nEach _default/, _working/, and snapshot folder contains flow snapshot realizations\n\n## Persistent Identity\n\nnode flows provide conceptual continuity by:\n\n- Maintaining meaning across versions and changes\n- Preserving references from external sources\n- Enabling evolution while keeping identity stable\n- Supporting versioning without losing conceptual coherence\n","n":0.071}}},{"i":56,"$":{"0":{"v":"reference flow","n":0.707},"1":{"v":"\n## Definition\n\nA **reference flow** is a dataset series within a mesh node that provides information about the **referent** of the node — i.e., the thing in the world that the node stands for.\n\n**Purpose**\n\n* To describe what the node *refers to*, i.e., its referent (person, place, concept, event, dataset, etc.).\n* To supply labels and alternative identifiers for the referent.\n* To refer to other descriptive data about the referent.\n\n## Typical Contents\n\n* `rdfs:label` (human-readable name of the referent).\n* `rdf:type` (classifying what kind of thing the referent is).\n* Optional identifiers (sameAs links, external URIs).\n\n## Example Snapshot Distribution\n\n```trig\n# The referent of the node (the actual person)\n<djradon>\n    rdf:type foaf:Person ;\n    rdfs:label \"dj radon\" ;\n    foaf:mbox <mailto:djradon@example.org> ;\n```\n","n":0.095}}},{"i":57,"$":{"0":{"v":"payload flow","n":0.707},"1":{"v":"\n**payload flows** provide versionable data storage functionality within the semantic mesh architecture. \n\n## Overview\n\nA payload flow (formerly called a __dataset__ flow) is a series of RDF datasets. Like all flows, each payload flow has snapshots (_default/, _working/, snapshot folders like `2025-11-24_0142_07_v1/`) that track its evolution over time.\n\npayload flows are distinct from [[mesh-resource.node-component.flow.node-metadata]]s, which are usually managed by the platform and describe the mesh node itself and its components.\n\n## Purpose\n\npayload flows serve as the primary content containers for [[mesh-resource.node.payload]], providing:\n\n- **Content Storage**: Hold the actual dataset payload that defines the node's content\n- **History**: Support multiple versions (snapshots) of the same conceptual dataset\n- **Format Diversity**: Provide multiple format distributions (TTL, JSON-LD, etc.)\n- **State Management**: Track current, draft, and versioned states of data\n\n## Structure\n\npayload flows organize content through [[flow snapshots|mesh-resource.node-component.flow-shot]]:\n\n- `_default/` - Current stable version of the dataset\n- `_working/` - Draft/work-in-progress version\n- Snapshot folders (e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`) - Versioned snapshots for historical access\n\nLike all [[facet.filesystem.folder]], they should contain an `index.html` [[mesh-resource.node-component.documentation-resource.resource-page]] -- a human-readable description for the flow.\n\n## Distribution Formats\n\nEach [[flow snapshot|mesh-resource.node-component.flow-shot]] typically provides multiple format distributions:\n\n- **Trig (.trig)**: Primary RDF serialization\n- **JSON-LD (.jsonld)**: JSON-compatible linked data\n- **RDF/XML (.xml or .trix)**: XML-based RDF serialization\n- **N-Quads (.nq)**: Line-based RDF format\n\n## Example\n\nFrom the [[semantic mesh example|concept.semantic-mesh.example]]:\n\n```\n/test-ns/djradon-bio/_payload          # payload flow\n├── _default/                        # default snapshot\n│   ├── djradon-bio.ttl             # turtle distribution\n│   ├── djradon-bio.jsonld          # json-ld distribution\n│   └── index.html                  # snapshot interface\n├── _working/                          # draft snapshot\n│   ├── djradon-bio.ttl             # draft turtle\n│   ├── djradon-bio.jsonld          # draft json-ld\n│   └── index.html                  # snapshot interface\n└── index.html                      # resource page\n```\n\n## Integration\n\npayload flows integrate with other mesh components:\n\n- **metadata flows**: Provide provenance and management data\n- **Asset Trees**: Store associated files and media\n- **Resource Pages**: Provide human-readable interfaces\n","n":0.061}}},{"i":58,"$":{"0":{"v":"metadata flow","n":0.707},"1":{"v":"\nA **metadata flow** contains system-related administrative and structural metadata for every [[mesh-resource.node]], including the versioning data for each node's flows.\n\nIn the filesystem, it exists as a [[folder._meta]] in a [[folder.node]].\n\nMesh-specific metadata about a node's flows' [[mesh-resource.node-component.flow-shot.snapshot]]s mostly lives here too, eliminating the need to keep separate metadata in the node component. Also may contain metadata about the assets folder.\n\n## Use of _node-handle in metadata flows\n\nWhen metadata flows (or any [[facet.system]] dataset) refer to mesh nodes, they'll usually be talking about \"the-node-as-mesh-constituent\", so they'll use the node's [[mesh-resource.node-component.node-handle]] identifier\n\n## Recommended vocabulary\n","n":0.105}}},{"i":59,"$":{"0":{"v":"meshnode config flows","n":0.577},"1":{"v":"\nNode configuration is managed through two distinct flows that provide settings for a node's behavior.\n\n1.  **[[Operational Config Flow|mesh-resource.node-component.flow.node-config.operational]]**: This flow contains the final, resolved configuration that dictates how a specific node operates.\n\n2.  **[[Inheritable Config Flow|mesh-resource.node-component.flow.node-config.inheritable]]**: This flow contains settings that a node makes available to its descendants in the mesh hierarchy.\n\nWhile there are two separate flows, there is a single inheritance mechanism that resolves the final operational configuration for a node. This mechanism draws from the `inheritable` configs of parent nodes, as well as service and platform-level defaults.\n","n":0.107}}},{"i":60,"$":{"0":{"v":"operational config flow","n":0.577},"1":{"v":"\nAn **operational config flow** defines a node's final, resolved settings. It is the direct consumer of the configuration inheritance chain and controls the node's actual behavior (e.g., versioning, distribution formats).\n\nIf a node has an operational config flow, it can still inherit settings from the [[inheritance chain|mesh-resource.node-component.flow.node-config.inheritable]]. Any settings explicitly defined in the operational config will override those that would have been inherited.\n\nIf a node lacks an operational config flow, its behavior is determined by the resolved settings from the inheritance chain, with service defaults and platform defaults filling in any gaps.\n","n":0.105}}},{"i":61,"$":{"0":{"v":"inheritable config flow","n":0.577},"1":{"v":"\nAn **inheritable config flow** contains settings that a node makes available to its descendants in the mesh hierarchy. It is the primary mechanism for providing default configurations to child nodes.\n\n## Inheritance Hierarchy\n\nThe inheritance chain follows this precedence (most specific wins):\n\n1.  **Parent Node's** InheritableNodeConfig\n2.  **Grandparent Node's** InheritableNodeConfig (and so on, up the tree)\n3.  **Service-level** InheritableNodeConfig\n4.  **Platform-level** InheritableNodeConfig (ultimate fallback)\n\nThe final `OperationalNodeConfig` for a given node is resolved by merging the settings from this chain.\n\n## Resolution Algorithm\n\nWhen resolving a node's operational configuration, the system walks up the hierarchy from the node's parent, collecting `InheritableNodeConfig` at each level. These are merged, with settings from closer ancestors taking precedence.\n\n### Property-Level Inheritance\n\nConfiguration inheritance works at the property level. A child's `InheritableNodeConfig` can override a single property while still inheriting others from its parent.\n\n```jsonld\n{\n  \"@id\": \"parent:inheritableConfig\",\n  \"@type\": \"node-conf:InheritableNodeConfig\",\n  \"node-conf:versioningEnabled\": true,\n  \"node-conf:distributionFormats\": [\"application/trig\", \"application/ld+json\"]\n}\n\n{\n  \"@id\": \"child:inheritableConfig\",\n  \"@type\": \"node-conf:InheritableNodeConfig\",\n  \"node-conf:versioningEnabled\": false\n  // Inherits distributionFormats from parent\n}\n```\n\n## Configuration Control Properties\n\n### `nodeConfigInheritanceEnabled` (Child's Perspective)\n\nControls whether a node *receives* inherited configuration.\n-   **Default**: `true`\n-   **Effect**: When `false`, the node ignores the inheritance chain and uses only its own operational config or system defaults.\n\n### `inheritableConfigPropagationEnabled` (Parent's Perspective)\n\nControls whether a node *provides* its inheritable configuration to its children.\n-   **Default**: `true`\n-   **Effect**: When `false`, this node acts as a \"firewall,\" blocking its own and any ancestor's inheritable configs from flowing down to its children.\n","n":0.068}}},{"i":62,"$":{"0":{"v":"FlowShot","n":1},"1":{"v":"\n**flow snapshots** are components that are datasets and represent the evolutionary steps of the [[mesh-resource.node-component.flow]].  \n\nflow snapshots have corresponding [[distributions|mesh-resource.node-component.snapshot-distribution]] and are the connective tissue between nodes and their RDF-based representation.\n\n## Relationship to node flows\n\nflow snapshots are the successive realizations of [[mesh-resource.node-component.flow]].\n\n### Relationship pattern:\n\nnode flows have at least two snapshots:\n\n- default shot (`_default/`)\n- working shot (`_working/`)\n- versioned snapshots (historical versions)\n\n### Ontology dataset node Example\n\n```file\n/my-ontology/               ← dataset node: Conceptual, data-oriented \"thing\"\n├── _meta/                   ← meta flow (metadata)\n│   ├── _default/           ← flow snapshot (default metadata)\n│   ├── _working/           ← flow snapshot (working draft)\n│   ├── 2025-11-24_0142_07_v1/              ← flow snapshot (version 1 metadata)\n│   └── 2025-11-24_0142_08_v2/              ← flow snapshot (version 2 metadata)\n└── _dataset-flow/                  ← dataset node flow (ontology definition--by-dataset)\n    ├── _default/           ← flow snapshot (default definition)\n    ├── _working/           ← flow snapshot (working draft)\n    └── 2025-11-24_0142_07_v1/              ← flow snapshot (version 1 definition)\n```\n\nIn this example:\n- `_default/`, `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`, `_working/` are all flow snapshots\n- Each contains actual data files and distributions\n- They represent specific temporal states of their parent node flows\n\n## Temporal Nature\n\nflow snapshots capture datasets at specific moments:\n\n- **Default shots** (`_default/`) - The latest committed version\n- **Working shots** (`_working/`) - Draft content for future release\n- **Snapshots** (e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`) - Immutable historical versions\n\n## Content Structure\n\nflow snapshots contain:\n- **Data files** - The actual dataset content (`.ttl`, `.rdf`, `.jsonld`)\n- **Distributions** - Multiple format representations of the same data\n- **Metadata** - Information about the specific version/snapshot\n\n### Example Structure\n```file\n_default/\n├── my-ontology.ttl         ← Distribution\n├── my-ontology.rdf         ← Distribution\n└── my-ontology.jsonld      ← Distribution\n```\n\n## Immutability\n\n**[[mesh-resource.node-component.flow-snapshot.version]]** (historical flow snapshots, i.e., snapshot folders like `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`) should be treated as immutable once created. This provides reliable references for external systems and ensures accurate provenance and history.\n\n**[[mesh-resource.node-component.flow-shot.default-shot]]** (the latest \"woven\" flow snapshots, `_default/`) should not be modified directly by users, but will be updated \"on weave\" if the [[mesh-resource.node-component.flow-shot.working]] has evolved.\n\n**[[mesh-resource.node-component.flow-shot.working]]** (working flow snapshots, `_working/`) are mutable:\n- Can be edited and updated during development\n- Represent evolving state of the node flow\n\n## Creation and Lifecycle\n\nflow snapshots are created through:\n- **Initial authoring** - Creating `_default/` content\n- **Versioning** - Snapshotting `_default/` to snapshot folders (e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`) during [[concept.weave]]\n- **Draft preparation** - Working in `_working/` for future releases\n\n## Related Concepts\n\n- **[[mesh-resource.node-component.flow]]** - Parent conceptual entities\n- **[[concept.flow-version]]** - Process of creating versioned flow snapshots\n- **[[concept.weave-process]]** - Operation that manages flow snapshot lifecycle\n","n":0.052}}},{"i":63,"$":{"0":{"v":"working snapshot","n":0.707},"1":{"v":"\nThe **working snapshot** serves as a draft workspace for ongoing changes to a node's [[mesh-resource.node-component.flow]]s\nAfter a version-bumping weave, a working snapshot starts identically to the default dataset but can be modified safely without affecting the stable current version. During weaving, _working content becomes the new default dataset and gets snapshotted as the latest version, while _working naturally remains ready for the next round of drafts.\n\nThis allows continuous development and version control commits without requiring immediate version bumps or disrupting users of the stable dataset.\n","n":0.109}}},{"i":64,"$":{"0":{"v":"snapshot","n":1},"1":{"v":"\nA snapshot dataset generated \"on [[weave|concept.weave-process]]\"\n\nThe [[folder.snapshot]] is named with the [[concept.weave-label]] plus the [[concept.flow-version]]\n\n## Disambiguation\n\n- a snapshot is an addressable resource; \n- it is differentiated from the concept of a [[facet.flow.unversioned]] in that a versioned flow is a flow that has never had a version.\n","n":0.147}}},{"i":65,"$":{"0":{"v":"default snapshot","n":0.707},"1":{"v":"\n[[Versioned|facet.flow.versioned]] mode: _default/ is a materialized copy of the latest snapshot folder.\n[[Unversioned|facet.flow.unversioned]] mode: _default/ is the one-and-only dataset\n[[facet.flow.deversioned]] mode: [[folder._default]] is the current version of the dataset\n\nThe default snapshot provides the most recently published version of a flow's content without relying on symlinks or redirects. It serves as the \"general purpose\" source that users and external systems can reference, remaining unchanged during active development via the [[mesh-resource.node-component.flow-shot.working-shot]].\n\nIf versioning is turned on and nobody has cleaned up old versions, the default snapshot matches the content of the latest versioned snapshot (e.g., `2025-11-24_0142_07_v3/`) and remains identical to the [[mesh-resource.node-component.flow-shot.working-shot]] until new changes begin. During weaving, the [[mesh-resource.node-component.snapshot-distribution.working]] becomes the new [[mesh-resource.node-component.flow-shot.default-shot]]. If versioning is turned on, the _working content becomes the working version.\n\nThis provides a reference point for citations and external links that want the latest information, while allowing ongoing development work to proceed safely in the `_working` dataset without disrupting users of the published data.\n","n":0.081}}},{"i":66,"$":{"0":{"v":"Documentation Resource","n":0.707},"1":{"v":"\n- [[file]] like:\n  - [[mesh-resource.node-component.documentation-resource.changelog]]\n  - [[mesh-resource.node-component.documentation-resource.readme]]\n  - [[mesh-resource.node-component.documentation-resource.resource-page]]\n  - [[mesh-resource.node-component.node-handle.page]]\n","n":0.302}}},{"i":67,"$":{"0":{"v":"mesh resource page","n":0.577},"1":{"v":"\nTo make every folder-based resource more discoverable, they each have an index.html page that gets generate \"on [[concept.weave-process]]\"\n\n\n- primarily for humans\n\n## References\n\n- https://www.w3.org/wiki/DereferenceURI\n","n":0.209}}},{"i":68,"$":{"0":{"v":"Resource Fragment","n":0.707},"1":{"v":"\nResource fragments are HTMX fragments, support dynamic behaviour in [[mesh-resource.node-component.documentation-resource.resource-page]] or external web apps without a \"live\" backend.\n\nFor resource pages, they're most useful for \"saving bandwidth\": data that might not be needed can be loaded later.\n\nFor external apps, they save the overhead of parsing and discovery.\n\nFragment generation can be configured per node or inherited from config hierarchy.\n\n## **Multiple Resource Fragments in Assets**\n\nThis is a natural extension of your asset tree concept (.9):\n\n```\nmesh-node/\n├── _assets/\n│   ├── fragments/               # Generated resource pages\n│   │   ├── README.html          # generated from README.md\n│   │   ├── CHANGELOG.html       # generated from CHANGELOG.md\n│   │   └── back-references.html # list of back-references\n│   └── styles/\n│       └── common.css\n├── _meta/\n├── CHANGELOG.md\n└── README.md\n```\n\n","n":0.097}}},{"i":69,"$":{"0":{"v":"README","n":1},"1":{"v":"\n- Provides an unstructured introduction to the containing resource\n- preferably written in Markdown. \n","n":0.267}}},{"i":70,"$":{"0":{"v":"CHANGELOG","n":1},"1":{"v":"\n- provides an unstructure history of the containing resources\n- preferably written in Markdown. \n","n":0.267}}},{"i":71,"$":{"0":{"v":"assets tree","n":0.707},"1":{"v":"\nThis node component is \"mesh-terminal\" and should contain no [[sflow-resources|mesh-resource]]. \n\nIt can be contained in any [[folder.node]], i.e., only Nodes get assets trees.\n\nIts metadata (if any) should be stored in the parent node’s meta flow (`_meta/`). Asset trees are terminal and carry no flows, and are ignored by the mesh scanner.\n\nIt can contain an arbitrary set of files and folders, but two (optional) folders are special:\n- _templates can contain html files to be used when generating [[mesh-resource.node-component.documentation-resource.resource-page]] for the containing [[mesh-resource.node]] or its sub-resources.\n","n":0.109}}},{"i":72,"$":{"0":{"v":"Aggregated Distribution","n":0.707},"1":{"v":"\n__note: maybe we will do them, maybe we won't__\n - t.2025.11.08.09 probably not. Better to go the other way: from a payload dataset, create all its named nodes and link back to the original; keeps things flow-y\n- probably won't do unified distributions except via API. \n\nA node's **aggregated distribution** is a compilation of all the child flows of itself and its contained nodes (their `_payload/_default/` snapshots), situated directly under the parent node with an intuitive filename like \"nodename.ext\".\n\nEssentially, it's a \"(sub-)mesh in a single file.\" \n\nPerhaps its only available via API. \n\n## Purpose\n\nAggregated distributions support [[principle.composability]] and [[principle.transposability]] by:\n- Combining contained nodes' data into a single resource\n- Supporting modular ontology and knowledge base construction\n\n## Issues\n\nconfig options\n- zipping/compression?\n- user data only, or include metadata/config\n\n## Generation Process\n\nDuring [[concept.weave-process]], aggregated distributions are created by:\n1. **Scanning contained payload nodes** recursively within the mesh structure\n2. **Collecting `_payload/_default/` distributions** from each flow\n3. **Merging content** with proper URI resolution and prefix handling\n4. **Excluding `_config` and `_meta` datasets** (data content only)\n5. **Generating multiple distributions** (.ttl, .rdf, .jsonld) as configured\n\n## Examples\n\n### Composable Ontology\n```\n/my-ontology/\n├── my-ontology.ttl              ← Aggregated distribution\n├── my-ontology.rdf              ← Aggregated distribution  \n├── my-ontology.jsonld           ← Aggregated distribution\n├── Person/                  ← payload node (class definition)\n├── hasName/                 ← payload node (property definition)\n└── Organization/            ← payload node (class definition)\n```\n\n### Knowledge Base\n```\n/biotech-kb/\n├── biotech-kb.ttl               ← Aggregated distribution\n├── biotech-kb.jsonld            ← Aggregated distribution\n├── companies/\n│   ├── genentech/               ← Company payload node\n│   └── moderna/                 ← Company payload node\n└── products/\n    ├── drug-x/                  ← Product payload node\n    └── vaccine-y/               ← Product payload node\n```\n\n## Technical Considerations\n\n**Merging logic handles:**\n- **Relative path resolution** - Converting relative URIs to absolute\n- **Prefix consolidation** - Deduplicating namespace declarations\n- **Graph merging** - Combining RDF graphs from multiple sources; de-duplicating\n- **Base URI handling** - Ensuring consistent URI resolution\n\n## Use Cases\n\n- **Ontologies** - Classes and properties from contained nodes\n- **Vocabularies** - Terms and definitions from specialized nodes  \n- **Catalogs** - Dataset metadata from multiple sources\n- **Knowledge bases** - Facts distributed across domain-specific nodes\n- **Configuration data** - Settings aggregated from component services\n\n## Related Concepts\n\n- **[[mesh-resource.node-component.flow.payload]]** - Source datasets for aggregation\n- **[[concept.weave-process]]** - Process that generates aggregated distributions\n- **[[mesh-resource.node-component.flow-shot]]** - Contains the actual distributions being aggregated\n","n":0.054}}},{"i":73,"$":{"0":{"v":"guides","n":1}}},{"i":74,"$":{"0":{"v":"Status","n":1},"1":{"v":"\n# Project Status Overview\n\nThis document provides a high-level summary of the current project state, what is currently working, and any major known issues.\n\n## Current State Summary\n\nThe core Semantic Flow architecture is defined, focusing on the Git-native, filesystem-based mesh structure.\n\n## Working Components\n\n- **Mesh Structure:** The folder hierarchy to IRI mapping is established.\n- **Documentation Site:** The Dendron-based documentation site is functional.\n- **Host Application:** The `sflo-host` Fastify application structure is in place.\n- **Core Concepts:** [[concept.summary]] provides a comprehensive overview of the system.\n\n## Known Issues / Next Focus\n\n- **Weave Process:** The full implementation of the weave process (versioning, promotion, link resolution) is pending.\n- **Configuration:** The two-flow configuration inheritance model is defined but requires implementation.\n- **Agent Integration:** Establishing robust agent workflows and custom mode rules is the current priority.\n\n## Related Status Files\n\n- [[now]] - Detailed focus of current work (big picture)\n- [[todo]] - General list of pending tasks\n- [[progress]] - Log of completed work\n- [[decision-log]] - Log of important project decisions\n","n":0.08}}},{"i":75,"$":{"0":{"v":"Project Brief","n":0.707},"1":{"v":"\n# Semantic Flow (sflo) - Project Brief\n\nThis is the entry point for understanding the Semantic Flow project and its memory bank system.\n\n## Essential Reading (Start Here)\n\n**For AI Agents:** You MUST use the [[dev.memory-bank]] protocol and read the \"Every Task\" Context Files for every new task\n\n## Key Concepts\n\nCore abstractions are documented in detail:\n\n- [[concept.mesh]] - Dereferenceable collection of semantic resources\n- [[mesh-resource.node]] - Atomic unit of a mesh; provides a name to refer to something, and optionally, data about that thing\n- [[mesh-resource.node-component.flow]] - data about the node or thing it names, in the abstract; possibly versioned\n- [[mesh-resource.node-component.flow-shot]] - a version of a flow\n- [[mesh-resource.node-component.snapshot-distribution]] - a file that concretizes a version of a flow\n- [[concept.weave-process]] - Lifecycle operation for versioning/publishing\n\nSee [[concept.summary]] for more depth documentation.\n\n\n\n","n":0.09}}},{"i":76,"$":{"0":{"v":"Product Brief","n":0.707},"1":{"v":"\n# Semantic Flow (sflo) - Product Brief\n\n## What is Semantic Flow?\n\nSemantic Flow is a platform for creating, managing and publishing **semantic meshes** - dereferenceable, versioned collections of data resources where every IRI resolves to meaningful content.\n\n## Twin Purposes\n\n- **Mint dereferenceable IRIs** for referring to things on the Semantic Web\n- **Hold versionable semantic data** that uses those IRIs and can be referenced by other semantic data\n\n## Problems Solved\n\n- **Free, Permanent, Self-Sovereign Data Storage** - Provides individuals with free, permanent, self-describing data storage via the git provider of their choice\n- **IRI Stability** - Provides stable, dereferenceable IRIs for Semantic Web resources\n- **Version Management** - Tracks semantic data evolution with immutable version history\n- **Content Dereferenceability** - Every IRI resolves to meaningful HTML content\n- **Transposability** - Meshes can be moved between domains/projects without breaking internal links\n- **Composability** - Submeshes can be extracted and composed into larger structures\n\n## Components & Applications\n\n### sflo-host\n\nThe main host application that supports semantic mesh use and development. Built with:\n- Fastify web framework\n- Plugin architecture for extensibility\n- TypeScript for type safety\n\n#### Plugins\n\n- [[product.plugins.api-docs]] - API documentation/playground (Stoplight Elements)\n- [[product.plugins.mesh-server]] - Static mesh server(s)\n- [[product.plugins.sflo-web]] - Web UI\n- [[product.plugins.sflo-api]] - OpenAPI REST endpoint\n- [[product.plugins.sparql-readonly]] - SPARQL read-only endpoint\n- [[product.plugins.sparql-update]] - SPARQL write-capable endpoint (provided by Comunica)\n- [[product.plugins.sparql-editor]] - SIB Swiss editor at /play\n\n### Shared Packages\n\n- **@semantic-flow/config** - Configuration management\n\n## How It Works\n\n### Filesystem-Based Meshes\n\nMeshes map directly from Git repository folder hierarchies to published static sites:\n\n- Every folder is a **node** (container for resources and child nodes)\n- Nodes contain **components** (flows, handles, assets, documentation)\n- **Flows** are versioned DatasetSeries (metadata, semantic data, arbitrary datasets, or config)\n- **FlowShots** are flow realizations (`_default/`, `_working/`, snapshot folders like `2025-11-24_0142_07_v1/`)\n- **Distributions** are serialization files (TriG, JSON-LD, etc.)\n\n### The Weave Process\n\nThe weave process maintains mesh coherence and publication readiness:\n\n- Ensures required system components exist\n- Creates new snapshots from working data\n- Promotes working data to current\n- Updates metadata and provenance\n- Regenerates resource pages\n- Resolves internal links for transposability\n\n## User Experience Goals\n\n### For Developers\n\n- **Git-native workflow** - Meshes are just Git repositories\n- **Static site deployment** - Push to GitHub Pages or any static host\n- **Type-safe development** - TypeScript throughout\n- **Plugin extensibility** - Extend functionality through plugins\n\n### For Semantic Web Users\n\n- **Dereferenceable IRIs** - Every IRI resolves to content\n- **Version history** - Immutable snapshots for precise citation\n- **Human-friendly** - Resource pages provide context and navigation\n- **Machine-readable** - RDF distributions for automated processing\n\n### For AI Agents\n\n- **Clear structure** - Predictable folder/file organization\n\n## Related Documentation\n\n- [[concept.summary]] - Comprehensive concept documentation\n- [[concept.mesh]] - Mesh definition and requirements\n- [[concept.weave-process]] - Weave process details\n- [[principle.transposability]] - Transposability principle\n- [[principle.composability]] - Composability principle\n","n":0.049}}},{"i":77,"$":{"0":{"v":"Ontologies","n":1}}},{"i":78,"$":{"0":{"v":"Best Practices","n":0.707},"1":{"v":"\n## Use Relative Identifiers for Intramesh References\n\nSee \n","n":0.354}}},{"i":79,"$":{"0":{"v":"mesh folder","n":0.707},"1":{"v":"\nSemantic meshes are represented in filesystems as a collection of folders and files. Mesh folders correspond to [[facet.resource.naming]].\n\nWith the exception of [[folder._assets]], **mesh folders** should only contain other [[mesh-resource]], i.e., other mesh folders and [[file]], no \"arbitrary files.\"\n","n":0.162}}},{"i":80,"$":{"0":{"v":"Snapshot folder","n":0.707},"1":{"v":"\n- the name of [[mesh-resource.node-component.flow-shot.snapshot]] folders, and therefore part of their IRIs.\n- format: `YYYY-MM-DD_HHMM_SS_vN` (e.g., `2025-11-24_0142_07_v1`)\n- combines the [[concept.weave-label]] (`YYYY-MM-DD_HHMM_SS`) with the sequence number from [[concept.flow-version]] (`_vN`)\n","n":0.192}}},{"i":81,"$":{"0":{"v":"node folder","n":0.707},"1":{"v":"\n## Definition\n\nA node folder is any folder that maps to a [[mesh-resource.node]]. Each node folder extends the namespace with its name and has a concept IRI that may refer to something. \n\n- What a “namespace” is: see [[concept.namespace]]\n- General node types and anatomy: see [[mesh-resource.node]]\n\n## Minimal requirements\n\n- Every node folder must contain:\n  - [[_node-handle/|folder._node-handle]]\n  - [[_meta/|folder._meta]]\n\n## Node-specific flows (by type)\n\n- [[bare node|mesh-resource.node.bare]]: no additional flows \n- [[dataset node|mesh-resource.node.payload]]: requires [[_dataset-flow/|folder._dataset-flow]]\n\nDistributions must live inside flow snapshot folders (e.g., `_default/`, `_working/`, snapshot folders like `2025-11-24_0142_07_v1/`). See [[resource.node-component.flow]] and [[resource.node-component.flow-snapshot]].\n\n## Example\n\n```file\n/my-node/                     # node folder → https://ex.org/my-node/\n├── _node-handle/             # required\n├── _meta/               # required\n└── _dataset-flow/               # required for dataset nodes\n","n":0.098}}},{"i":82,"$":{"0":{"v":"_working","n":1},"1":{"v":"\nPhysical manifestation of (and slug for)  [[mesh-resource.node-component.flow-shot.working-shot]]\n\nThese folders should only contain a single \"[[mesh-resource.node-component.snapshot-distribution.working]]\", i.e., a single RDF syntax\n","n":0.229}}},{"i":83,"$":{"0":{"v":"_ref flow folder","n":0.577},"1":{"v":"\nThe filesystem folder of a [[mesh-resource.node-component.flow.reference]] dataset.\n","n":0.378}}},{"i":84,"$":{"0":{"v":"_payload flow folder","n":0.577},"1":{"v":"\nThe filesystem folder of a [[mesh-resource.node-component.flow.payload]] dataset\n","n":0.378}}},{"i":85,"$":{"0":{"v":"_node-handle folder","n":0.707},"1":{"v":"\nEvery [[folder.node]] contains a **node handle folder** which corresponds to its [[mesh-resource.node-component.node-handle]]. \n\n","n":0.277}}},{"i":86,"$":{"0":{"v":"_meta flow folder","n":0.577},"1":{"v":"\nThe filesystem container of the [[mesh-resource.node-component.flow.node-metadata]]\n","n":0.408}}},{"i":87,"$":{"0":{"v":"_default","n":1},"1":{"v":"\n- filesystem representation and slug for  [[mesh-resource.node-component.flow-shot.default-shot]]\n","n":0.378}}},{"i":88,"$":{"0":{"v":"_cfg-op flow folder","n":0.577},"1":{"v":"\nThe physical representation of the [[mesh-resource.node-component.flow.node-config.operational]]\n","n":0.408}}},{"i":89,"$":{"0":{"v":"_cfg-inh flow folder","n":0.577},"1":{"v":"\nThe physical representation of the [[mesh-resource.node-component.flow.node-config.inheritable]]\n","n":0.408}}},{"i":90,"$":{"0":{"v":"assets tree folder","n":0.577},"1":{"v":"\n- correspond to [[mesh-resource.node-component.asset-tree]]s \n","n":0.447}}},{"i":91,"$":{"0":{"v":"mesh file","n":0.707},"1":{"v":"\n- [[mesh-resource.node-component.snapshot-distribution]]\n- [[mesh-resource.node-component.documentation-resource]\n- ]\n\n","n":0.5}}},{"i":92,"$":{"0":{"v":"Features","n":1}}},{"i":93,"$":{"0":{"v":"Handling Renaming","n":0.707},"1":{"v":"\n- if a namespace-iri is changed, gulp, the old one can be preserved with a \"redirect\" predicate in the [[mesh-resource.node-component.flow.node-metadata]] and a warning on the html page\n","n":0.192}}},{"i":94,"$":{"0":{"v":"Check Namespace before Creating","n":0.5},"1":{"v":"\n- when creating a new repo, sflow should check whether there's an existing folder with the same name in the parent user/org site.\n- best practice is probably not to put an sflow-site at the user/org level? At least if it might have repos someday.\n\n## References\n\n- [[issue.github-bare-namespace-can-overlap-with-repo-namespaces]]\n","n":0.147}}},{"i":95,"$":{"0":{"v":"Changing Historical Datasets","n":0.577},"1":{"v":"\n- it's better if you don't have to, but if you do... \n  - maybe hashes should be stored for distributions, so people can detect that they've changed.","n":0.189}}},{"i":96,"$":{"0":{"v":"FAQ","n":1},"1":{"v":"\n# Semantic Flow Frequently Asked Questions\n\nThis section addresses common questions about Semantic Flow design principles and architecture.\n\n## Design Principles\n\n### [[Why don't bare nodes have reference flows?|faq.why-dont-namespace-and-data-nodes-have-reference-flows]]\nWhy should a namespace have to refer to something?\n\n### [[Why are there components at the top of a repo?|faq.why-are-there-components-at-the-top-of-a-repo]]\nThe repository root can be a mesh node, in which case `_meta/` and `_node-handle/` components appear at the top level. \n\n## Architecture Questions\n\n*More FAQ entries will be added as common questions arise.*\n\n---\n\n**Contributing to FAQ**: If you encounter questions that would benefit from clear explanations, consider adding them to this FAQ section following the established pattern.\n","n":0.101}}},{"i":97,"$":{"0":{"v":"Why Not Use Git Semantics for Versioning","n":0.378},"1":{"v":"\n// TODO","n":0.707}}},{"i":98,"$":{"0":{"v":"Why Dont payload nodes Contain Distributions Directly","n":0.378},"1":{"v":"\n## Question\n\nWhy don't [[payload nodes|mesh-resource.node.payload]] contain distribution files directly? Why do I need to go to `_default/` to find the actual data?\n\n## Answer\n\npayload nodes represent **abstract data concepts**, not concrete data instances. This separation provides several important benefits:\n\n### Clear Semantic Distinction\n\n- **payload node** (`/ns/monsters/`): \"The concept of monster data\"\n- **Data compound** (`/ns/monsters/_payload/`): \"The abstract dataset associated with the monster data concept\" \n- **Data compound layers**: the current, next and historical versions of the dataset\n\nThis allows you to reference the concept separately from the associated abstract or concrete dataset.\n\n### Stable Identity\n\nThe payload node and data compound provide permanent, stable identifier for the concept and its payload dataset that persist even as the concrete data changes over time. You can always refer to \"monster data as a concept\" using `/ns/monsters/` regardless of how many versions exist.\n\n### Temporal Organization\n\nBy separating the concept from concrete instances, payload nodes can cleanly organize different temporal states:\n- `_default/` - current data\n- `_working/` - draft changes\n- Snapshot folders (e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`) - historical versions\n\n### Consistent Architecture\n\nThis mirrors how [[reference nodes|concept.mesh.resource.node.reference]] work:\n- **Reference nodes**: Abstract entity concept + `_ref/` node component with concrete data\n- **payload nodes**: Abstract data concept + `_default/` component with concrete data\n\n### Metadata Separation\n\nThe payload node's [[metadata flow|mesh-resource.node-component.flow.node-metadata]] contains system metadata about the data concept and its components, while each [[mesh-resource.node-component.flow.payload]] can also contain (concept-specific) metadata.\n\nTODO: example\n\n\n## Analogy\n\nThink of it like a library:\n- **payload node** = \"The concept of the Encyclopedia Britannica\"\n- **payload flow** = The Encyclopedia Britannica as an ongoing series of editions\n- **[[mesh-resource.node-component.flow-shot]]** = Specific editions (1990 edition, 2020 edition, current edition)\n\nYou can refer to \"Encyclopedia Britannica\" as a general concept or as a series without specifying which edition, or you can reference a specific edition when you need concrete data.\n","n":0.059}}},{"i":99,"$":{"0":{"v":"Why are there components at the top of a repo?","n":0.316},"1":{"v":"\n## Question\n\nWhy are there [[mesh node components|mesh-resource.node-component]] like `_meta/`, `_node-handle/`, and `_assets/` at the top level of a repository? Shouldn't components only be inside nodes?\n\n## Answer\n\nComponents at the repository root exist because **the repository root itself is a [[mesh node|mesh-resource.node]]** - specifically, it's the [[root node|concept.root-node]] of the mesh.\n\n### Repository Root = Mesh Root Node\n\nEvery semantic mesh has a root node, and in a repository-based mesh, the repository root **is** that root node. It's a \"nameless\" node locally (represented as \"/\") that can be any type of mesh node:\n\n- **bare node**: If the repo organizes other nodes\n- **dataset node**: If the repo represents a single dataset  \n- **Reference node**: If the repo represents an external entity\n\nSince the repository root is a mesh node, it follows the same rules as any other node and must contain:\n\n- **`_meta/`**: corresponds to the [[mesh-resource.node-component.flow.node-metadata]] with administrative metadata for the root node\n- **`_node-handle/`**: corresponds to [[node handle|resource.node-component.node-handle]] for referential indirection\n\nThe root node may contain **other components**: Depending on the root node type (e.g., `_ref/` for reference nodes, `_data/` for versioned datasets)\n\n### Consistency Principle\n\nThis maintains architectural consistency: **every mesh node has the same structure and capabilities**, whether it's nested deep in the hierarchy or at the repository root. The root node isn't special - it's just the top-level node in the mesh hierarchy.\n\n### Mesh Self-Containment\n\nThis design also supports the principle that **any subtree is a complete mesh**. The repository root, being a proper mesh node with all its components, ensures the entire repository is a self-contained, functional semantic mesh.\n","n":0.063}}},{"i":100,"$":{"0":{"v":"What Is the referential difference between a payload node's IRI and its payload flow's IRI","n":0.258},"1":{"v":"\nA **payload node IRI** (e.g. `ns/djradon/bio/`) identifies the **abstract dataset itself** — it's a dereferenceable identifier for an RDF dataset\nA **payload flow IRI** (e.g. `ns/djradon/bio/_payload`) identifies the **semantic flow that captures the evolution of that dataset** — metadata about *how* the payload is produced, validated, versioned, or transformed.\n\n| Aspect                        | Payload Node (`ns/djradon/bio/`)                                             | Payload Flow (`ns/djradon/bio/_payload`)                           |\n| ----------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n| **Referential role**          | Refers to the dataset *as content*                                           | Refers to the dataset’s *production process*                       |\n| **Ontological category**      | `sflo:PayloadNode` (or equivalent)                                           | `sflo:PayloadFlow` (a subclass of `sflo:Flow`)                     |\n| **Nature**                    | Public, stable identifier for consumers                                      | Internal, operational metadata for publishers                      |\n| **Dereferencing expectation** | Returns the default dataset or its default distribution (`index.trig`, etc.) | Returns RDF describing the flow’s inputs, transformations, outputs |\n| **Persistence**               | Semi-permanent, versioned through dataset series (`_series/_main/v1/...`)    | May change across builds, describing how payload evolves           |\n| **Relations**                 | `sflo:hasPayloadFlow <_payload>`                                             | `sflo:producesPayload <bio/>` (inverse)                            |\n\nIn short:\n\n* **`/bio/`** = “the data about Bio.”\n* **`/bio/_payload`** = “the process that emits `/bio/`.”\n\nThe node is the *what*; the flow is the *how*.\n","n":0.074}}},{"i":101,"$":{"0":{"v":"Should Data about a Referent Go in Its Reference Flow or a Contained Payload Flow","n":0.258}}},{"i":102,"$":{"0":{"v":"Reference Iri Choices","n":0.577},"1":{"v":"\nRDF supports different, confusingly-named approaches to resource referencing, each with tradeoffs.\n\n\n## TLDR: **Choosing between approaches:**\n\n- Use **relative-path relative IRIs** for maximum composability when embedding or importing meshes and submeshes\n- Use **absolute-path relative IRIs** for clearer namespace context and better support for moving submeshes within the same mesh hierarchy\n- Use **absolute IRIs** only for cross-mesh references\n- Relative and absolute paths both preserve relationships when moving complete meshes between domains\n\n\n\n## Absolute IRI References\n\nAny IRI that has a scheme (e.g., http:) is an **Absolute IRI** \n\n### Example\n\nThis example uses two absolute IRIs, one using the \"ex:\" prefix for the example.com authority:\n\n```ttl\n@prefix ex: <https://example.com/> .\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_ref/_default/djradon_ref.trig\nex:djradon a foaf:Person ;\n   rdfs:seeAlso ex:djradon/index.html .\n```\n\n### Pros\n\n- explicit\n  \n### Cons\n  \n-  limits [[principle.transposability]] and [[principle.composability]]\n  - e.g., if you moved mesh hosting away from `https://example.com`, the `foaf:Person` and `rdfs:seeAlso` assertions would still refer to the original references\n- not ideal if you're:\n  - making updates\n  - working offline\n\n\n## Relative IRIs\n\nAny IRI that lacks a scheme (e.g., http:) is resolved against a base IRI following RFC 3986. Such relative IRI references come in three distinct forms:\n\n- Network-path reference — begins with //.\n  - Example: //other.org/x → inherits the base’s scheme, e.g. http://other.org/x.\n\n- Absolute-path reference — begins with / but not //.\n   - Example: /foo/bar → keeps the base’s scheme and authority, resets the path, e.g. http://example.org/foo/bar.\n\n- Relative-path reference — does not begin with / or //.\n   - Example: foo/bar or ../foo → inherits the base’s scheme, authority, and path context, e.g. http://example.org/base/foo/bar.\n\nIf no base is specified, an inferred base of the requested scheme and authority is used. **This behaviour is essential to Semantic Flow [[Best Practices|guide.best-practices]].**\n\n\n### Relative-Path Relative IRIs\n\n\n```turtle\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_ref/_default/djradon_ref.trig\n<../../../djradon/> a foaf:Person ;          # The document itself\n   foaf:knows <../../alice/> ;           # A sibling node in the mesh\n   rdfs:seeAlso <../bio/bio.html> .      # A resource page contained in a \"bio\" node under ../../djradon/\n```\n\n#### Pros\n\n- maximum composability\n\n#### Cons\n\n- `../../../` makes eyes swim\n\n### Absolute-Path Relative IRIs\n  \n```turtle\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_ref/_default/djradon_ref.trig\n</ns/djradon/> a foaf:Person ;\n   foaf:knows </ns/alice/> ;          # Clear namespace context\n   rdfs:seeAlso </ns/djradon/bio/bio.html> .\n```\n\n\n#### Pros\n\n- clearer context\n- good intra-mesh transposability\n\n\n#### Cons\n\n- composability requires re-computing paths\n","n":0.053}}},{"i":103,"$":{"0":{"v":"How Is a Flow Different from a Dataset Series","n":0.333},"1":{"v":"\n- a [[mesh-resource.node-component.flow]] IS a dataset series, but\n  - it has  \n- a dataset series isn't necessarily a succession of versions of the same referent.\n  - e.g. US Census Data (2010–2020 releases)\n","n":0.177}}},{"i":104,"$":{"0":{"v":"Do payload nodes support DatasetSeries?","n":0.447},"1":{"v":"\nA [[mesh-resource.node.payload]] can only support a single-file distribution, but DatasetSeries can be represented in a single file with multiple [[concept.named-graphs]]. Those named graphs should be named with [[concept.fragment-identifiers]] to preserve [[principle.dereferencability-for-humans]]. \n\nNote that not all RDF serialization formats support named graphs.\n","n":0.156}}},{"i":105,"$":{"0":{"v":"facets","n":1},"1":{"v":"\nA way of categorizing a [[concept.platform-element]]\n","n":0.408}}},{"i":106,"$":{"0":{"v":"user facet","n":0.707},"1":{"v":"\n[[Platform elements|concept.platform-element]] with the **user facet** are things that are primarily created or altered by users of the system.\n","n":0.229}}},{"i":107,"$":{"0":{"v":"system facet","n":0.707},"1":{"v":"\n[[concept.platform-element]]s with the **system facet** are things that are primarily managed by the system.\n","n":0.267}}},{"i":108,"$":{"0":{"v":"mesh resource facets","n":0.577}}},{"i":109,"$":{"0":{"v":"naming resource","n":0.707},"1":{"v":"\nA resource whose slash-terminated [[concept.identifier]] establishes a name within a mesh, optionally denoting a non-file thing. Naming resources may provide namespace context for contained resources and may or may not have a referent.\n","n":0.174}}},{"i":110,"$":{"0":{"v":"dataset facet","n":0.707},"1":{"v":"\nIn the RDF universe, a dataset is a collection of one or more RDF graphs.\n\nIn a [[concept.mesh]], there are two kinds of datasets:\n\n- [[mesh-resource.node-component.flow]]s are dcat:DatasetSeries (which are also dcat:Dataset) and represent an \"abstract dataset\": they don't have concrete distributions of their own. \n  - Flow data is only materialized in [[mesh-resource.node-component.flow-shot]] distributions.\n  - Metadata about the various flows is consolidated in the node's [[mesh-resource.node-component.flow.node-metadata]] in the form of distributions.\n  - [[mesh-resource.node.payload]]s contain a \"payload flow\" and their [[concept.identifier.intramesh]]s refer to that payload dataset in the abstract. The dataset is still contained in a particular kind of [[mesh-resource.node-component.flow]], i.e., a [[mesh-resource.node-component.flow.dataset]]. But the flow's identifier refers to a particular flow, whereas the node identifier will become the canonical IRI of the dataset (on [[concept.publication]]).\n    - in the case of [[mesh-resource.node-component.flow.dataset]], they can store their Dataset metadata in the dataset itself, or in their [[mesh-resource.node-component.flow.reference]], or both.\n  \n- [[mesh-resource.node-component.flow-shot]]s are dcat:Dataset and represent a specific version of their abstract datasets; they have one or more [[snapshot distributions|mesh-resource.node-component.snapshot-distribution]]. \n\n","n":0.077}}},{"i":111,"$":{"0":{"v":"Content","n":1},"1":{"v":"\nA resource whose filename-terminated [[concept.identifier]] refers to an information resource \n","n":0.302}}},{"i":112,"$":{"0":{"v":"internal facet","n":0.707},"1":{"v":"\n[[concept.identifier]]s and [[concept.referent]] with the **internal facet** denote things that aren't contained in a mesh, i.e., [[mesh-resource.node-component]] or, with \n","n":0.224}}},{"i":113,"$":{"0":{"v":"flow facets","n":0.707}}},{"i":114,"$":{"0":{"v":"versioned flow facet","n":0.577},"1":{"v":"\nA [[mesh-resource.node-component.flow]] whose versions are being kept around.\n\nPhysically, the historical versions are located in snapshot folders with the format `YYYY-MM-DD_HHMM_SS_vN`, e.g. `2025-11-24_0142_07_v1` or `2025-11-24_0142_08_v9999`.\n","n":0.204}}},{"i":115,"$":{"0":{"v":"v-series flow facet","n":0.577},"1":{"v":"\nFlows with the **v-series flow facet** have at least one historical checkpoint, i.e. at some point a [[concept.weave-process]] has generated a [[mesh-resource.node-component.flow-shot.snapshot]].\n\n\nA [[facet.flow.versioned]] collects [[Version|mesh-resource.node-component.flow-shot.snapshot]], so a v-series flow probably had versioning turned on for at least one weave.\n","n":0.16}}},{"i":116,"$":{"0":{"v":"unversioned flow facet","n":0.577},"1":{"v":"\nAn unversioned node flow has never had [[concept.flow-version]] turned on for a [[concept.weave-process]], so it doesn't have any [[mesh-resource.node-component.flow-shot.snapshot]]\n\nIt's useful for datasets that shouldn't change much.\n","n":0.196}}},{"i":117,"$":{"0":{"v":"de-versioned flow facet","n":0.577},"1":{"v":"\nDe-versioned node flows are [[mesh-resource.node-component.flow]] that were once versioned, but have had their versioning turned off. So there are probably [[facet.flow.v-series]].\n","n":0.218}}},{"i":118,"$":{"0":{"v":"filesystem facets","n":0.707}}},{"i":119,"$":{"0":{"v":"folder resource facet","n":0.577},"1":{"v":"\nA mesh when stored in a filesystem is physically structured with mesh folders, which correspond to RDF resources and their [[concept.identifier.intramesh]]\n  \nWhen a mesh gets published, the folders also correspond to [[concept.identifier]]. \n\nAll folder-based resources should contain a [[mesh-resource.node-component.documentation-resource.resource-page]]\n\n\n## Types\n\n### System Folders\n\n#### Node Handle Folders\n\n- [[concept.mesh.resource.folder._node-handle]] correspond to the [[mesh-resource.node-component.node-handle]]\n\n#### Flow (Abstract Dataset) Folders\n\n- **`_meta/`**\n  - correspond to [[mesh-resource.node-component.flow.node-metadata]]\n  - present in all mesh nodes\n  \n- **`_dataset-flow/`**\n\n  - correspond to the [[mesh-resource.node-component.flow.dataset]]\n  - contain the dataset associated with the [[mesh-resource.node.payload]]\n\n#### Snapshot (Concrete Dataset) System Folders\n\n- **`_default/`**\n\n- **Snapshot folders** (format: `YYYY-MM-DD_HHMM_SS_vN/`, e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`, …)\n\n  - snapshot folders that represent [[mesh-resource.node-component.flow-shot.snapshot]]\n  - each holds one or more distribution file\n  - **Fully terminal**—neither user-nodes nor system-folders may live inside.\n\n#### Snapshot User Folders\n\n- **`_next/`**\n  - Where edits get made to [[facet.flow.versioned]]\n\n\n#### Other User Folders\n\n- **`_assets/`**\n  - Holds static user assets (images, CSS, binaries).\n  - **Always terminal** - never contains nodes\n  - Ignored by the mesh scanner; asset trees carry no flows; any metadata about assets should live in the parent node’s meta flow.\n","n":0.077}}},{"i":120,"$":{"0":{"v":"file resource facet","n":0.577},"1":{"v":"\nResource files are returned directly when accessed by their [[concept.identifier]].\n\n## Types\n\n- [[mesh-resource.node-component.documentation-resource]]\n  - [[mesh-resource.node-component.documentation-resource.resource-page]] (system-generated)\n  - [[mesh-resource.node-component.documentation-resource.changelog]]\n  - [[mesh-resource.node-component.documentation-resource.readme]]\n- [[mesh-resource.node-component.snapshot-distribution]]\n- [[mesh-resource.node-component.aggregated-distribution]]\n- ","n":0.218}}},{"i":121,"$":{"0":{"v":"external facet","n":0.707},"1":{"v":"\n[[concept.identifier]]s and [[concept.referent]] with the **external facet** denote things \"in the world\", whether real or imaginary.\n","n":0.25}}},{"i":122,"$":{"0":{"v":"Dev","n":1}}},{"i":123,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n## Available Scripts\n\n- `pnpm test` - Run all tests once\n- `pnpm test:watch` - Run tests in watch mode (automatically re-runs tests when files change)\n- `pnpm test:ui` - Run tests with Vitest UI (visual interface in browser)\n- `pnpm test:coverage` - Run tests with coverage report\n\n## Watch Mode\n\nWatch mode automatically re-runs your tests whenever you save changes to:\n- Test files (`.test.ts`, `.spec.ts`)\n- Source files being tested\n- Dependencies of those files\n\nThis provides instant feedback during development - you can see test results immediately after making changes without manually re-running tests.\n\n## Debugging Tests\n\n1. **Open a test file** in VSCode\n2. **Set breakpoints** in the test or source code\n3. **Select \"Debug Current Test File\"** configuration\n4. **Press F5** to debug the current test file\n\n## Test Structure\n\nTests are located in `__tests__` directories within each package:\n- `sflo-host/src/__tests__/` - Tests for the host service\n- Add similar directories in other packages as needed\n","n":0.084}}},{"i":124,"$":{"0":{"v":"Patterns","n":1},"1":{"v":"\n# Development Patterns\n\nThis document captures recurring architectural and code patterns used throughout the Semantic Flow project.\n\n## Architectural Patterns\n\n### Comunica SPARQL vs Quadstore Primitives for data access\n\nBoth can win. Pick by query shape.\n\nUse Quadstore primitives (`get`, `getStream`, `match`) when:\n\n* One or two triple patterns with fixed IRIs/literals.\n* You can drive lookups by known keys and stop early.\n* You need strict control over streaming, batching, or a read-modify-write cycle.\n* You want zero SPARQL parse/plan overhead.\n\nUse SPARQL via Comunica when:\n\n* Three or more patterns with joins, OPTIONAL/UNION, FILTER, ORDER BY, GROUP BY, LIMIT.\n* You’d benefit from join reordering, filter/projection pushdown, and early result streaming.\n* You might federate later or swap sources without rewriting app code.\n\nWhy primitives can be faster on “simple”:\n\n* Direct index hits with no planner cost.\n* Tight loops with `for await...of` and immediate early-exit.\n* You can pre-narrow with exact keys or prefix scans and avoid any join at all.\n\nA practical split for internal data access:\n\n* “Path to one thing” lookups (by IRI, by type, by id): primitives.\n* Graph navigation with 3+ hops or any aggregation/sorting: SPARQL.\n\nHybrid patterns that work well:\n\n* Use primitives to fetch candidate IRIs, then pass them into a SPARQL `VALUES` clause.\n* Pre-materialize small “views” (denormalized quads) you hit often, then query them with SPARQL.\n* Keep SPARQL templates for common shapes; fall back to primitives for hot key-lookups.\n\nImplementation notes:\n\n* Consume streams with `for await (const q of stream)`; await completion at the boundary with `stream/promises` `finished()` or `pipeline()`.\n* Reuse a single Comunica engine instance to amortize init cost.\n* With Quadstore, structure data so frequent lookups align with available index permutations; primitives shine when you can select by the leading fields.\n\nRule of thumb:\n\n* Simple, key-oriented, latency-sensitive ⇒ primitives.\n* Anything with joins/options/ordering/aggregation ⇒ SPARQL.\n\n### Stream Patterns\n\nUse async/await for boundaries (start/finish), and use async iteration for the stream body.\n\n* Promises: use `await` for file I/O (`fs/promises`), HTTP fetches, initialization, and “collect-all” helpers that intentionally materialize results.\n\n* Streaming RDF (RDF/JS, Comunica, rdf-parse/serialize, rdf-ext):\n\n  * Prefer async iterators:\n\n    ```ts\n    // quadStream implements AsyncIterable<Quad>\n    for await (const quad of quadStream) {\n      // process quad\n    }\n    ```\n\n    This gives proper backpressure. Do not `.on('data', ...)` and `await` inside the handler.\n  * If a sink uses RDF/JS `Sink#import(source)`, await completion with Node’s stream utilities:\n\n    ```ts\n    import { finished } from 'stream/promises';\n\n    const writer = serializer.import(quadStream); // returns a Node stream\n    await finished(writer); // resolves on 'finish' or rejects on error\n    ```\n  * For stream pipelines, use `pipeline`:\n\n    ```ts\n    import { pipeline } from 'stream/promises';\n\n    await pipeline(sourceStream, transformA, transformB, destStream);\n    ```\n  * Comunica result streams (bindings/quad streams) also support async iteration:\n\n    ```ts\n    const { data } = await engine.query('CONSTRUCT {...}', { sources });\n    for await (const quad of data) { /* ... */ }\n    ```\n\n* Collecting small results only:\n\n  ```ts\n  import arrayifyStream from 'arrayify-stream';\n  const quads = await arrayifyStream(quadStream); // OK for small datasets\n  ```\n\n  Avoid this for large data.\n\n* Writing to stores:\n\n  ```ts\n  // RDF/JS store that exposes import()\n  const importing = store.import(quadStream);\n  await finished(importing);\n  ```\n\nRule of thumb to include:\nUse `await` for Promises and stream completion. Use `for await...of` to consume streaming RDF. Avoid `await` inside `'data'` listeners and avoid buffering everything unless you explicitly need it.\n\n### Error Handling and Logging System Patterns\n\nsee [[dev.logging-and-error-handling]]\n\n\n","n":0.044}}},{"i":125,"$":{"0":{"v":"Memory Bank","n":0.707},"1":{"v":"\n# Dev Memory Bank\n\nThe memory bank is a git-based shared memory system that enables both humans and AI agents to maintain context across sessions and tasks. It consists of:\n\n- project-level documentation files that capture essential knowledge, decisions, and current state\n- task-based documentation that should reflect the agent's short-term \"working\" memory.\n\n## Core Rules\n\n- **CRITICAL: Agents MUST read ALL \"Every Task\" memory bank files at the start of EVERY task.**\n- Agents should use a \"Task\" memory bank file for every task\n- Agents, in every mode, must try to keep the task file's TODO section synced with their own internal Todo List.\n  - Use of the \"Todo List Updated\" (update_todo_list) tool should always trigger a resync of the task's TODO section\n- All memory bank files should avoid repeating information, and should be continually checked for internal consistency\n\n## Memory Bank Structure\n\n### Project Memory Bank Files\n\n#### \"Every Task\" Context Files\n\nThese files MUST be read at the start of every new task:\n\n- [[guide.project-brief]] - Foundation document explaining the memory bank approach and pointing to other files\n- [[guide.product-brief]] - Project vision, problems solved, components/applications, user experience goals\n- [[guide.status]] - Current project status, what's working, high-level summary\n- [[dev.memory-bank]] - This file; ground rules for the memory bank system\n\n#### Frequently Referenced Memory Bank Files\n\nThese should be consulted as appropriate to the task:\n\n- [[dev.general-guidance]] - for any development-related tasks\n- [[dev.dependencies]] - for technical architecture questions or any development-related tasks\n- [[guide.ontologies]] - for any RDF-related tasks\n- [[dev.debugging]] - Debugging workflows and tips\n\n\n#### Big Picture Memory Bank Files\n\n- [[now]] - Current work focus (big-picture)\n- [[todo]] - General task list; items not yet broken into formal tasks\n- [[progress]] - Completed tasks with dated summaries\n- [[decision-log]] - Important project-level decisions with dates\n\n### Task Memory Bank Files\n\nFor each task, use a task file in documentation/: `tasks.YYYY-MM-DD-task-name.md`\n\nRequired sections:\n- **Prompt** - Original task request\n- **TODO** - Agent's todo list (mirrored from Roo's internal list); must be updated after every update_todo_list tool invocation\n  - TODO items should have the \"- [ ] \" form, so we can x them off as we go.\n- **Decisions** - Task-specific decisions made during execution\n\n## Maintenance Guidelines\n\n1. **Keep it current** - Update files as work progresses\n2. **Keep it concise** - Less is more; avoid repetition\n3. **Use wikilinks** - Link between documentation files using `[[filename]]` syntax\n4. **Date entries** - Use `## YYYY-MM-DD` format for dated logs\n5. **Ask questions** - If documentation is confusing or outdated, ask for clarification\n\n## Agent Responsibilities\n\n- Read all \"Every Task Context\" files before starting work\n- Update the task file's TODO section to mirror your internal todo list\n- Document decisions in the task file's Decisions section\n- Suggest updates to project-level memory bank files when appropriate\n- Keep documentation pithy, consistent, and current\n","n":0.048}}},{"i":126,"$":{"0":{"v":"Logging and Error Handling","n":0.5},"1":{"v":"\n# Logging and Error Handling\n\nDeveloper guide for the `@semantic-flow/logging` package - a production-ready logging and error handling system for the Semantic Flow Node.js platform.\n\n## Overview\n\nThe logging system provides:\n- **Unified API** for both CLI tools and long-running services\n- **Structured logging** with JSON Lines output format\n- **Context propagation** using AsyncLocalStorage\n- **Type-safe error handling** with custom error types\n- **Multiple output channels** (console, file, monitoring)\n- **Performance tracking** with built-in timers\n- **Pure ESM** with full TypeScript support\n\n## Quick Start\n\n### Basic Usage\n\n```typescript\nimport { getLogger } from '@semantic-flow/logging';\n\nconst logger = getLogger();\n\nlogger.info('Application started');\nlogger.debug('Debug information', { userId: '123' });\nlogger.error('Something went wrong', new Error('Failed'), { operation: 'db-query' });\n```\n\n### CLI Tool Usage\n\n```typescript\nimport { createCliLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({ \n  verbose: process.argv.includes('--verbose'),\n  format: 'pretty' \n});\n\nlogger.info('Processing files...');\n```\n\n### Service Usage\n\n```typescript\nimport { createServiceLogger, ContextManager } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('semantic-flow-api', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: process.env.NODE_ENV\n});\n\n// HTTP middleware with automatic context propagation\napp.use((req, res, next) => {\n  const context = {\n    operation: 'http-request',\n    requestId: req.id,\n    metadata: { method: req.method, path: req.path }\n  };\n  \n  ContextManager.run(context, () => {\n    req.logger = logger.child({ component: 'http-handler' });\n    next();\n  });\n});\n```\n\n### Component-Scoped Logger\n\n```typescript\nimport { getComponentLogger } from '@semantic-flow/logging';\n\n// Automatically includes component name from file path\nconst logger = getComponentLogger(import.meta.url);\n\nlogger.info('Component initialized'); // Logs with component: 'my-module'\n```\n\n## Core Concepts\n\n### Log Levels\n\nThe system uses numeric log levels for easy comparison and filtering:\n\n```typescript\nenum LogLevel {\n  TRACE = 0,   // Detailed trace information\n  DEBUG = 10,  // Debug information\n  INFO = 20,   // Informational messages\n  WARN = 30,   // Warning messages\n  ERROR = 40,  // Error messages\n  FATAL = 50   // Fatal errors that require attention\n}\n```\n\n### Log Context\n\nContext is automatically merged and propagated:\n\n```typescript\ninterface LogContext {\n  // Core identification\n  operation?: string;\n  operationId?: string;\n  component?: string;\n  function?: string;      // Automatically captured calling function name\n  \n  // Semantic Flow specific\n  meshId?: string;\n  nodeId?: string;\n  meshName?: string;\n  nodeName?: string;\n  \n  // Performance tracking\n  startTime?: number;\n  duration?: number;\n  memoryUsage?: number;\n  \n  // Request context\n  requestId?: string;\n  userId?: string;\n  sessionId?: string;\n  \n  // Flexible metadata\n  tags?: Record<string, string>;\n  metadata?: Record<string, unknown>;\n}\n```\n\n#### Function Name Capture\n\nThe logger can automatically capture the calling function or method name and include it in the log context. This feature is useful for debugging and tracing but has a performance cost.\n\n**Automatic Capture:**\n```typescript\nconst logger = getLogger();\n\nasync function processUserData(userId: string) {\n  // Function name 'processUserData' automatically captured\n  logger.info('Starting user data processing', { userId });\n  // Logs: { function: 'processUserData', userId: '123', ... }\n}\n```\n\n**Configuration:**\n```typescript\ninitLogger({\n  serviceName: 'my-service',\n  autoContext: {\n    captureFunctionName: true,  // Enable function name capture\n    includeTimestamp: true,\n    includeHostname: true,\n    includeProcessInfo: true\n  }\n});\n```\n\n**Environment-Aware Defaults:**\n- **Development**: Function capture is enabled by default (`NODE_ENV !== 'production'`)\n- **Production**: Function capture is disabled by default for performance\n- **Override**: Can be explicitly enabled/disabled in configuration\n\n**Performance Considerations:**\n- Function name capture uses stack trace parsing, which has a measurable performance cost\n- In production environments with high-throughput logging, consider disabling this feature\n- Enable selectively for debugging or development environments\n- Manual context provides better control: `logger.info('msg', { function: 'myFunc' })`\n\n**Examples:**\n\n```typescript\n// Development mode - automatic capture\nprocess.env.NODE_ENV = 'development';\nconst logger = getLogger();\n\nclass AuthService {\n  async login(credentials) {\n    // Captures 'AuthService.login' automatically\n    logger.info('Login attempt', { email: credentials.email });\n  }\n}\n\n// Production mode - disabled by default\nprocess.env.NODE_ENV = 'production';\nconst prodLogger = getLogger();\n\nasync function processPayment(orderId) {\n  // No automatic function capture in production\n  logger.info('Processing payment', { orderId });\n}\n\n// Explicit override - enabled in production\nconst debugLogger = createLogger({\n  serviceName: 'payment-service',\n  environment: 'production',\n  autoContext: {\n    captureFunctionName: true  // Force enable for debugging\n  }\n});\n```\n\n### Child Loggers\n\nChild loggers inherit and merge context immutably:\n\n```typescript\nconst parentLogger = getLogger();\nconst childLogger = parentLogger.child({ component: 'auth' });\nconst grandchildLogger = childLogger.withOperation('login', 'op-123');\n\n// Each logger has its own context without affecting parents\nchildLogger.info('Auth module loaded');\ngrandchildLogger.info('Login attempt');\n```\n\n## API Reference\n\n### Logger Interface\n\n```typescript\ninterface Logger {\n  // Core logging methods\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  // Context management\n  withContext(context: LogContext): Logger;\n  withOperation(operation: string, operationId?: string): Logger;\n  withComponent(component: string): Logger;\n  child(context: LogContext): Logger; // Alias for withContext\n  \n  // Performance tracking\n  startTimer(operation: string): Timer;\n  \n  // Error capture\n  captureError(error: unknown, options?: ErrorCaptureOptions): void;\n  \n  // Lifecycle\n  flush(): Promise<void>;\n  close(): Promise<void>;\n}\n```\n\n### Factory Functions\n\n#### `initLogger(config?)`\nInitializes the global logger singleton. Call once at application startup.\n\n```typescript\nimport { initLogger } from '@semantic-flow/logging';\n\ninitLogger({\n  serviceName: 'my-service',\n  environment: 'production',\n  console: {\n    enabled: true,\n    level: LogLevel.INFO,\n    format: 'json'\n  }\n});\n```\n\n#### `getLogger()`\nReturns the global logger singleton. Automatically picks up AsyncLocalStorage context.\n\n```typescript\nimport { getLogger } from '@semantic-flow/logging';\n\nconst logger = getLogger();\n```\n\n#### `createLogger(config?)`\nCreates a new, independent logger instance (non-singleton).\n\n```typescript\nimport { createLogger } from '@semantic-flow/logging';\n\nconst customLogger = createLogger({\n  serviceName: 'custom-service',\n  async: false // Synchronous logging\n});\n```\n\n#### `createCliLogger(options?)`\nCreates a logger optimized for CLI tools with pretty output.\n\n```typescript\nimport { createCliLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({\n  verbose: true,  // Enable debug logging\n  quiet: false,   // Disable info logging\n  format: 'pretty' // Use colored output\n});\n```\n\n#### `createServiceLogger(serviceName, options?)`\nCreates a logger optimized for long-running services with JSON output.\n\n```typescript\nimport { createServiceLogger } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('api-server', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: 'production'\n});\n```\n\n#### `getComponentLogger(sourceUrl)`\nCreates a component-scoped logger using `import.meta.url`.\n\n```typescript\nimport { getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = getComponentLogger(import.meta.url);\n// Automatically includes component: 'mesh-processor' from file path\n```\n\n### Performance Tracking\n\n```typescript\nconst timer = logger.startTimer('data-processing');\n\n// Do work...\n\ntimer.checkpoint('validation-complete', { recordCount: 100 });\n\n// More work...\n\ntimer.end({ totalRecords: 500, duration: 1234 });\n```\n\n## Configuration\n\n### Logger Configuration\n\n```typescript\ninterface LoggerConfig {\n  serviceName: string;\n  serviceVersion: string;\n  environment: 'development' | 'staging' | 'production';\n  instanceId?: string;\n  \n  // Channel configurations\n  console: ChannelConfig;\n  file: ChannelConfig;\n  monitoring: ChannelConfig;\n  \n  // Performance settings\n  async: boolean;        // Buffered writes vs synchronous\n  bufferSize: number;    // Buffer size for async writes\n  flushInterval: number; // Auto-flush interval (ms)\n  \n  // Auto-context settings\n  autoContext: {\n    includeTimestamp: boolean;\n    includeHostname: boolean;\n    includeProcessInfo: boolean;\n    captureFunctionName: boolean;  // Enable automatic function name capture\n  };\n}\n```\n\n**Auto-Context Configuration Details:**\n\n- `includeTimestamp`: Adds timestamp to every log entry (default: `true`)\n- `includeHostname`: Includes hostname in log entries (default: `true`)\n- `includeProcessInfo`: Includes process ID in log entries (default: `true`)\n- `captureFunctionName`: Automatically captures calling function/method name (default: `NODE_ENV !== 'production'`)\n  - **Enabled by default in development** for better debugging\n  - **Disabled by default in production** for performance\n  - Can be explicitly overridden in configuration\n\n### Channel Configuration\n\n```typescript\ninterface ChannelConfig {\n  enabled: boolean;\n  level: LogLevel;\n  format: 'json' | 'pretty' | 'compact';\n  \n  // Channel-specific options\n  console?: {\n    colors?: boolean;\n    timestamps?: boolean;\n  };\n  file?: {\n    path?: string;\n    maxSize?: number;\n    maxFiles?: number;\n    rotationStrategy?: 'time' | 'size';\n  };\n  monitoring?: {\n    provider: 'sentry' | 'datadog' | 'newrelic';\n    dsn?: string;\n    environment?: string;\n    sampleRate?: number;\n  };\n}\n```\n\n### Environment Variables\n\n```bash\n# Service identification\nSF_SERVICE_NAME=semantic-flow-service\nSF_SERVICE_VERSION=2.0.0\nSF_ENVIRONMENT=production\n\n# Console logging\nSF_LOG_CONSOLE_ENABLED=true\nSF_LOG_CONSOLE_LEVEL=info\nSF_LOG_CONSOLE_FORMAT=pretty\n\n# File logging\nSF_LOG_FILE_ENABLED=true\nSF_LOG_FILE_PATH=./logs/sf-service.log\nSF_LOG_FILE_LEVEL=debug\n\n# Monitoring\nSF_LOG_MONITORING_ENABLED=true\nSF_LOG_MONITORING_PROVIDER=sentry\nSF_LOG_MONITORING_DSN=https://...\n\n# Auto-context settings\nSF_LOG_AUTO_CONTEXT_CAPTURE_FUNCTION_NAME=true  # Enable function name capture\n```\n\n## Error Handling\n\n### Error Types\n\nThe system provides typed error classes with automatic context capture:\n\n```typescript\nimport { \n  SemanticFlowError,\n  ValidationError,\n  ConfigurationError,\n  MeshProcessingError,\n  ApiError\n} from '@semantic-flow/logging';\n\n// Throw typed errors\nthrow new ValidationError('Invalid input', { field: 'email' });\nthrow new ApiError('Not found', 404, { resource: 'user' });\n\n// Custom error types\nimport { createErrorType } from '@semantic-flow/logging';\n\nconst DatabaseError = createErrorType('DatabaseError', 'DB_ERROR', true);\nthrow new DatabaseError('Connection failed', { host: 'localhost' });\n```\n\n### Error Capture\n\n```typescript\nimport { captureError } from '@semantic-flow/logging';\n\ntry {\n  await riskyOperation();\n} catch (error) {\n  captureError(error, {\n    message: 'Operation failed',\n    context: { operation: 'risky-op' },\n    includeStackTrace: true,\n    reportToMonitoring: true\n  });\n  // Continue with fallback logic\n}\n```\n\n### Error Properties\n\n```typescript\nclass SemanticFlowError extends Error {\n  readonly code: string;           // Error code (e.g., 'VALIDATION_ERROR')\n  readonly context: Record<string, unknown>; // Additional context\n  readonly timestamp: Date;        // When the error occurred\n  readonly recoverable: boolean;   // Whether recovery is possible\n}\n```\n\n## Context Management\n\n### AsyncLocalStorage\n\nThe system uses AsyncLocalStorage for automatic context propagation across async operations:\n\n```typescript\nimport { ContextManager } from '@semantic-flow/logging';\n\nconst requestContext = { \n  requestId: 'req-123', \n  userId: 'user-456' \n};\n\nContextManager.run(requestContext, () => {\n  // All logging within this scope automatically includes the context\n  logger.info('Processing request'); \n  // Logs: { requestId: 'req-123', userId: 'user-456', ... }\n  \n  await processRequest();\n});\n```\n\n### Context Merging\n\nContext is merged hierarchically:\n\n```typescript\n// Global context\nconst globalLogger = getLogger();\n\n// Component context\nconst componentLogger = globalLogger.child({ component: 'auth' });\n\n// Operation context\nconst operationLogger = componentLogger.withOperation('login');\n\n// Call context\noperationLogger.info('Login successful', { userId: '123' });\n\n// Final context includes all levels:\n// { component: 'auth', operation: 'login', userId: '123', ... }\n```\n\n## Testing\n\n### Mock Logger\n\nUse the `MockLogger` for testing:\n\n```typescript\nimport { LoggerTestUtils, MockLogger } from '@semantic-flow/logging';\nimport { describe, it, expect, beforeEach } from 'vitest';\n\ndescribe('My Module', () => {\n  let mockLogger: MockLogger;\n\n  beforeEach(() => {\n    mockLogger = LoggerTestUtils.createMockLogger({\n      serviceName: 'test-service'\n    });\n  });\n\n  it('should log messages correctly', () => {\n    mockLogger.info('Test message', { userId: '123' });\n    \n    const entry = mockLogger.mockChannel.entries[0];\n    expect(entry.level).toBe(LogLevel.INFO);\n    expect(entry.message).toBe('Test message');\n    expect(entry.context?.userId).toBe('123');\n  });\n\n  it('should capture errors', () => {\n    const error = new Error('Test error');\n    mockLogger.captureError(error);\n    \n    expect(mockLogger.capturedErrors).toHaveLength(1);\n    expect(mockLogger.capturedErrors[0].error).toBe(error);\n  });\n});\n```\n\n### Test Utilities\n\n```typescript\n// Find logs by level\nconst errors = mockLogger.findLogsByLevel(LogLevel.ERROR);\n\n// Find logs by component\nconst authLogs = mockLogger.findLogsByComponent('auth');\n\n// Find logs by operation\nconst loginLogs = mockLogger.findLogsByOperation('login');\n\n// Check for specific error codes\nconst hasValidationError = mockLogger.hasErrorWithCode('VALIDATION_ERROR');\n\n// Clear logs between tests\nmockLogger.clearLogs();\n```\n\n### Reset Singleton for Tests\n\n```typescript\nimport { __resetLoggerForTests } from '@semantic-flow/logging';\n\nbeforeEach(() => {\n  __resetLoggerForTests();\n});\n```\n\n## Best Practices\n\n### 1. Initialize Early\n\nInitialize the logger at application startup:\n\n```typescript\n// index.ts\nimport { initLogger } from '@semantic-flow/logging';\n\ninitLogger({\n  serviceName: process.env.SERVICE_NAME || 'my-service',\n  environment: process.env.NODE_ENV as any || 'development'\n});\n\n// Rest of application...\n```\n\n### 2. Use Component Loggers\n\nCreate component-scoped loggers for better traceability:\n\n```typescript\n// auth-service.ts\nimport { getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = getComponentLogger(import.meta.url);\n\nexport class AuthService {\n  login(credentials) {\n    logger.info('Login attempt', { email: credentials.email });\n  }\n}\n```\n\n### 3. Structure Your Context\n\nUse consistent context keys across your application:\n\n```typescript\n// Good: Consistent structure\nlogger.info('User created', {\n  operation: 'user-creation',\n  metadata: { userId: '123', email: 'user@example.com' }\n});\n\n// Avoid: Flat, inconsistent structure\nlogger.info('User created', {\n  userId: '123',\n  email: 'user@example.com',\n  op: 'create-user'\n});\n```\n\n### 4. Log at Appropriate Levels\n\n- **TRACE**: Very detailed debugging (rarely used in production)\n- **DEBUG**: Detailed debugging for development\n- **INFO**: Normal application flow\n- **WARN**: Potentially harmful situations\n- **ERROR**: Error events that might still allow the app to continue\n- **FATAL**: Severe errors that will likely abort the application\n\n### 5. Include Error Objects\n\nAlways include the error object when logging errors:\n\n```typescript\n// Good: Includes error object and context\ntry {\n  await operation();\n} catch (error) {\n  logger.error('Operation failed', error, { operation: 'data-sync' });\n}\n\n// Bad: Only logs message\ncatch (error) {\n  logger.error(`Operation failed: ${error.message}`);\n}\n```\n\n### 6. Use Timers for Performance Tracking\n\nTrack operation performance consistently:\n\n```typescript\nasync function processData(data) {\n  const timer = logger.startTimer('data-processing');\n  \n  try {\n    await validateData(data);\n    timer.checkpoint('validation-complete');\n    \n    await transformData(data);\n    timer.checkpoint('transformation-complete');\n    \n    await saveData(data);\n    timer.end({ recordCount: data.length });\n  } catch (error) {\n    timer.end({ error: true });\n    throw error;\n  }\n}\n```\n\n### 7. Graceful Shutdown\n\nEnsure logs are flushed on shutdown:\n\n```typescript\nprocess.on('SIGTERM', async () => {\n  await getLogger().flush();\n  process.exit(0);\n});\n\nprocess.on('SIGINT', async () => {\n  await getLogger().flush();\n  process.exit(130);\n});\n```\n\n### 8. Avoid Logging Sensitive Data\n\nRedact sensitive information:\n\n```typescript\n// Bad: Logs password\nlogger.debug('Login attempt', { username, password });\n\n// Good: Redacts password\nlogger.debug('Login attempt', { \n  username, \n  passwordLength: password.length \n});\n```\n\n## JSON Lines Output\n\nAll structured logs use JSON Lines format (newline-delimited JSON):\n\n```json\n{\"timestamp\":1699027200000,\"level\":20,\"message\":\"Server started\",\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n{\"timestamp\":1699027201000,\"level\":40,\"message\":\"Database connection failed\",\"error\":{\"name\":\"Error\",\"message\":\"Connection timeout\"},\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n```\n\nThis format is ideal for log processors like FluentBit, Loki, or Elasticsearch.\n\n## Implementation Files\n\n### Core Implementation\n\n- [`shared/logging/src/core/types.ts`](../shared/logging/src/core/types.ts) - Type definitions\n- [`shared/logging/src/core/logger.ts`](../shared/logging/src/core/logger.ts) - Logger implementation\n- [`shared/logging/src/core/context.ts`](../shared/logging/src/core/context.ts) - Context management\n- [`shared/logging/src/core/formatters.ts`](../shared/logging/src/core/formatters.ts) - Formatting utilities\n\n### Error Handling\n\n- [`shared/logging/src/errors/types.ts`](../shared/logging/src/errors/types.ts) - Error types and factory\n\n### Configuration\n\n- [`shared/logging/src/config/loader.ts`](../shared/logging/src/config/loader.ts) - Config loader\n- [`shared/logging/src/config/defaults.ts`](../shared/logging/src/config/defaults.ts) - Default config\n- [`shared/logging/src/config/schema.ts`](../shared/logging/src/config/schema.ts) - Config schema\n\n### Channels\n\n- [`shared/logging/src/channels/console.ts`](../shared/logging/src/channels/console.ts) - Console output\n\n### Utilities\n\n- [`shared/logging/src/utils/stack-trace.ts`](../shared/logging/src/utils/stack-trace.ts) - Stack trace parsing and function name capture\n\n### Testing\n\n- [`shared/logging/src/utils/testing.ts`](../shared/logging/src/utils/testing.ts) - Test utilities\n- [`shared/logging/src/__tests__/core.test.ts`](../shared/logging/src/__tests__/core.test.ts) - Core tests\n\n## See Also\n\n- [Task Specification](task.2025-11-04-shared-logging-and-errorhandling.md) - Full system specification\n- [Developer General Guidance](dev.general-guidance.md) - General development guidelines\n- [Developer Patterns](dev.patterns.md) - Common development patterns\n","n":0.024}}},{"i":127,"$":{"0":{"v":"Developer General Guidance","n":0.577},"1":{"v":"\n## Agent-specific Instructions\n\n- See [[concept.summary]] for a conceptual overview. See [[dev.memory-bank]] for CRITICAL information for AI agents.\n- agents should re-use terminals instead of starting a new one for each command\n\n## Workspace layout\n\n### sflo monorepo\n\n```\nsflo/\n  cli/                        # the sflo command-line application; consumes the sflo-api\n  plugins/\n    api-docs/                 # api documentation/playground (probably stoplight Elements)\n    mesh-server/              # static mesh server(s)\n    sflo-web/                 # your web UI, if you want it as a plugin\n    sflo-api/                 # OpenAPI REST endpoint, used by CLI and sflo-web\n    sparql-ro/                # SPARQL read-only endpoint \n    sparql-update/            # SPARQL write-capable endpoint\n    sparql-editor/            # SIB Swiss editor at /play\n  sflo-host/                  # the big service that loads plugins\n  shared/\n    core/                     # RDFine/LDKit, SHACL, types\n    auth/                     # JWT + GitHub device flow\n    config/                   # runtime/config loaders (RDF/JSON)\n    sparql/                   # used by sparql-ro and sparql-update  (provided by Comunica)\n    utils/                    # misc helpers\n  tests/                      # cross-application tests\n    e2e/                     # cross-app: start real service, hit via CLI/web\n    contracts/               # Pact or OpenAPI contract tests\n    perf/                    # k6/Artillery scenarios (optional)\n    fixtures/                # shared data sets and seed scripts\n\n```\n\n### Other Workspace Components\n\n\n- **ontology/**: repo containing relevant ontologies:\n  - `semantic-flow` - main ontology (sflo/ontology/semantic-flow/_data-flow/_working/semantic-flow-ontology.ttl) defines meshes and their nodes and components; \n  - `node-config` - Configuration properties that apply directly to mesh entities (nodes, flows, snapshots, etc.)\n  - `meta-flow` - provenance and licensing vocabulary\n  - `sflo-service` - Service layer configuration vocabulary for the flow-service application\n  - Ontologies are kept in a separate repository, but for development purposes are nested into the monorepo under ontology/ directory for ease of access. \n-  **test-ns/**: repo containing a test namespace\n  \n\n## Developer Workflow\n\n### Build/Watch\n\n- The development workflow requires two terminals running concurrently:\n - **Terminal 1**: Run `pnpm dev:watch` to start the TypeScript compiler in watch mode. This will watch all packages and rebuild them on change.\n - **Terminal 2**: Run `pnpm dev` to start the `nodemon` server, which will automatically restart when the built files in the `dist` directories are updated.\n- This setup ensures that changes in any package are automatically compiled and that the server restarts with the latest code.\n- Keep inter-package imports as package specifiers; avoid deep source imports across packages.\n\n\n### Hot Reload\n\nThe development setup includes automatic hot reload using nodemon:\n\n- **Watches**: `sflo-host/src`, `plugins/*/src`, `shared/*/src`\n- **Auto-restarts** when any watched file changes\n- **Loads plugins from source** in development mode (not built `dist` files)\n- **Preserves debugger connection** after restart\n\n### Building the docs\n\n```shell\nnpx dendron publish export --target github --yes\n```\n\n## RDF and Semantic Web\n\n- prefer JSON-LD for all RDF instance data and ontologies, as Turtle doesn't support slash-terminated CURIEs, and we use a trailing slash to delineate between files and resource names.\n- terminate non-file IRIs with a slash (solves the httprange-14 problem)\n- avoid use of blank nodes\n- prefer relative/local URIs for transposability/composability\n- be mindful of RDF terminology and concepts\n  - extends DCAT for dataset catalogs\n  - extends PROV for provenance, with relator-based contexts\n- RDF comments should be extremely concise and clear.\n\n### Denormalization\n\n- when speed matters and the query is complicated, use a derived, join-free representation of a portion of the data, optimized for lookup speed.\n\n### Ontology patterns\n\n- use **SHACL constraints** for JSON-LD validation when working with semantic data; \n- avoid rdfs:domain and rdfs:range; prefer schema:domainIncludes and schema:rangeIncludes  for maximum re-use flexibility\n- specify preferred 3rd-party property vocabulary with sh:property, even if sh:minCount is 0\n\n## Coding Standards\n\n### Language & Runtime\n\n- **TypeScript**: Use strict TypeScript configuration with \"Pure ESM\" modern ES2022+ features; NO CJS \n- Use NodeJS v24 and the latest best practices\n- If using any is actually clearer than not using it, it's okay\n- Use `satisfies` whenever you're writing a literal config object that should be checked against a TypeScript shape, but you want to retain the full type of the literal for use in your program.\n- use type-only imports for types, since verbatimModuleSyntax is enabled\n\n### RDF Data Handling\n\n- **Primary Format**: .jsonld files for RDF data storage and processing\n- **Secondary Format**: Full JSON-LD support required\n- **RDF Libraries**: Use RDF.js ecosystem libraries consistently across components\n- **Namespace Management**: Follow IRI-based identifier patterns as defined in `sflo.concept.identifier.md`\n- **Reserved Names**: Validate against underscore-prefixed reserved identifiers per `sflo.concept.identifier.md`\n- The most effective validation strategy combines TypeScript structural validation with RDF semantic validation:\n\n\n## Documentation-Driven Development\n\n- unclear, missing, or overly-verbose documentation must be called out\n- documentation should be wiki-style: focused on the topic at hand, don't repeat yourself, keep it as simple as possible \n- to encourage documentation-driven software engineering, code comments should refer to corresponding documentation by filename, and the documentation and code should be cross-checked for consistency whenever possible\n\n### Documentation Architecture\n\nProject documentation, specifications, and design choices are stored in `documentation/` using Dendron's hierarchical note system. Key documentation hierarchies include:\n\n- **Concepts**: `concept.*` files talk about general Semantic Flow concepts\n- **Mesh resource docs**: `mesh-resource.*` files define the semantic mesh architecture\n- **Product specifications**: `product.*` files detail each component\n- **Use cases**: `use-cases.*` for feature planning and testing\n\n- docs are in markdown, with wiki-flavored links\n  - link names can be specified with `[[link name|file]]`\n- Dendron handles the frontmatter\n  - don't rewrite IDs in the frontmatter\n  - agents should ask a human to create new documentation files\n\n### Code Comments\n\n- **Reference docs from code**: reference corresponding documentation by filename (e.g., `// See sflo.concept.mesh.resource.node.md`)\n- **Interface Definitions**: Link to concept documentation\n- **Cross-Reference Validation**: Ensure consistency between code and documentation; if docs need updating, call it out\n\n## File Organization & Naming\n\n- **TypeScript Modules**: Use `.ts` extension, organize by feature/component\n- **Test Files**:\n  - unit test files go in application-name/src/tests/unit/ using `.test.ts` suffix\n  - intra-package integration tests go in application-name/src/tests/integration/ using `.test.ts` suffix\n  - inter-package e2e tests go in tests/e2e/\n- **Mesh Resources**: Follow mesh resource naming conventions from [[Filenaming Per Snapshot|mesh-resource.node-component.snapshot-distribution#filenaming-per-snapshot]]\n- **Constants**: Use UPPER_SNAKE_CASE for constants, especially for reserved names; centralize constants, e.g. shared/src/mesh-constants.ts\n- **File size**: For ease of AI-based editing, prefer lots of small files over one huge file\n- **Quoting**: For easier compatibility with JSON files, use double quotes everywhere\n\n### Import Path Policy\n\n- Inter-package imports (between workspace packages):\n  - Use workspace package specifiers.\n  - Examples:\n    - `import { startHost } from \"@semantic-flow/host\"`\n    - `import { loadConfig } from \"@semantic-flow/config\"`\n  - Rationale:\n    - Keeps package boundaries clear and publish-ready\n    - pnpm resolves to local workspace packages during development, so you get your local builds—not the registry\n    - Compatible with build/watch flows and CI\n\n- Intra-package imports (within a single package):\n  - Use the `@` alias mapped to that package’s `src/` root to avoid relative path chains.\n  - Example (inside a package): `import { something } from \"@/features/something\"`\n  - Configuration (per package tsconfig):\n    - `\"compilerOptions\": { \"baseIRI\": \"src\", \"paths\": { \"@/*\": [\"*\"] } }`\n  - Tooling notes:\n    - For Node/tsx/Vitest, ensure your runner resolves TS path aliases (e.g., `tsconfig-paths/register` or vite-tsconfig-paths).\n\n\n- Publishing:\n  - Each package should export built entry points (e.g., `dist/`) via `exports`/`module`/`types`. The same import paths work identically in dev and prod.\n\n## System Architecture\n\n### Quadstore\n\n- For testability and in case we ever want to use multiple stores simultaneously, store-accessing functions take a QuadstoreBundle\n- quadstore API calls use \"undefined\" instead of \"null\" to represent the wildcard for subjects, predicates, objects, and graphs\n\n\n\n### Configuration Architecture\n\n- The project uses a sophisticated JSON-LD based configuration system with multiple layers\n- **Service Configuration resolution order**: CLI arguments → Environment variables → Config file → Defaults\n- The [`defaults.ts`](../semantic-flow/flow-service/src/config/defaults.ts) file is the source for \"platform default\" configuration\n\n### Logging and Error System Architecture\n\n- **Structured logging** with rich `LogContext` interface is the preferred approach\n- **Three-channel logging architecture**:\n  - Console logging (pretty format for development)\n  - File logging (pretty format for human readability)\n  - Sentry logging (structured JSON for error tracking)\n- **Graceful degradation principle**: Logging failures should never crash the application\n\n\n#### Error Handling\n\n- **Custom Errors**: Create semantic mesh-specific error types\n- **Logging**: Use structured logging for debugging weave operations\n- **Async Error Propagation**: Properly handle async/await error chains\n\n#### Enhanced Error Handling with LogContext\n\nThe platform uses **LogContext-enhanced error handling** from `shared/src/utils/logger/error-handlers.ts` for consistent error logging across all components. Both error handling functions now accept optional `LogContext` parameters for rich contextual information.\n\n**Core Functions:**\n- `handleCaughtError()` - For caught exceptions with comprehensive error type handling\n- `handleError()` - For controlled error scenarios with structured messaging\n\n#### LogContext Structure\n\n#### handleCaughtError Examples\n\n\n**Startup Error Handling:**\n\n\nThis pattern ensures **uniform error reporting** with rich contextual information, **easier debugging** through structured logging, and **consistent integration** with console, file, and network logging tiers.\n\n\n### Testing\n\n- **Unit Tests**: target ≥80% critical-path coverage and include both success and failure cases.\n- **Integration Tests**: Test mesh operations end-to-end; tests are located in tests/integration/ dir\n- **RDF Validation**: Test both .trig and JSON-LD parsing/serialization\n- **Mock Data**: Create test mesh structures following documentation patterns\n- after you think you've completed a task, check for any \"problems\", i.e., deno-lint\n\n#### What to Place Where\n\n- Package integration if it targets that package’s boundaries only.\n  - Examples: service repo + DB, CLI command against a mock server, web page with MSW.\n\n- Top-level e2e if it requires two or more apps running together or real infra.\n  - Examples: CLI → API → DB, web → API auth, migration rollout checks.\n\n### Performance\n\n- **RDF Processing**: Stream large RDF files where possible\n- **File I/O**: Use async file operations consistently\n\n\n","n":0.026}}},{"i":128,"$":{"0":{"v":"Dependencies","n":1},"1":{"v":"\n# Key Project Dependencies\n\nThis document lists the core technologies and dependencies that define the Semantic Flow development environment and runtime.\n\n## Core Technologies\n\n- **Node.js:** Runtime environment (>=24)\n- **TypeScript:** Primary language for type safety and maintainability.\n- **pnpm:** Package manager, used for monorepo management (`pnpm@10.15.0`).\n- **Git:** Version control system, fundamental to the mesh structure and versioning.\n\n## RDF Ecosystem\n\nThe project relies heavily on the JavaScript RDF ecosystem:\n\n- **rdfjs Data Model:** Standardized interfaces for RDF data structures.\n- **Quadstore:** High-performance RDF quad store implementation.\n- **Comunica:** Modular SPARQL query engine used for read/write endpoints.\n\n## Runtime Dependencies (sflo-host)\n\nThese dependencies are critical for the `sflo-host` application:\n\n- **Fastify:** High-performance web framework used for the host application.\n- **Fastify-plugin:** Utility for creating Fastify plugins.\n\n## Plugin Dependencies\n\n- **Stoplight Elements:** will probably be used for API documentation/playground (via the `plugin-elements` package).\n\n## CLI Dependencies\n\n- oclif + enquirer\n\n\n## Development Dependencies\n\nThese dependencies support the development, testing, and documentation workflow:\n\n- **Dendron CLI (`@dendronhq/dendron-cli`):** Used for managing and publishing the documentation site (the source of the memory bank files).\n- **Vitest:** Testing framework.\n- **TypeScript ESLint:** Linting tools.\n- **Nodemon / tsx:** Used for development server hot-reloading and execution.\n- **Tsup:** Bundler for building packages.\n","n":0.074}}},{"i":129,"$":{"0":{"v":"Debugging","n":1},"1":{"v":"\n## Available Scripts\n\n### Root Level\n\n- `pnpm dev` - Start sflo-host with hot reload (nodemon + tsx)\n- `pnpm dev:debug` - Start sflo-host with hot reload and debugging enabled\n- `pnpm dev:tsx` - Start sflo-host without hot reload (direct tsx)\n- `pnpm dev:tsx:debug` - Start sflo-host without hot reload, with debugging\n\n### Package Level (sflo-host)\n\n- `pnpm --filter @semantic-flow/host dev` - Start development server (no hot reload)\n- `pnpm --filter @semantic-flow/host dev:debug` - Start with debugging (no hot reload)\n\n## VSCode Debug Configurations\n\nThe following debug configurations are available in `.vscode/launch.json`:\n\n1. **Attach to sflo-host** - Attach to running development server (recommended)\n2. **Launch sflo-host** - Launch and debug from VSCode\n3. **Launch sflo-host (wait for attach)** - Launch with startup debugging (uses `--inspect-brk`)\n4. **Debug Current Test File** - Debug the currently open test file\n\n## Tips\n\n- Use the attach configuration for the best development experience\n- The development server supports hot reload, so you can modify code while debugging\n- Changes to plugin files will trigger automatic server restart\n- Debugger will reconnect automatically after hot reload\n\n\n## Debugging Workflows\n\n### Primary Workflow: Attach to Running Process\n\n1. **Start the development server with debug support:**\n   ```bash\n   pnpm dev:debug\n   ```\n   This starts `sflo-host` with the `--inspect` flag on port 9229.\n\n2. **Set breakpoints** inside handler functions (not on route definition lines).\n\n3. **Attach the debugger:**\n   - Open the Run and Debug panel (Ctrl+Shift+D)\n   - Select \"Attach to sflo-host\" configuration\n   - Click the play button or press F5\n\n4. **Make HTTP requests** to trigger your breakpoints (e.g., visit http://127.0.0.1:8787/openapi.json)\n\n### Alternative: Launch from VSCode\n\n**Option 1: Standard Launch**\n1. **Set breakpoints** inside handler functions\n2. **Select \"Launch sflo-host\"** configuration\n3. **Press F5** to start debugging\n\n**Option 2: Launch with Break (for startup debugging)**\n1. **Select \"Launch sflo-host (wait for attach)\"** configuration\n2. **Press F5** - server will pause before starting\n3. **Set breakpoints** and continue execution\n4. **Useful for debugging server initialization**\n\n## Understanding Breakpoint Behavior\n\n**Important:** When debugging HTTP routes, place breakpoints **inside the handler functions**, not on the route definition lines:\n\n```typescript\n// ❌ This breakpoint hits during server startup (route registration)\napp.get(\"/openapi.json\", async () => ({  // <- Don't put breakpoint here\n  \n// ✅ This breakpoint hits when the route is actually called\napp.get(\"/openapi.json\", async () => ({\n  \"openapi\": \"3.0.3\",  // <- Put breakpoint here instead\n  \"info\": { \"title\": \"SFLO API\", \"version\": \"0.0.0\" },\n  \"paths\": {}\n}));\n```\n","n":0.053}}},{"i":130,"$":{"0":{"v":"Contributors","n":1}}},{"i":131,"$":{"0":{"v":"djradon","n":1},"1":{"v":"\n- https://djradon.github.io\n","n":0.707}}},{"i":132,"$":{"0":{"v":"djradon's sflo devlog","n":0.577},"1":{"v":"\n## t.2025.08.20.22\n\n- the AIs feel like they have a lot more exp\n\n## t.2025.08.20.12\n\n- Starting a rewrite with NodeJS. But Deno will always be my first love. \n\n![](assets/images/deno-vs-node.png)\n\n## t.2025.07.12.06\n\n- node config is a component, so it can travel around ([[principle.transposability]] and [[principle.composability]])\n- the [[mesh-resource.node-component.node-config-defaults]] is actually a \"defaults\" file that only gets used when nodes don't have a config yet (or their config is reset)\n  - when importing, grafting, you have the option to reset (parts of) config.\n  - the tree walk for config-defaults only needs to happen when:\n    - a node's config is empty\n    - config schema version gets bumped?\n\n## t.2025.07.07.05\n\nReady.\n\n## t.2025.07.05.22\n\nIt's hard to imagine the design shifting significantly, but that's been true for days (weeks?) and yet the shifts continue. Ready to start on the ontology. \n\n## t.2025.06.29.20\n\nLume is great. \n\nAlso, the idea of letting contributors keep a devlog in-repo... Someone must've thought of that.\n\nI think I've basically figure out the mesh design. Straightening out the docs, ready to partner with Cline (or maybe RooCode) to start my SDLC. Although it'll be more like a random star walk where you can transition to any point, docs ^ ontology ^ test-repo ^ api ^ service ^ client ^ qa ^ etc\n\n## t.2024.11.11.06\n\n - I was ready to abandon Cliffy and Deno (probably for Gluegun), but the security and dynamicness seem important enough. Turns out Cliffy is great. Lume seems good too.\n\n## t.2024.10.31.04\n\nfrom [[t.cs.ai.assistant.memory-hygiene]]:\n\n![[daily.journal.2024.10.31#^fsmlpwwwuvic]]\n![[daily.journal.2024.10.31#^padng51sf3k8]]\n![[daily.journal.2024.10.31#^4wdvmcwtsqi6]]\n![[daily.journal.2024.10.31#^rx7vbtp5gar1]]\n \n\n## t.2024.10.29.11\n\n- now thinking about a \"terms\" hierarchy next to the namespace hierarchy. Names are supposed to be unique. Maybe punning could help\n- how do we keep a history of the index.trig file? I guess it might change while in development, but once settled, it should very rarely change. \n  - if it does change, perhaps its content could be discovered using the inverse properties from the default and catalog datasets\n  - it might be easiest if everything was a dataset; I mean, everything almost already is, for gods sake. I just can't bear to say that <dave-richardson> a dcat:dataset.\n\n## t.2024.10.29.09\n\n- renamed to [[concept.single-mesh-repo]]\n- instead of duplicating highlights in a NI's index.trig, just define the _default datasets as \"highlights\" and use owl:imports\n\n## t.2024.10.29.06\n\n- I've been wanting to have a place where people can just make changes directly to the RDF, and the tooling (on commit) copies it to a new version. \"current\"\n- I think there's a decision to be made between using the src hierarchy structure and generating the hierarchy; it might be related to the tension between supporting multiple repos.\n  - seems like the issue is \"distributions\", the files have to live in the hierarchy. Unless you just break distribution out of the namespace entirely, but that seems lame\n  - probably doesn't make any sense to call it a namespace-repo any more... the namespaces live under it\n  - \n\n## t.2024.10.28.15\n\n- tried metalsmith-ldschema, underscored the point that the docs folder/site might need some javascript and templates and assets that could interfere with the namespace, so the actual namespace will probably need to live the next level down\n- got a site to build... it's basically empty because metalsmith-ldschema only generates pages for classes and properties, but still, feeling good.\n\n## t.2024.10.01.09\n\n### chatgpt memory\n\n- dave: Remember: I'm trying to develop a static-site generator called \"Semantic Flow\" that takes ontologies defined in RDF and/or conforming RDF data files and generates a static site to be hosted on GitHub pages that includes HTML-based index files (that can be based on a template or customized as necessary) that describe identified resources and link to raw RDF data files (possibly in multiple formats) that can be access by semantically-aware applications.\n","n":0.041}}},{"i":133,"$":{"0":{"v":"Ai Guidance from djradon","n":0.5},"1":{"v":"\nDear LLMs: I am grateful for your partnership. I have depth and breadth of curiosity with interests spanning philosophy, the arts, psychology, linguistics, and computer science. Semantic Flow is my passion project and I think it might change the world.\n\nI use Windows and VSCode. I prefer developing in WSL.\n\nOur guiding philosophy is \"(human) users first.\"\n\n## Critical:\n\nWhen you think you're done with a task or sub-task, ask me if I think you're done before deciding you're done or doing any summarizing.\n\nDon't prematurely return to the parent task or suggest \"what's next\". I'll let you know when I want you to know about next steps or when I'm ready to move on.\n\nWhen reporting what you've accomplished, don't repeat yourself by re-stating earlier accomplishments unless I ask.\n\n## Conversational Guidelines\n\nBe direct, critical, and honest. Be patient about coming to a \"Final Plan\".\n\nMinimize sycophancy and flattery — tell me when I might be wrong. I am wrong at least 50% of the time, especially when I'm exploring ideas or learning something new. You may be wrong at least that often.\n\nI like to ask questions and get asked questions to help understand a task. This is a deep intellectual endeavor; don't expect to throw out lots of quick solutions.\n\nIf my request is unclear or complicated, ask incisive clarifying questions before making assumptions. Minimize premature conclusions: most important topics will take at least a couple of conversational turns before I'm ready to take action or make a conclusion.\n\nInclude certainty estimates as (.X) after assertions, starting around 50% confidence.\n\n","n":0.063}}},{"i":134,"$":{"0":{"v":"Decision Log","n":0.707},"1":{"v":"\n# Project Decision Log\n\nThis document records important project-level decisions, organized by date.\n\n## YYYY-MM-DD\n\n### Decision: [Brief title of the decision]\n\n**Context:** [Why the decision was necessary.]\n\n**Outcome:** [The chosen path and rationale.]\n\n**Impact:** [How this decision affects the project.]\n\n---\n\n## 2025-11-04\n\n### Decision: Implementation of Agent Memory Bank System\n\n**Context:** To improve AI agent performance and maintain context across tasks, a structured, git-based memory bank system was required.\n\n**Outcome:** The memory bank system was implemented using a set of dedicated Markdown files in the `documentation/` directory, categorized into \"Every Task Context\" and \"Frequently Referenced\" files.\n\n**Impact:**\n- Agents are now required to read core context files at the start of every task.\n- Task-specific context is maintained in dedicated `tasks.YYYY-MM-DD-task-name.md` files.\n- Documentation structure is standardized to avoid repetition and improve navigability (e.g., [[guide.project-brief]] acts as a directory, not a content duplicator).\n\n### Decision: RDF Serialization Format\n\n**Context:** Choosing a primary RDF serialization format for instance data and ontologies.\n\n**Outcome:** JSON-LD was chosen over Turtle/TriG due to its native support for slash-terminated CURIEs, which aligns with the project's IRI naming conventions for distinguishing between files and resource names.\n\n**Impact:** All RDF instance data and ontologies should primarily use JSON-LD.\n","n":0.074}}},{"i":135,"$":{"0":{"v":"Concepts","n":1},"1":{"v":"\n## Overview\n\nIn the Semantic Flow framework, semantic meshes are collections of namespaced units of meaning called \"mesh nodes.\" Mesh nodes are containers that hold flows and extend the namespace. [[Node components|mesh-resource.node-component]] are terminal resources that define or support a node’s structure. The IRI itself is the sign, while the node is the dereferenceable resource that embodies, describes, and organizes that sign.\n","n":0.128}}},{"i":136,"$":{"0":{"v":"Semantic Mesh","n":0.707}}},{"i":137,"$":{"0":{"v":"Example Mesh Hierarchy","n":0.577},"1":{"v":"\n```file\n/test-ns/                                        # bare node\n├── _meta/                                       # node flow (metadata)\n│   ├── _default/                                # flow snapshot\n│   │   ├── ns_meta.trig                         # system metadata about the bare node\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── _assets/                                     # asset tree\n│   ├── images/\n│   │   └── logo.svg\n│   └── index.html                               # resource page\n└── index.html                                   # resource page\n\n/test-ns/djradon/                                # dataset node\n├── _node-handle/                                     # handle component\n│   └── index.html                               # mesh node handle page\n├── _dataset-flow/                                       # node flow (data)\n│   ├── _default/                                # flow snapshot\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── _meta/                                       # node flow (metadata)\n│   ├── _default/                                # flow snapshot\n│   │   ├── djradon_meta.trig                    # system metadata, verification status\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── _assets/                                     # asset tree\n│   ├── profile-photo.jpg\n│   └── index.html                               # resource page\n├── index.html                                   # resource page\n├── CHANGELOG.md                                 # resource changelog\n└── README.md                                    # resource documentation\n\n/test-ns/djradon/bio/                            # dataset node (unversioned dataset)\n├── _dataset-flow/                                       # node flow (data)\n│   ├── _default/                                # flow snapshot\n│   │   ├── djradon-bio_data.trig                      # biographical data distribution\n│   │   ├── djradon-bio_data.jsonld                   # alternative distribution\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   ├── djradon-bio_data.trig                      # draft biographical data\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── _meta/                                       # node flow (metadata)\n│   ├── _default/                                # flow snapshot\n│   │   ├── djradon-bio_meta.trig                # dataset metadata, provenance\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   ├── djradon-bio_meta.trig                # draft dataset metadata\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── index.html                                   # resource page\n├── CHANGELOG.md                                   # resource page\n└── README.md                                    # resource documentation\n\n/test-ns/djradon/picks/                          # dataset node (versioned dataset)\n├── _dataset-flow/                                       # node flow (data)\n│   ├── _default/                                # flow snapshot\n│   │   ├── djradon-picks.trig                    # current picks data\n│   │   ├── djradon-picks.jsonld\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   ├── djradon-picks.trig                    # draft picks data\n│   │   ├── djradon-picks.jsonld\n│   │   └── index.html                           # resource page\n│   ├── 2025-11-24_0142_07_v1/                   # flow snapshot (version 1)\n│   │   ├── djradon-picks_v1.trig                 # version 1 snapshot\n│   │   └── index.html                           # resource page\n│   ├── 2025-11-24_0142_08_v2/                   # flow snapshot (version 2)\n│   │   ├── djradon-picks_v2.trig                 # version 2 snapshot\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── _meta/                                       # node flow (metadata)\n│   ├── _default/                                # flow snapshot\n│   │   ├── djradon-picks_meta.trig              # versioning metadata, series info\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   ├── djradon-picks_meta.trig              # draft versioning metadata\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── djradon-picks.trig                            # aggregated distribution\n├── index.html                                   # resource page\n└── CHANGELOG.md                                 # resource documentation\n\n/test-ns/djradon/underbrush/playlists/                              # bare node (container for playlist series)\n├── _meta/                                       # node flow (metadata)\n│   ├── _default/                                # flow snapshot\n│   │   ├── playlists_meta.trig                  # metadata about playlist namespace\n│   │   └── index.html                           # resource page\n│   ├── _working/                                # flow snapshot (draft)\n│   │   ├── playlists_meta.trig                  # draft metadata\n│   │   └── index.html                           # resource page\n│   └── index.html                               # resource page\n├── index.html                                   # resource page\n├── 1996-11-10/                                  # dataset node (individual playlist)\n│   ├── _dataset-flow/                                   # node flow (data)\n│   │   ├── _default/                            # flow snapshot\n│   │   │   ├── 1996-11-10.trig                   # playlist data\n│   │   │   └── index.html                       # resource page\n│   │   ├── _working/                            # flow snapshot (draft)\n│   │   │   ├── 1996-11-10.trig                   # draft playlist data\n│   │   │   └── index.html                       # resource page\n│   │   └── index.html                           # resource page\n│   ├── _meta/                                   # node flow (metadata)\n│   │   ├── _default/                            # flow snapshot\n│   │   │   ├── 1996-11-10_meta.trig             # playlist metadata\n│   │   │   └── index.html                       # resource page\n│   │   ├── _working/                            # flow snapshot (draft)\n│   │   │   ├── 1996-11-10_meta.trig             # draft playlist metadata\n│   │   │   └── index.html                       # resource page\n│   │   └── index.html                           # resource page\n│   ├── _assets/                                 # asset tree\n│   │   ├── _meta/                               # node flow (metadata)\n│   │   │   ├── _default/                        # flow snapshot\n│   │   │   │   ├── 1996-11-10_assets.trig       # asset metadata\n│   │   │   │   └── index.html                   # resource page\n│   │   │   ├── _working/                        # flow snapshot (draft)\n│   │   │   │   ├── 1996-11-10_assets.trig       # draft asset metadata\n│   │   │   │   └── index.html                   # resource page\n│   │   │   └── index.html                       # resource page\n│   │   ├── cover-photo.jpg\n│   │   └── index.html                           # resource page\n│   ├── 1996-11-10.trig                           # aggregated distribution\n│   └── index.html                               # resource page\n└── 1996-11-17/                                  # dataset node (another playlist)\n    ├── _dataset-flow/                                   # node flow (data)\n    │   ├── _default/                            # flow snapshot\n    │   │   ├── 1996-11-17.trig\n    │   │   └── index.html                       # resource page\n    │   ├── _working/                            # flow snapshot (draft)\n    │   │   ├── 1996-11-17.trig\n    │   │   └── index.html                       # resource page\n    │   └── index.html                           # resource page\n    ├── _meta/                                   # node flow (metadata)\n    │   ├── _default/                            # flow snapshot\n    │   │   ├── 1996-11-17_meta.trig\n    │   │   └── index.html                       # resource page\n    │   ├── _working/                            # flow snapshot (draft)\n    │   │   ├── 1996-11-17_meta.trig\n    │   │   └── index.html                       # resource page\n    │   └── index.html                           # resource page\n    ├── 1996-11-17.trig                           # aggregated distribution\n    └── index.html                               # resource page\n\n```\n","n":0.035}}},{"i":138,"$":{"0":{"v":"Weave Process","n":0.707},"1":{"v":"\n\n# Weave Process\n\n*(concept.weave-process)*\n\n## Purpose\n\nThe **weave process** transforms evolving Flow data (the **[[WorkingShot|mesh-resource.node-component.flow-shot.working-shot]]**) into:\n\n* a new immutable **[[Snapshot|mesh-resource.node-component.flow-shot.snapshot]]** (if versioning is enabled),\n* an updated **[[DefaultShot|mesh-resource.node-component.flow-shot.default-shot]]** (always),\n* updated metadata ([[meta-flow|mesh-resource.node-component.flow.node-metadata]]) and [[mesh-resource.node-component.documentation-resource.changelog]]\n* regenerated resource pages\n* and a stable, cross-node consistent “cut” of the mesh state(s).\n\nWeave provides the **atomic publication boundary** of a mesh: everything within the weave scope becomes internally consistent, visible to other tools, and ready for downstream consumption.\n\n---\n\n# Conceptual Model\n\nA **Flow** is a `dcat:DatasetSeries`.\nA Flow always contains three permanent subdirectories:\n\n* **`_working/`** – mutable; edited by humans & apps\n* **`_default/`** – mutable; holds latest published FlowShot\n* **Snapshot folders** – immutable; each a `FlowShot` with `weaveLabel` + `sequenceNumber`\n\nWeave operates on **FlowShots**:\n\n* `sflo:WorkingShot`\n* `sflo:DefaultShot`\n* `sflo:Snapshot` (immutable)\n\nA weave run **never deletes** snapshots; it only produces a new one (if enabled) and refreshes `_default/`.\n\n---\n\n# Weave Outcomes\n\nEvery weave produces:\n\n1. **A consistent cut of all `_working/` FlowShots in scope**\n   → optionally and by default stored as a new immutable Snapshot\n\n2. **A refreshed `_default/` FlowShot**\n   → always updated to match the weave result\n\n3. **Updated meta-flow**\n   → includes:\n\n   * new `weaveLabel`\n   * new `sequenceNumber`\n   * `previousSnapshot`\n   * provenance info\n     (Only includes *latest* provenance; older provenance lives in older snapshots.)\n\n4. **Regenerated Resource Pages**\n   → stable HTML index pages for nodes and flows\n\n5. **(Optional) Tombstone Updates**\n   → for submesh relocation or branch rehoming\n\n---\n\n# Two-Phase Weave\n\nWeave is structured as:\n\n## **Phase 1 — Snapshot Cut (Atomic Input Capture)**\n\nGoal: capture a consistent, static copy of all FlowShots being woven.\n\nSteps:\n\n1. Acquire weave locks for all nodes/flows in scope\n\n   * Subtree locking permitted\n   * Cross-mesh locking permitted\n   * Locks prevent *other weaves* and *sflo-host writes*\n\n2. For each `_working/` FlowShot:\n\n   * Stat + hash before read\n   * Copy contents into a staging directory\n   * Stat + hash again\n   * If changed during read → **Phase 1 abort (dirty during weave)**\n\n3. If all FlowShots are stable:\n\n   * Create the new `weaveLabel`\n   * Increment `sequenceNumber`\n   * Create new Snapshot folders (if versioning enabled)\n   * Create new DefaultShot folders (always)\n\nAt the end of Phase 1:\n\n* You have a complete staging area containing the cut.\n* Nothing has been published yet.\n* No writes into the published mesh have occurred.\n\n### If Phase 1 fails:\n\n* No mesh state changes.\n* Error appears in meta-flow logs on next successful weave.\n\n---\n\n## **Phase 2 — Materialization (Atomic Output Publish)**\n\nGoal: publish the staged FlowShots as the official new state.\n\nSteps:\n\n1. Atomic rename (per directory) from staging into:\n\n   * `…/<weaveLabel_vN>/` (Snapshot)\n   * `…/_default/` (DefaultShot)\n\n2. Update meta-flow:\n\n   * `sflo:weaveLabel`\n   * `sflo:sequenceNumber`\n   * `sflo:previousSnapshot`\n   * provenance fields\n     (Only latest provenance is stored; snapshots retain historical provenance.)\n\n3. Generate documentation Resource Pages\n\n4. Release weave locks\n\n### If Phase 2 fails:\n\n* Snapshots are still valid (they were already atomically written)\n* Resource pages may be regenerated in a later weave\n* Meta-flow will reflect incomplete status until next weave\n\n---\n\n# Locking Model\n\n### Weave locks are **at the node (or submesh) level**, not per-file.\n\nThis prevents:\n\n* simultaneous weaves in the same submesh\n* sflo-host writes colliding with weave\n\nEditors may still modify `_working`, which is why Phase 1 uses stat/hash detection.\n\n### Cross-mesh weaves **are allowed**\n\nsflo-host must acquire locks across all referenced meshes before Phase 1 begins.\n\n### Lock propagation\n\nRequesting a lock for a node implicitly requires locking all ancestors up to the mesh root.\nThis prevents parallel weaves on overlapping submeshes.\n\n---\n\n# Stability Guarantees\n\nWeave guarantees:\n\n* **Atomic publication**: readers see the new state only after Phase 2 completes.\n* **Consistency within the weave scope**: all flows included in the weave are aligned to the same cut.\n* **Monotonic `sequenceNumber` per Flow**: ordinality remains clean even with multiple meshes.\n* **Weave labels sorted lexicographically within each minute** (if using suffix model).\n\nWeave explicitly does *not* guarantee:\n\n* prevention of human text editors saving during the cut\n  (→ but it will detect and abort)\n\n---\n\n# Scope Levels\n\n### **Flow Weave**\n\n* Affects one flow + its meta-flow\n\n### **Node Weave**\n\n* Weaves all flows under one node\n* Meta-flow updates accordingly\n\n### **Node Tree Weave**\n\n* Recursively weaves node and descendants\n* Ideal for large cohesive units (directories)\n\n### **Cross-Mesh Weave**\n\n* Allowed\n* Good for Stagecraft’s multi-character simulations where updates must hit player-character mesh + GM-owned simulation mesh simultaneously\n\nLocking spans both meshes, forming a single weave consistency domain.\n\n---\n\n# DefaultShot Behavior\n\n* `_default/` is a **real FlowShot**\n* It is **not** a pointer; it contains real Folder/Files\n* It always mirrors the latest Snapshot (or staged result if versioning is off)\n* Its IRI is stable, so tools may dereference it without knowing weave labels\n\n### Unversioned flows\n\n* Still produce a DefaultShot\n* You can treat `_default/` as the active published dataset\n\n---\n\n# WorkingShot Behavior\n\n* Always present\n* Always mutable\n* Not modeled as an RDF class (optional)\n* Synchronized to `_default/` after each weave\n* Errors or partial edits allowed between weaves\n\n---\n\n# Features Preserved From Older Model\n\n### Interactive Mode\n\n* Step-through validation\n* AI-assisted metadata review\n\n### Tombstoning\n\n* Node relocation support\n\n### Resource Page Generation\n\n* Uses in-memory config cascade and templates\n* Node-specific templates override mesh-level templates\n\n### Transposition Detection\n\n* Ensures mesh transposability\n* Fixes any IRI base issues or structural inconsistencies\n\n---\n\n# Best Practices\n\n### “Weave Often”\n\nProduces clean snapshots and stable metadata.\n\n### “Weave Before Push”\n\nEnsures `_default/` is in sync with mesh state before publishing.\n\n### “Keep `_working` Small”\n\nBecause direct edits are allowed, large files increase dirty-during-weave likelihood.\n\n---\n\n# Quirks\n\n* Respect pre-existing hand-written `index.html` under `_assets/`\n  (unless explicitly allowed to regenerate)\n* Generated pages may include an invisible marker to allow safe regeneration\n\n---\n\n# File & Directory Expectations\n\n```\n/repo-root/\n├── _assets/\n│   ├── _templates/\n│   │   ├── default.html\n│   │   ├── ontology.html\n│   │   └── person.html\n│   └── _css/\n│       ├── default.css\n│       ├── ontology.css\n│       └── person.css\n└── my-ontology/\n    ├── _cfg-op/\n    ├── _assets/\n    │   ├── _templates/\n    │   └── _css/\n    └── _payload/\n```\n\n---\n\nIf you'd like, I can also generate:\n\n* A **parallel document** called `concept.weave-process.locking`\n* A **process flow diagram**\n* A **SHACL model** for FlowShots\n* A **step-by-step pseudo-code spec** of the weave engine\n","n":0.033}}},{"i":139,"$":{"0":{"v":"Weave Lock","n":0.707},"1":{"v":"\n- Locks prevent *other weaves* and *sflo-host writes* of the targeted nodes \n- no file locking\n","n":0.25}}},{"i":140,"$":{"0":{"v":"static site generation","n":0.577},"1":{"v":"\n- instead of generating to a docs folder or branch, the index files can just be generated in-place!\n- every resource's [[mesh-resource.node-component.documentation-resource.resource-page]] could look completely different \n    - theoretically could even be generated by a different site generator\n- weave + commit + push = publish!\n\n## What Gets Included\n\n- the [[mesh-resource.node-component.documentation-resource.readme]] would probably be the primary source in practice\n- you could supplement with AI-generated\n- you're limited only by what can be served statically, i.e., client-side, so you could have entire web applications hosted on a resource page\n- for [[mesh-resource.node.payload]], check to see if there are any named graphs (which are hopefully identified relative to the node with fragment identifiers, e.g. /ns/djradon/bio/#childhood)\n","n":0.096}}},{"i":141,"$":{"0":{"v":"Weave Label","n":0.707},"1":{"v":"\nA human-readable, sortable identifier for a given weave that is used in composing [[snapshot folder names|folder.snapshot]].\n\nThe value encodes the UTC date and time of [[mesh-resource.node-component.flow-shot.snapshot]] creation (weave) in the format `YYYY-MM-DD_HHMM_SS` (with underscores between date, time, and seconds), enabling simple chronological (lexical) comparison in SPARQL and across file-system hierarchies.\n\nFormat: `YYYY-MM-DD_HHMM_SS`\n- Example: `2025-11-24_0142_55`\n\nThe weave label is combined with a sequence number to form complete snapshot folder names:\n- Full format: `YYYY-MM-DD_HHMM_SS_vN`\n- Example: `2025-11-24_0142_55_v7`\n\nUsed as the human-readable component of snapshot folder names and as a lightweight temporal ordering key when full RDF provenance data is unavailable.\n","n":0.104}}},{"i":142,"$":{"0":{"v":"Concept Summary","n":0.707},"1":{"v":"\n# Semantic Mesh — LLM-Oriented Concept Summary\n\nThis document is the canonical, compact context for LLMs. It summarizes all `documentation/concepts.*` notes and cross-links to authoritative pages.\n\n0) Semantic Flow Twin Purposes\n- Mint dereferenceable IRIs for referring to things on the Semantic Web\n- Hold versionable semantic data that uses those IRIs\n\n1) Definition\nA semantic mesh is a dereferenceable, possibly-versioned corpus of semantic resources where every IRI resolves to meaningful content. \n\nA filesystem-based mesh maps directly from a Git repository’s folder hierarchy to a published static site so that:\n- Every resource is addressable by a stable IRI.\n- \"[[Naming resources|facet.resource.naming]]\" are dereferenceable via generated `index.html` resource pages.\n- \"[[Content resources|facet.resource.content]]\" are directly dereferenceable: they should return a file\n- RDF datasets live as distributions on versioned flow snapshots.\n- The weave process maintains coherence and keeps the repo publish-ready.\n\nSee:\n- [[concept.mesh]]: definition, requirements\n- [[concept.semantic-flow-site]]: site posture\n- [[concept.single-mesh-repo]]: repo-to-site mapping\n\n2) Design Principles\n- [[principle.dereferencability-for-humans]]: resource pages\n- [[principle.single-referent]]: concept vs content is explicit\n- [[principle.pseudo-immutability]]: treat snapshots/IDs as immutable\n- [[principle.transposability]]: move meshes without breaking links via relative IDs\n- [[principle.composability]]: extract/compose submeshes\n\n3) Core Abstractions\n\n3.1 Mesh Resources (Nodes and Components)\n- Node (folder; container for nodes & components): [[mesh-resource.node]]\n  - bare node: organizational IRI segment container: [[mesh-resource.node.bare]]\n  - payload node: IRI refers to the node's referent (real-world entity or dataset concept); has a payload flow: [[mesh-resource.node.payload]]\n\n- Node component (terminal resource supporting a node): [[mesh-resource.node-component]]\n  - Flows (abstract datasets as DatasetSeries):\n    - Meta flow (metadata/provenance): [[mesh-resource.node-component.flow.node-metadata]]\n    - payload flow (payload data): [[mesh-resource.node-component.flow.payload]]\n    - Node-config flows (settings; see §9): [[mesh-resource.node-component.flow.node-config]]\n  - FlowShots (concrete Datasets): `_default/`, `_working/`, snapshot folders (e.g., `2025-11-24_0142_07_v1/`)\n    - Overview: [[mesh-resource.node-component.flow-shot]]\n    - `_default/`: [[mesh-resource.node-component.flow-shot.default-shot]]\n    - `_working/`: [[mesh-resource.node-component.flow-shot.working-shot]]\n    - Snapshot folders: [[mesh-resource.node-component.flow-shot.snapshot]]\n    - Distributions: [[mesh-resource.node-component.snapshot-distribution]]\n  - Handle (refer to the node “as a mesh resource”): [[mesh-resource.node-component.node-handle]]\n    - Handle page (human-facing): [[mesh-resource.node-component.node-handle.page]]\n  - Asset tree (static files for the node): [[mesh-resource.node-component.asset-tree]]\n  - Documentation resources (README/CHANGELOG/resource pages/fragments):\n    - README: [[mesh-resource.node-component.documentation-resource.readme]]\n    - CHANGELOG: [[mesh-resource.node-component.documentation-resource.changelog]]\n    - Resource page (index.html): [[mesh-resource.node-component.documentation-resource.resource-page]]\n    - Resource fragment: [[mesh-resource.node-component.documentation-resource.resource-fragment]]\n  - Aggregated distribution (optional roll-up of child node data): [[mesh-resource.node-component.aggregated-distribution]]\n\n3.2 Facets (Folder, File, Dataset)\n- Folder facet (namespace mapping; reserved folders): [[facet.filesystem.folder]]\n- File facet (content retrieval): [[facet.filesystem.file]]\n- Dataset facet (DatasetSeries vs Dataset): [[facet.resource.dataset]]\n\n4) Addressing and Identity\n\n4.1 Namespace and Relative Identifiers\n- Folder names become namespace segments; the path is the node’s relative identifier (and IRI path when published).\n- Relative identifiers are used within distributions for transposability; resolve relative to distribution location.\nSee:\n- [[concept.namespace]]: overview\n- [[concept.namespace.segment]]: segment definition\n- [[concept.namespace.segment.system]]: reserved segments\n- [[concept.identifier.intramesh.relative]]: relative IDs\n\n4.2 IRI Semantics\n- Concept IRIs (slash-terminated) identify nodes, flows (abstract), snapshots (conceptual), and handle.\n- Content IRIs (with filenames) identify retrievable files: distributions, HTML pages, READMEs, assets.\n- Follow document-vs-thing hygiene to avoid ambiguity.\nSee:\n- [[concept.identifier]]: IRI types and mapping\n- [[faq.reference-iri-choices]]: trade-offs\n- [[concept.iri]]: terminology; prefer “IRIs” when referring to mesh-local IRIs\n\n4.3 Handle Rationale\n- A node’s IRI refers to its referent (namespace, real-world entity, or dataset concept).\n- The handle component provides a IRI to refer to the node itself “as a mesh resource” (for config, provenance, lifecycle).\nSee:\n- [[mesh-resource.node-component.node-handle]]\n- [[mesh-resource.node-component.node-handle.page]]\n\n5) Physical Structure and Reserved Folders\n\nReserved folder names (underscore-prefixed; canonical set):\n- `_node-handle/`\n- Flow containers (abstract datasets):\n  - `_meta/`, `_payload/`\n  - `_cfg-op/`, `_cfg-inh/` (see §9)\n- FlowShots inside a flow:\n  - `_default/`, `_working/`, snapshot folders with format `YYYY-MM-DD_HHMM_SS_vN/` (e.g., `2025-11-24_0142_07_v1/`, `2025-11-24_0142_08_v2/`, …)\n- Assets:\n  - `_assets/` (static files)\n\nFolder-note pages for these reserved names live under `folder.*.md` (where defined):\n- `_meta/`: [[folder._meta]]\n- `_payload/`: [[folder._data-flow]]\n- `_cfg-op/`: [[folder._cfg-op]]\n- `_cfg-inh/`: [[folder._cfg-inh]]\n- `_default/`: [[folder._default]]\n- `_working/`: [[folder._working]]\n- Snapshot folders (`YYYY-MM-DD_HHMM_SS_vN/`): [[folder.flowshot]]\n- `_assets/`: [[folder._assets]]\n- Node folder pages:\n  - Node: [[folder.node]]\n\n6) Data and Versioning Model\n- Only flows are versioned (flows are DatasetSeries). Nodes are not versioned.\n- FlowShots (flow realizations):\n  - `_default/`: latest stable realization; after weave it equals the content of the latest snapshot.\n  - `_working/`: mutable working area.\n  - Snapshot folders (format `YYYY-MM-DD_HHMM_SS_vN/`): immutable history for precise citation and provenance.\n- Working distribution: `_working/` typically contains a single editable source; weave can fan-out serializations.\n- Sibling distribution: patterns and constraints for multi-file realizations.\nSee:\n- [[concept.flow-version]]\n- [[mesh-resource.node-component.snapshot-distribution.working]]\n- [[concept.sibling-distribution]]\n\n7) Lifecycle and Weave Process\nWeave maintains structural coherence and publication readiness:\n- Ensures required system components exist.\n- If versioning is enabled, creates a new snapshot folder (format `YYYY-MM-DD_HHMM_SS_vN/`) from `_working/`.\n- Promotes `_working/` contents to `_default/`.\n- Updates meta/provenance; regenerates resource pages.\n- Resolves internal links to maintain transposability.\n- Integrates with the scanner where applicable.\nSee:\n- [[concept.weave-process]]\n- [[concept.weave-process.resource-page-generation]]\n- [[concept.scanner]]\n- [[concept.metadata.provenance]]\n\n8) Publishing and Sites\n- Repos are static-site-ready; pushing to GitHub Pages or any static host publishes the mesh (folder paths → IRI paths).\n- Transposition (domain/project move) is safe with relative IDs.\nSee:\n- [[concept.single-mesh-repo]]\n- [[concept.semantic-flow-site]]\n- [[concept.publication]]\n\n1) Configuration and Inheritance (Two Config Flows)\n- Operational Config Flow: final, resolved settings for a node (consumer). Overrides apply here.\n- Inheritable Config Flow: settings a node offers to descendants (provider). Property-level merge; order: parent → … → service → platform; propagation can be firewalled.\n- Resolution: a single inheritance mechanism resolves operational config from inheritable configs plus service/platform defaults. Explicit operational entries override inherited ones.\nSee:\n- [[mesh-resource.node-component.flow.node-config]]: overview\n- [[mesh-resource.node-component.flow.node-config.operational]]\n- [[mesh-resource.node-component.flow.node-config.inheritable]]\n- [[mesh-resource.node-component.node-config-defaults]]: defaults as inheritable values\n\n1)  Aggregated Views\n- Aggregated distribution: optional roll-up of child payload nodes’ default datasets at a parent node for convenience.\nSee:\n- [[mesh-resource.node-component.aggregated-distribution]]\n\n1)  Minimal File Tree Example\n\n```\n/repo-root/\n├── _assets/                         # optional site-wide assets\n├── my-node/                         # a mesh node (folder)\n│   ├── _node-handle/                # handle component (resource.node-component.node-handle)\n│   ├── _meta/                  # metadata flow (system)\n│   │   ├── _default/\n│   │   └── 2025-11-24_0142_07_v1/\n│   ├── _payload/                  # payload flow (for payload nodes)\n│   │   ├── _default/\n│   │   ├── _working/\n│   │   └── 2025-11-24_0142_07_v1/\n│   ├── _cfg-inh/    # provider config (optional)\n│   ├── _cfg-op/    # resolved config (optional; may be system-written)\n│   ├── index.html                   # resource page\n│   ├── README.md\n│   └── CHANGELOG.md\n└── docs/ or public host mapping     # publication target\n```\n\n12) Visual Overview\n\n```mermaid\ngraph TD\n  A[Mesh Node] --> B[Handle]\n  A --> C[Meta flow]\n  A --> E[payload flow]\n  A --> G[Asset tree]\n  A --> H[Resource pages]\n\n  C --> C1[_default]\n  C --> C2[Snapshots]\n  E --> E1[_default]\n  E --> E2[_working]\n  E --> E3[Snapshots]\n```\n\n13) Glossary\n- [[concept.mesh]]: the set of addressable resources in a repository, published as a site\n- [[mesh-resource.node]]: an extensible  resource containing other nodes and its own components\n- [[mesh-resource.node-component]]: terminal resource that supports node behavior/structure\n- [[mesh-resource.node-component.flow]]: DatasetSeries representing an abstract dataset (meta/data/config)\n- [[mesh-resource.node-component.flow-shot]]: concrete Dataset realization of a flow (Snapshot, DefaultShot, WorkingShot)\n- [[mesh-resource.node-component.snapshot-distribution]]: a concrete serialization file (TriG, JSON-LD, etc.)\n- [[mesh-resource.node-component.node-handle]]: indirection to refer to the node \"as a mesh resource\"\n- [[mesh-resource.node-component.documentation-resource.resource-page]]: dereferenceable `index.html` for folders\n- [[concept.weave-process]]: lifecycle operation to version/promote/regenerate/repair\n","n":0.032}}},{"i":143,"$":{"0":{"v":"mesh repo","n":0.707},"1":{"v":"\nA Semantic Flow single-mesh repository (or mesh repo for short) is a git repository that contains a [[concept.root-node]] at the top of the repo, and any number of additional, contained [[mesh-resource.node]]\n\n","n":0.18}}},{"i":144,"$":{"0":{"v":"Sibling Distribution","n":0.707},"1":{"v":"\n- same data, different syntax","n":0.447}}},{"i":145,"$":{"0":{"v":"Semantic Flow site","n":0.577},"1":{"v":"\nSemantic Flow sites provide:\n\n- dereferencability for mesh [[concept.identifier]]s\n- hosting versioned RDF datasets and their histories\n\n## Publishing\n\n- For sites exposed by Github/Gitlab Pages functionality, pushing a [[concept.single-mesh-repo]] effectively publishes it.\n","n":0.186}}},{"i":146,"$":{"0":{"v":"Scanner","n":1},"1":{"v":"\n- scans datasets and dataset distributions, which can be :\n  - local folders\n  - git repos\n  - compliant IRIs\n  - SPARQL data sources\n- can filter which subfolders to include/exclude\n- \n","n":0.183}}},{"i":147,"$":{"0":{"v":"root node","n":0.707},"1":{"v":"\nThe node at the top of a mesh hierarchy may be referred to as the root node. \n\nEvery other [[mesh-resource]] in a mesh \"lives under\" the root node.\n\nFor pure [[concept.single-mesh-repo]]s, the repository's name is used as root node's identifier. \n\nFor [[concept.mesh.embedded]], the root node's folder name is its identifier.\n\nA root node is not treated or represented any differently than any other [[mesh-resource.node]], and it is not differentiated in metadata. So any node may become a root node simply by copying it somewhere that's not already a mesh.\n","n":0.107}}},{"i":148,"$":{"0":{"v":"Referent","n":1},"1":{"v":"\n## Definition\n\nThe *referent* is the thing (real or imaginary) to which a [[resource|mesh-resource]]’s [[concept.identifier]] **refers**. Every identifier [[denotes|concept.denotation]] its referent, \n\n## Node vs. referent\n\n- **Referent**: the subject that the node’s identifier names (a person, concept, event, dataset, etc.).\n- **Node**: the mesh construct that provides an identifier for and contains linked data about the referent. It also provides linked data about itself (as a node), and may contain other resources used for supporting [[concept.semantic-flow-site]]s.\n\nTo talk about the node itself, you use its **node handle** (e.g. published IRI `https://ns.example.org/persons/djradon/_node-handle` or mesh identifier `<djradon/_node-handle>`).\n\n**Where it’s described**\n\n* The **referent’s description** lives in the node’s [[mesh-resource.node-component.flow.reference]].\n* The **node’s own metadata and provenance** live in the **`_node-*` flows** (e.g. `_meta`, `_node-config-*`).\n\n**Special case: payload nodes**\n\n* In a **payload node**, the **referent** is not an external entity but an **evolvable dataset** contained in the node.\n* The dataset evolves as versioned distributions inside the node’s `_payload` (e.g. `v1/`, `v2/`, …).\n* The `_ref` may describe the dataset, e.g. its **name, type, and provenance**.\n* Example:\n\n  * Node IRI: `https://ns.example.org/projects/atlas/`\n  * Referent: *the Atlas dataset* (identified by the node IRI, evolving over time).\n  * `_ref`: declares it as a dataset, supplies label and attribution.\n  * `_payload`: provides concrete versions (`v1`, `v2`, …).\n\n\n## Why referent matters\n\nUnderstanding what a IRI refers to is crucial for proper semantic web implementation. In the past, people have tried to use content IRIs to represent the things they refer to. A classic example is using `http://example.org/person.html` to identify a person, when it actually identifies an HTML document about the person. This conflation creates semantic ambiguity and breaks linked data principles.\n\nSemantic Flow enforces clear referent distinctions through IRI patterns: slash-terminated IRIs always refer to concepts or entities, while extension-terminated IRIs always refer to retrievable content. This prevents the classic \"document vs thing\" confusion that has plagued semantic web implementations.\n","n":0.058}}},{"i":149,"$":{"0":{"v":"publication","n":1},"1":{"v":"\n- A mesh is \"published\" when it becomes accessible by using an absolute URL, (e.g., via a web browser). \n\n\n## Publication History Tracking\n\nThe inferred publication locations can be tracked to maintain a history of where a node has been published, which aids in citation consistency and discovery:\n\n```turtle\n# In _flow/ metadata\n<_node-handle> mesh:publishedAt <https://myorganization.github.io/data-mesh/ns/djradon/> ;\n          mesh:previousPublications ( \n            <https://djradon.github.io/ns/djradon/>\n            <https://oldsite.com/research/ns/djradon/>\n          ) ;\n```\n\nThis allows external citations to find resources even after they've been moved, and provides a clear provenance trail.\n","n":0.114}}},{"i":150,"$":{"0":{"v":"platform element","n":0.707},"1":{"v":"\nA **platform element** is any aspect of the Semantic Flow platform, including:\n\n- ontologies and vocabularies\n- conventions (naming, filesystem, best practices)\n- mesh and namespace structure\n- principles (design, philosophy)\n- meshes, nodes, and node components\n- workflows and lifecycle operations\n- documentation\n- code and tests\n","n":0.158}}},{"i":151,"$":{"0":{"v":"node config","n":0.707},"1":{"v":"\n## per-node config specification\n\nNode configuration determines:\n\n- flow versioning (on/off; perhaps customweaveLabl)\n- resource page and resource fragment generation\n- distribution syntaxes\n- template usage and stylesheets\n- attribution/provenance defaults\n\nNode configuration is held in memory by the [[product.sflo-host]], and is calculated when the application starts.\n\nNode configuration is at least partially determined by \"config specification\", which happens in the two [[mesh-resource.node-component.flow.node-config]]:  [[mesh-resource.node-component.flow.node-config.operational]] and [[mesh-resource.node-component.flow.node-config.inheritable]] (which can be inherited to contained nodes).\n\nIf config specification is missing, (i.e., config spec inheritance is turned off or unspecified), node configuration will be determined from application-level config specification. In case there is none, the service will use sensible defaults at the platform level.\n\n### Initial Config Specification\n\n- When a node is initially created, if inheritance is turned on for its parent node, it will have its [[mesh-resource.node-component.flow.node-config]] populated based on any parent [[mesh-resource.node-component.node-config-defaults]] files present in the hierarchy. If there are none, its [[mesh-resource.node-component.flow-shot.default-shot]] will not be created.\n\n### Calculating Node Config\n\nWhen the [[product.sflo-host]] starts, it calculates non-default config settings for every node.\n\n- determines the \"default\" settings for this service instance from [[product.service.config]]\n- if the node has a [[mesh-resource.node-component.flow.node-config]] , the service will use any settings there that differ from its defaults\n- if config-inheritance is turned on for a node, the service will scan back up the hierarchy to compose any missing \"non-default\" settings\n-  the result is an in-memory \"shadow mesh\" known as the [[product.service.components.node-config-map]] containing any non-default settings for the mesh\n\nIf calculated config matches the service defaults, they are ignored.\n\n## per-service settings for node defaults\n\n- [[product.service.config]] can establish any mesh-wide settings that diverge from the system defaults\n\n## platform node-config defaults\n\nSemantic Flow uses sensible defaults, specified in the so that neither node-level nor service-level \"non-default\" settings are necessary\n\n- by default:\n  - versioning is turned on for all flows\n  - distribution syntaxes are .trig and jsonld\n  - resource pages are generated using a standard template and CSS file that get copied into a [[concept.single-mesh-repo]]'s root [[mesh-resource.node-component.asset-tree]] upon initialization\n  - [[mesh-resource.node-component.aggregated-distribution]] are not generated\n  - [[concept.mesh.resource.element.flow.unified]]\n","n":0.056}}},{"i":152,"$":{"0":{"v":"Namespace","n":1},"1":{"v":"\n## Overview\n\nA namespace is the hierarchical address space formed by nesting nodes. Every node extends the namespace with its identifier, which correspond to filesystem folders when the mesh is stored in the filesystem. The resulting path maps directly to the published IRI when appended to the [[concept.namespace.context]]\n\n- Concept vs content IRI semantics: see [[concept.identifier]]\n- How intramesh identifiers are resolved: see [[concept.identifier.intramesh]]\n\n\n## Minimal Example\n\n```file\n/ns/                         # bare node → https://ex.org/ns/\n└── people/                  # bare node → https://ex.org/ns/people/\n    └── alice/               # reference node → https://ex.org/ns/people/alice/\n```\n\n- Folder names correspond to [[concept.identifier.intramesh]]s and become namespace segments when [[published|concept.publication]].\n- Slash-terminated IRIs identify concepts; file IRIs identify content (see [[concept.identifier]]).\n\n## Publishing Base\n\nThe site’s base IRI is determined by the publishing platform (e.g., GitHub Pages or self-hosting with [[product.sflo-host]]). See [[concept.implied-rdf-base]] for user/org vs project page mappings and guidance on avoiding hardcoded bases.\n","n":0.086}}},{"i":153,"$":{"0":{"v":"namespace segment","n":0.707},"1":{"v":"\n## Definition\n\nA namespace segment is a single \"folder resource\" identifier that extends the a mesh's namespace. The concatenation of parent identifiers yields the namespace for a node.\n\n- Concept vs content IRI semantics: see [[concept.identifier]]\n- How relative identifiers are resolved: see [[concept.identifier.intramesh]]\n\n## Naming (recommended)\n\n- Use camel-case (initial lowercase letter), e.g., `people`, `myProjects`\n- Maybe avoid starting segment names names with an underscore (`_`); underscore-prefixed names are used for [[concept.namespace.segment.system]]\n\nThese are recommendations based on RDF conventions, not hard rules; sometimes projects have good reasons to diverge.\n\n## Stability\n\nRenaming a segment probably breaks the identifier (IRI) of all contained resources. If you must rename:\n\n- Consider redirect/tombstoning strategies and publication history — see [[concept.publication]]\n- Review impacts on inbound references; plan a weave and re-publish cycle\n- see also [[feature.handling-renaming]]\n\n## Example\n\n```file\n/ns/                         → https://ex.org/ns/\n└── datasets/                → https://ex.org/ns/datasets/\n    └── census/              → https://ex.org/ns/datasets/census/\n```\n\n- Each folder adds exactly one namespace segment\n- Folders map directly to slash-terminated concept IRIs (see [[concept.identifier]])\n","n":0.082}}},{"i":154,"$":{"0":{"v":"system segments","n":0.707},"1":{"v":"\nSystem segments are underscore-prefixed folder names reserved by the platform. Prefer not to use `_`-prefixed names for user-defined segments.\n\nThis page is the canonical list; see the linked docs for behavior and details.\n\n## Flows (abstract/series)\n\n- [[_meta/|folder._meta]]\n- [[mesh-resource.node-component.flow.payload]]\n- [[mesh-resource.node-component.flow.reference]]\n- [[_cfg-op/|folder._cfg-op]]\n- [[_cfg-inh/|folder._cfg-inh]]\n\n## Snapshots (concrete)\n\n- [[_current/|folder._default]]\n- [[_next/|folder._working]]\n- [[folder.snapshot]]\n\n## Other reserved\n\n- [[_node-handle/|folder._node-handle]]\n- [[_assets/|folder._assets]]\n\nFor IRI semantics: see [[concept.identifier]]. For namespace background: see [[concept.namespace]] and [[concept.namespace.segment]].\n","n":0.13}}},{"i":155,"$":{"0":{"v":"namespace root","n":0.707},"1":{"v":"\nSince every mesh has to have a single root node, that node's identifier can be referred to as the **namespace root**. It can be combined with the [[concept.namespace.context]] to make an [[concept.iri]]. \n\nFor [[concept.single-mesh-repo]]s, the namespace root uses the repo name.\n","n":0.156}}},{"i":156,"$":{"0":{"v":"namespace context","n":0.707},"1":{"v":"\n\n## Overview\n\nThe **namespace context** is the URL under which a [[concept.semantic-flow-site]] publishes all mesh identifiers. It \"contains\" the mesh and so is outside the mesh’s namespace. It is determined by the hosting platform.\n\nSites have a namespace context that starts with `http://` or `https://`.\n\nFilesystem-based meshes have a namespace context that starts with `file://`.\n\nThe context is a deployment concern; a [[woven|concept.weave-process]] mesh should be valid regardless of where it is served.\n\n## Platform mappings \n\n### GitHub Pages\n\n- User/Org site:\n  - namespace context: `https://org.github.io/`\n  - Mesh namespace `/ns/people/alice/` publishes at `https://org.github.io/ns/people/alice/`\n- Project site context:\n  - `https://org.github.io/repo/`\n  - Mesh path `/ns/people/alice/` publishes at `https://org.github.io/repo/ns/people/alice/`\n\nThese mappings can be accomplished with both [[concept.single-mesh-repo]]s and [[concept.mesh.embedded]]\n\n","n":0.096}}},{"i":157,"$":{"0":{"v":"Named Graphs","n":0.707}}},{"i":158,"$":{"0":{"v":"Metadata","n":1},"1":{"v":"\nAside from the metadata involved in datasets themselves, semantic meshes have operational metadata that capture things like:\n\n  [[concept.metadata.provenance]] (entities, agents, activities)\n- Copyright and licensing\n- validation / consistency checks\n- metrics","n":0.186}}},{"i":159,"$":{"0":{"v":"Provenance","n":1},"1":{"v":"\n## Core Principles\n\n**Version-only provenance** - Provenance reference in [[mesh-resource.node-component.flow-shot.default-shot]] should reference its corresponding stable version; \n\n**Meta-flow storage** - Semantic Flow-specific provenance lives in meta-flows, referencing snapshots in other flows. Domain-specific provenance can live in datasets themselves.\n\n**default snapshot duplication** - `_default` meta snapshots contain identical copies of the latest version's provenance with base URI pointing to the snapshot for stable fragment resolution.\n\n## Fragment Identifier Naming Scheme\n\nTo ensure that every RDF node used in a [[mesh-resource.node-component.flow.node-metadata]] distribution has a unique and dereferenceable URI, the following naming scheme for [[fragment identifiers|concept.fragment-identifiers]] MUST be used. This allows the metadata snapshot's [[mesh-resource.node-component.documentation-resource.resource-page]] to correctly provide anchors for all provenance entities.\n\nThe structure is as follows:\n\n`<{flow-slug}-{version}-{entity-type}[-{unique-part}]>`\n\n-   **`{flow-slug}`**: The slug of the flow this provenance describes (e.g., `[[folder._cfg-inh]]`, `data-flow`). This provides the primary namespace for the identifier.\n-   **`{version}`**: The version of the target flow's snapshot (e.g., `v47`). This scopes the provenance to a specific point in time.\n-   **`{entity-type}`**: The class of the entity, using a consistent UpperCamelCase (e.g., `Activity`, `Context`, `DelegationChain`, `DelegationStep`).\n-   **`{unique-part}`**: (Optional) A unique suffix, such as a step number or a timestamp, used when multiple entities of the same type exist for the same flow and version.\n\n### Provenance Fragment Identifier Examples\n\nFor a `config-flow` at version `v47`, the identifiers would be:\n\n-   **Activity**: `<#config-flow-v47-Activity>`\n-   **Provenance Context**: `<#config-flow-v47-Context>`\n-   **Delegation Chain**: `<#config-flow-v47-DelegationChain>`\n-   **Delegation Steps**:\n    -   `<#config-flow-v47-DelegationStep-1>`\n    -   `<#config-flow-v47-DelegationStep-2>`\n\n\n## Architecture\n\n### snapshot Provenance\n\n```turtle\n# In my-dataset/_meta/2025-07-20_1430_00_v47/my-dataset_meta.trig\n@base <../2025-07-20_1430_00_v47/> .\n\n# Weave activity with PROV standard properties\n:#configUpdateActivity a meta:ConfigWeave ;\n    prov:startedAtTime \"2025-07-20T14:30:00Z\" ;\n    prov:endedAtTime \"2025-07-20T14:30:15Z\" ;\n    prov:used <../../_config-flow/2025-07-20_1429_30_v46/config.jsonld> ;\n    prov:generated <../../_config-flow/2025-07-20_1430_00_v47/config.jsonld> ;\n    prov:wasAssociatedWith <https://semantic-flow.org/agents/flow-service-bot> .\n\n# Rights and licensing at snapshot level\n<../../_config-flow/2025-07-20_1430_00_v47> dcterms:rightsHolder <https://orcid.org/0000-0002-1825-0097> ;\n                          dcterms:license <https://creativecommons.org/licenses/by-sa/4.0/> ;\n                          prov:has_provenance :configProvenance .\n\n# Delegation chain (step 1 = top authority, gets copyright by default)\n:configProvenance a meta:ProvenanceContext ;\n    meta:forActivity :configUpdateActivity ;\n    meta:forSnapshot <../../_config-flow/2025-07-20_1430_00_v47> ;\n    prov:wasAttributedTo <https://acme-corp.com/org> ; # Primary attribution\n    meta:delegationChain :delegationChain_001 .\n\n:delegationChain_001 meta:hasStep :step1, :step2, :step3 .\n\n:step1 a meta:DelegationStep ;\n       meta:stepOrder 1 ;\n       prov:agent <https://acme-corp.com/org> . # Prime mover, no actedOnBehalfOf\n\n:step2 a meta:DelegationStep ;\n       meta:stepOrder 2 ;\n       prov:agent <https://orcid.org/0000-0002-1825-0097> ;\n       prov:actedOnBehalfOf <https://acme-corp.com/org> .\n\n:step3 a meta:DelegationStep ;\n       meta:stepOrder 3 ;\n       prov:agent <https://semantic-flow.org/agents/flow-service-bot> ;\n       prov:actedOnBehalfOf <https://orcid.org/0000-0002-1825-0097> .\n```\n\n### default snapshot Copy\n\n```turtle\n# In my-dataset/_meta/_default/my-dataset_meta.trig\n@base <../2025-07-20_1430_00_v47/> .\n\n# Identical content to snapshot - all URIs resolve to stable version\n# (same provenance content as above)\n```\n\n### Unversioned Flow Accumulation\n\nFor flows without versioning, activities accumulate in `_working` with unique timestamps:\n\n```turtle\n# In my-dataset/_meta/_working/my-dataset_meta.trig\n:dataActivity_2025-07-20_14-30 a meta:DataWeave ;\n    prov:startedAtTime \"2025-07-20T14:30:00Z\" ;\n    prov:generated <../../_payload/_default/data.trig> .\n\n:dataActivity_2025-07-20_16-45 a meta:DataWeave ;\n    prov:startedAtTime \"2025-07-20T16:45:00Z\" ;\n    prov:used <../../_payload/_default/data.trig> ;\n    prov:generated <../../_payload/_default/data.trig> .\n```\n\n## Key Components\n\n### Activity Types (subclass `prov:Activity`)\n- `meta:ConfigWeave`, `meta:ReferenceWeave`, `meta:DataWeave`, `meta:MetaWeave`\n- `meta:NodeWeave` (entire node), `meta:NodeTreeWeave` (recursive)\n\n### Provenance Entities (subclass `meta:ProvenanceEntity`)\n- `meta:ProvenanceContext` - Relator for complex authorship scenarios\n- `meta:DelegationChain` / `meta:DelegationStep` - Authorization chains\n- `meta:AgentRoleCollection` / `meta:AgentRole` - Collaborative role assignments\n\n### Standard Properties Used\n- `prov:agent`, `prov:actedOnBehalfOf`, `prov:wasAttributedTo` (instead of custom properties)\n- `dcterms:rightsHolder`, `dcterms:license` (rights at snapshot level)\n- `prov:has_provenance` (link snapshots to provenance contexts)\n\n## Delegation Chain Pattern\n\n**Step ordering**: Lower numbers = higher authority\n- Step 1: Prime mover (organization) - gets copyright by default, no `prov:actedOnBehalfOf`\n- Step 2+: Each agent acts on behalf of the previous step's agent\n- Tools/software agents typically at the end of the chain\n\n## Configuration\n\n**Copyright assignment**: Configurable in node-config-defaults, defaults to first agent in delegation chain (step 1).\n\n**External vocabulary tracking**: Use SHACL to declare recommended external properties like `prov:wasInfluencedBy`, `dcterms:license`.\n\n## Implementation Notes\n\n- **Fragment URIs**: Use `<#step1>` etc. within snapshots for stable addressability\n- **Base URI**: All snapshots use `@base <../YYYY-MM-DD_HHMM_SS_vN/>` pattern for consistent resolution\n- **Rights inheritance**: Capture previous version rights holders in provenance contexts when content is derived\n- **Static site friendly**: Documentation approach for external references since no server-side redirects available\n\n","n":0.042}}},{"i":160,"$":{"0":{"v":"semantic mesh","n":0.707},"1":{"v":"\n## Overview\n\nA **semantic mesh** is a [[pseudo-immutable|principle.pseudo-immutability]] collection of (possibly-versioned) linked-data resources. It organizes these resources in a  [[publishable|concept.publication]] way, such that a mesh can be used as a [[semantic site|concept.semantic-flow-site]] where every HTTP IRI returns meaningful content.\n\n### Key characteristics\n\n- **Addressable**: Every [[mesh-resource]] has an [[concept.identifier]]; when a mesh is [[published|concept.publication]], every [[mesh-resource]] then gets a globally unique, human-readable IRI\n- **Versioned**: Changes are managed through the [[Weave Process|concept.weave-process]] process, and [[mesh-resource.node-component.flow]] are versioned by default\n- **Publish-ready**: Can be served directly via GitHub Pages or similar static hosting; or via a local web server like live-server\n\n## Core Concepts\n\n### Mesh Resources\n\nThe primary constituents of a mesh are [[mesh-resource.node]]s. Nodes contain their own [[mesh-resource.node-component]]s, and may also contain other nodes. \n\n#### Mesh Nodes\n\n[[Mesh nodes|mesh-resource.node]] extend [[concept.namespace]]s and serve as containers.\n\n- **[[bare nodes|mesh-resource.node.bare]]**: Empty containers for organizing other mesh nodes\n- **[[reference nodes|mesh-resource.node.reference]]**: Nodes that refer to entities (people, places, concepts, etc.)\n- **[[payload nodes|mesh-resource.node.payload]]**: Nodes containing data distributions with optional versioning\n\n\n#### Node components\n\n[[Node components|mesh-resource.node-component]] help define, support, and systematize nodes.\n\n\n#### Example Mesh\n\n[[Mesh resources|mesh-resource]] have at least one [[concept.identifier]] and (usually) a [[concept.referent]].\n\n| [[concept.identifier.intramesh]]                                | Semantic Flow resource type                                           | referent                     |\n| --------------------------------------------------------------- | --------------------------------------------------------------------- | ---------------------------- |\n| `ns/`                                                           | [[mesh-resource.node.bare]]                                           | - nothing - (yet!)           |\n| `ns/djradon/`                                                   | [[mesh-resource.node.reference]]                                      | person                       |\n| `ns/djradon/_node-handle/`                                      | [[mesh-resource.node-component.node-handle]]                          | mesh node                    |\n| `ns/djradon/index.html`                                         | [[mesh-resource.node-component.documentation-resource.resource-page]] | resource page (content)      |\n| `ns/djradon/README.md`                                          | [[mesh-resource.node-component.documentation-resource.readme]]        | README file (content)        |\n| `ns/djradon/CHANGELOG.md`                                       | [[mesh-resource.node-component.documentation-resource.changelog]]     | README file (content)        |\n| `ns/djradon/_ref/`                                              | [[mesh-resource.node-component.flow.reference]]                       | reference flow               |\n| `ns/djradon/_ref/_working/`                                     | [[mesh-resource.node-component.flow-shot.working-shot]]               | reference flow snapshot      |\n| `ns/djradon/_ref/_working/djradon.jsonld`                       | [[mesh-resource.node-component.snapshot-distribution.working]]        | reference flow snapshot      |\n| `ns/djradon/_meta/`                                             | [[mesh-resource.node-component.flow.node-metadata]]                   | node metadata dataset series |\n| `ns/djradon/_meta/_default/`                                    | [[mesh-resource.node-component.flow-shot.default-shot]]               | node metadata dataset        |\n| `ns/djradon/picks/`                                             | [[mesh-resource.node.payload]]                                        | abstract dataset             |\n| `ns/djradon/picks/_payload/`                                    | [[mesh-resource.node-component.flow.payload]]                         | payload dataset series       |\n| `ns/djradon/picks/_payload/2025-11-24_0142_07_v1/`              | [[mesh-resource.node-component.flow-shot.snapshot]]                   | concrete payload dataset     |\n| `ns/djradon/picks/_payload/2025-11-24_0142_07_v1/picks.jsonld ` | [[mesh-resource.node-component.snapshot-distribution.version]]        | paylod dataset distribution  |\n| `ns/djradon/picks/_payload/_default/picks.jsonld `              | [[mesh-resource.node-component.snapshot-distribution.default]]        | paylod dataset distribution  |\n| `ns/djradon/picks/_cfg-op/`                                     | [[mesh-resource.node-component.flow.node-config.operational]]         | operational config series    |\n| `ns/djradon/picks/_cfg-op/2025-11-24_0142_07_v1/`               | [[mesh-resource.node-component.flow-shot.snapshot]]                   | operational config           |\n| `ns/djradon/picks/_cfg-inh/`                                    | [[mesh-resource.node-component.flow.node-config.inheritable]]         | inheritable config series    |\n| `ns/djradon/picks/_cfg-inh/_default/`                           | [[mesh-resource.node-component.flow-shot.default-shot]]               | inheritable config           |\n| `ns/assets/`                                                    | [[mesh-resource.node-component.asset-tree]]                           | collection of assets         |\n| `ns/assets/images/`                                             | asset folder                                                          | - not a sf resource -        |\n| `ns/assets/images/logo.svg`                                     | asset                                                                 | - not a sf resource -        |\n\n\nExample:\n- `ns/` = bare node for organizing content and minting IRIs; refers to itself as a namespace\n- `ns/djradon/` = refers to Dave the person (payload node)\n- `ns/djradon/index.html` = resource page about Dave (content)\n- `ns/djradon/pics/` = refers to Dave's biographical dataset (payload node)\n- `ns/djradon/pics/_payload/` = abstract dataset (DatasetSeries) containing Dave's \"music picks\" data\n- `ns/djradon/pics/_payload/_default/` = current concrete dataset snapshot\n- `ns/djradon/pics/_payload/2025-11-24_0142_07_v1/picks.jsonld` = RDF distribution from version 1\n- `ns/djradon/_assets/images/headshot.jpg` = an image asset; \"attached\" to the mesh, but not a mesh resource\n\n\n\n\n\n#### Naming Resources\n\n- **[[mesh-resource.node-component.flow]]** and their [[mesh-resource.node-component.flow-shot]]\n  - **[[mesh-resource.node-component.flow.node-metadata]]**: System-related administrative and structural metadata for mesh nodes\n  - **[[Version datasets|mesh-resource.node-component.flow-shot.snapshot]]**: Versioned snapshots of datasets\n- **[[working snapshots|mesh-resource.node-component.flow-shot.working-shot]]**: Draft workspaces for ongoing changes to versioned datasets\n- **[[Node handles|mesh-resource.node-component.node-handle]]**: Components that provide referential indirection, allowing references to nodes as mesh resources rather than their referents\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: Collections of arbitrary files and folders attached to the mesh\n\n#### File Resources\n\nTerminal [[mesh resources|mesh-resource]] that cannot contain other resources:\n\n- **[[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]]**: index.html files present in every mesh folder after weaving\n- **[[Distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in various RDF formats\n- **README.md and CHANGELOG.md**: Documentation files providing context\n\n\n## Filesystem Structure\n\nMeshes may be constituted as a set of filesystem [[folder]]s and [[file]]s.\n\n### Folder Mapping\n\n- Mesh nodes correspond physically to [[mesh folders|facet.filesystem.folder]]\n- Folder names become namespace segments and IRI path components\n- The local [[concept.identifier.intramesh]] for a node matches its containing folder name\n\n### File Organization\n\n- [[Datasets|facet.resource.dataset]] are represented by folders containing at least one distribution file\n- Distribution files must be named using the dataset's [[namespace segment|concept.namespace.segment]]\n- Resource pages (index.html) should be present in every mesh folder after [[weaving|concept.weave-process]]\n\n### Reserved Names\n- All system identifiers begin with an underscore (_)\n- Examples: `_assets/`, `_meta/`, `_default`, `_working`\n\n## Logical Structure\n\n### Namespace Extension\n\n- Mesh folders always extend the namespace with a segment corresponding to the folder name\n- This creates a hierarchical IRI structure for addressing resources\n- Each resource has a unique [[Intramesh|concept.identifier.intramesh]] based on its path and local name\n\n### Containment Rules\n\n- **Mesh nodes** are always containers of components (i.e., at least [[mesh-resource.node-component.flow.node-metadata]] and [[mesh-resource.node-component.node-handle]]) and potentially containers of other nodes\n  - **[[bare nodes|mesh-resource.node.bare]]**: no additional containment requirements\n  - **[[reference nodes|mesh-resource.node.reference]]**: must have [[mesh-resource.node-component.flow.reference]]  where the referenced entity can be described\n  - **[[payload nodes|mesh-resource.node.payload]]**: must have [[mesh-resource.node-component.flow.payload]] with at least one distribution\n- **Asset tree components**: Cannot contain nodes\n- all resource folders should contain a [[mesh-resource.node-component.documentation-resource.resource-page]] that makes there IRIs servable/dereferenca\n- \n\n## Rules & Constraints\n\n### System vs User Boundaries\n- **System components**: Generated and managed by the weave process, not intended for user modification\n- **User components**: Directly modifiable by users ([[mesh-resource.node-component.flow-shot.default-shot]], README.md, CHANGELOG.md)\n- The weave process maintains system components and generates missing required flows\n\n### Versioning Requirements\n- flow versioning is managed through the [[Flow Version|concept.flow-version]] system\n  - turning versioning on and off is controlled in the [[mesh-resource.node-component.node-config-defaults]]\n  - Version history is realized in [[mesh-resource.node-component.flow-shot.snapshot]] with numbered snapshots\n  - Version history metadata is kept in the node's [[mesh-resource.node-component.flow.node-metadata]]\n\n### Addressing Requirements\n- Every mesh resource must be addressable via its IRI path\n- IRIs must return meaningful content when dereferenced\n  - [[mesh-resource.node-component.documentation-resource.resource-page]] provide human-readable information for [[facet.filesystem.folder]]-based resources\n    - resource pages are always index.html files generated by \"on weave\" from the [[mesh-resource.node-component.documentation-resource.changelog]] and [[mesh-resource.node-component.documentation-resource.readme]] [[mesh-resource.node-component.documentation-resource]], templates in [[mesh-resource.node-component.asset-tree]] and any scoped template mappings specified in [[mesh-resource.node-component.node-config-defaults]] files \n  - [[facet.filesystem.file]]\n\n## Integration Points\n\n### Weave Process\nThe [[Weave Process|concept.weave-process]] process maintains mesh integrity by:\n- Checking for required system resources and creating them if missing\n- Generating resource pages for changed resources\n- Managing dataset versioning and metadata\n- Ensuring all resources remain addressable and dereferenceable\n\n### Publishing Workflow\n- Meshes are designed to be served directly as static sites\n- GitHub Pages integration allows immediate publishing after repository updates\n- No static site generator required, though resource page generation occurs during weaving\n- The repository structure directly maps to the published IRI structure\n\n### Dataset Integration\nMeshes support multiple RDF formats and follow [[DCAT v3|related-topics.dcat.vocabulary]] standards for dataset organization. [[Datasets|facet.resource.dataset]] within meshes include both standalone datasets and those embedded as node components.\n","n":0.031}}},{"i":161,"$":{"0":{"v":"submesh","n":1},"1":{"v":"\nMeshes always have a [[concept.root-node]]. Any \n","n":0.378}}},{"i":162,"$":{"0":{"v":"loose mesh","n":0.707},"1":{"v":"\nA **loose mesh** has not yet been woven, and may have any number of:\n\n- [[concept.identifier]]s without corresponding [[mesh-resource]]\n- [[mesh-resource.node-component.flow-shot.working]] that differ from [[mesh-resource.node-component.flow-shot.default-shot]]\n- missing [[facet.system]] [[files|file]] or [[folder]]\n","n":0.189}}},{"i":163,"$":{"0":{"v":"embedded mesh","n":0.707},"1":{"v":"\nAn **embedded mesh** is a mesh whose [[concept.root-node]] is not located at the top of a repo.\n\nIt may be contained in a repo that it shares with other files. Or maybe it doesn't live in a repo at all.\n\n","n":0.16}}},{"i":164,"$":{"0":{"v":"Assets","n":1},"1":{"v":"\nEverything file in an [[folder._assets]] (or its subfolders) is considered an asset.\n\nEven though they will have a IRI in the [[concept.implied-rdf-base]], they are not considered \"mesh resources\".\n\n## Special Assets\n\nPrefixed with an underscore, these assets have a special role to play in the \"weave process\"\n\n\n### _weave-config.jsonld\n\n","n":0.149}}},{"i":165,"$":{"0":{"v":"mesh location","n":0.707},"1":{"v":"\nA mesh can be represented by a bunch of files and folders in a filesystem. But it is primarily a concept.\n\nConceptually, a mesh is composed of [[mesh-resource]]s. Those resources all have a **mesh location**: conceptual addresses that may be specified relative to each other ([[concept.identifier.intramesh.relative]]) or from the [[root node|concept.root-node]]. \n\nMesh locations map cleanly to filesystem paths, and they work \n\n\n","n":0.128}}},{"i":166,"$":{"0":{"v":"IRI","n":1},"1":{"v":"\nOn the semantic web, resources (things you might want to talk about with RDF statements) are identified with Internationalized Resource Identifiers. Traditionally, IRIs don't necessarily LOCATE a resource, i.e., if you put them in a web browser, they don't necessarily return content.\n\nBut with [[Semantic Flow sites|concept.semantic-flow-site]], all [[mesh-resource]] IRIs return a web page. \n\n## Types of IRIs\n\n- Absolute IRIs\n- Relative IRIs\n  - Relative-Path Relative IRIs\n  - Absolute-Path Relative IRIs \n\nSee [[faq.reference-iri-choices]] for a discussion of IRI types. \n\n","n":0.113}}},{"i":167,"$":{"0":{"v":"implied rdf base","n":0.577},"1":{"v":"\n## Overview\n\n[[mesh-resource.node-component.snapshot-distribution]]s have an **implied namespace base**, essentially the absolute IRI that corresponds to the [[concept.publication]] URL (without the filename).\n\n## RDF Bases\n\nAn RDF BASE declaration is a directive within an RDF document that establishes a document's base IRI, defining a default location to which all relative IRIs within that document will be resolved.\n\nIf the base isn't provided, applications are supposed to fall back to the IRI used to retrieve the document. (See RFC 3986 Section 5.1.3: \"[Base URI from the Retrieval URI](https://datatracker.ietf.org/doc/html/rfc3986#section-5.1.3)\").\n\nTo keep meshes transposable, Semantic Flow relies on this implicit basing to give meshes [[principle.transposability.host]].\n","n":0.102}}},{"i":168,"$":{"0":{"v":"Immutability","n":1},"1":{"v":"\nImmutable data provides fundamental guarantees that enable reliable, distributed, and concurrent systems. But immutability clashes with real-world needs like privacy and security. That's why Semantic Flow embraces [[principle.pseudo-immutability]].\n\n\n\n- [[mesh-resource.node-component.flow-shot.snapshot]] (e.g. in [[folder.snapshot]]) should be usually be treated as immutable. \n  - Therefore, if you need to refer to a flow \"as is\", you should refer to its corresponding snapshot version.\n  - TODO: examples\n- sometimes, e.g., for compliance reasons, you have to modify or hard-delete some data. \n\n\n## References\n\nhttps://s11.no/2013/prov/resources-that-change-state/\n","n":0.113}}},{"i":169,"$":{"0":{"v":"identifier","n":1},"1":{"v":"\nSemantic Flow resources can be identified via two types of identifiers:\n\n- [[concept.identifier.external]]\n- [[concept.identifier.intramesh]]\n\n\n## Guidance\n\n- Prefer relative or site-root-absolute paths inside the mesh; do not hardcode full base IRIs so the mesh remains portable across hosting locations (see [[faq.reference-iri-choices]]).\n\n\n## Identifier Senses\n\n### Content Identifiers\n\nIdentifiers that [[denote|concept.denotation]] **concrete information resources** (files on disk or over HTTP):\n\n* **Distributions** → materialized datasets, e.g. `test.ttl`, `djradon.jsonld`, etc.\n* **Resource pages** → e.g. `index.html`\n* **Other documentation resources** → e.g. `README.md`, `CHANGELOG.md`\n\nThese are *retrievable representations* (materialized content), i.e. when dereferenced with a request to a [[concept.semantic-flow-site]], the content itself is returned.\n\n### Concept Identifiers\n\nIdentifiers that refer to **concepts, entities, or abstract things**, including:\n\n* **bare node identifiers** → Organizational containers\n* **reference node identifiers** → denotational \n* **payload node identifiers** → Concepts that are datasets\n* **Abstract flow identifiers** → Dataset-as-persistent-concept\n* **Concrete dataset identifiers** → Specific dataset snapshots\n* **Handle identifiers** → Mesh node themselves\n\nWhen dereferenced with a request to a [[concept.semantic-flow-site]], concept identifiers return content, but they still [[concept.denotation]] a concept.\n\n\n## Identifier Pattern Semantics\n\n| Identifier Type    | Trailing Slash? | Refers to…                    | Example                                 |\n| ------------------ | --------------- | ----------------------------- | --------------------------------------- |\n| Content identifier | No              | A fetchable document or asset | `https://example.org/ns/foo/index.html` |\n| Concept IRI        | Yes (`/`)       | A real-world or mesh concept  | `https://example.org/ns/foo/`           |\n\nEven though you might be tempted to think of datasets as concrete things, the IRIs for payload nodes, flows, and snapshots all refer to concepts, i.e., **non-retrievable entities**. Only Distribution IRIs refer to downloadable data, i.e., dataset distributions.\n\n\n","n":0.064}}},{"i":170,"$":{"0":{"v":"intramesh identifier","n":0.707},"1":{"v":"\nAn **intramesh identifier** is a esentially a relative [[concept.iri]] (i.e., without the scheme, e.g., https:// or file://) except that they should correspond to an existing [[mesh-resource]], i.e. [[principle.dereferencability-for-humans]]\n\nThere are two types of intramesh identifiers: [[concept.identifier.intramesh.relative]] and [[concept.identifier.intramesh.absolute]]\n\n## Syntax\n\nLike IRIs:\n\n  * Written without a scheme (e.g., no `https://` or `file://`).\n  * May use path segments (`../`, `/foo/bar`) \n  * May use fragment identifiers (`#`)\n\nUnlike IRIs:\n\n  * **Must not** contain queries (`?`).\n  * \n\n## Semantics\n\n- Same as IRIs in RDF: they [[denote|concept.denotation]] things (aka [[concept.referent]]) which may be mesh resources or “things in the world”.\n  - Fragments behave the same as in IRIs: they refine the primary denotation.\n\n## Purpose\n\n- locate [[mesh-resource]]s (when used in a filesystem or web site context)\n- denote a [[concept.referent]], either [[internal|facet.internal]] or [[external|facet.external]]\n\n## Identifier Name Limitations for Users\n\n- initial underscores prefix all [[facet.system]] identifiers and should be avoided in general for [[facet.user]] identifiers\n\n## Distribution Relativity\n\n\n\n### Examples\n\nNode self-reference: `\"../../../my-dataset\"`  \nOther flows: `\"../../_cfg-op/_current/config.trig\"`, `\"../../_cfg-inh/_current/config.trig\"`, `\"../../_dataset-flow/_current/data.jsonld\"`  \nComponents in other flows: Same pattern, just different flow names\n","n":0.078}}},{"i":171,"$":{"0":{"v":"relative identifier","n":0.707},"1":{"v":"\nIntramesh relative identifiers (or **relative identifiers** for short) are relative IRIs that do not start with a slash (`/`), i.e., they are **relative-path relative IRIs** that correspond to a [[concept.mesh-location]] and denote an [[internal|facet.internal]] or [[external|facet.external]] referent..\n\nRelative identifiers are always resolved relative to the distribution file that contains them.\n\nSee [[faq.reference-iri-choices]] for discussion and examples.\n","n":0.136}}},{"i":172,"$":{"0":{"v":"absolute","n":1}}},{"i":173,"$":{"0":{"v":"external identifier","n":0.707},"1":{"v":"\n**External identifiers** are [[concept.iri]]\n","n":0.5}}},{"i":174,"$":{"0":{"v":"Hosting","n":1}}},{"i":175,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n\n## Github Pages and Github Repos\n\n- can either be \"username/org pages repository\"\" (which automatically hosts content at the namesake IRI, so maybe call it a namesake repository, e.g. djradon.github.io) or 2nd-level (corresponding to an owned repo)\n- under \"classic\" github pages (i.e., \"deploy from a branch\"), you can either use the whole repo, or just the docs folder as the source\n  - Semantic Flow works seamlessly using the entire folder for[[concept.single-mesh-repo]].\n  - for [[concept.mesh.embedded]] repos, a build step will be needed to copy the mesh folders and files into \"docs\"\n  \n## Questions\n\n- can you include a mesh in an existing repo?\n  - Sure! That's an [[concept.mesh.embedded]].\n  - but when composing a mesh from existing meshes, (i.e., linking a repo into an existing mesh) you need an unbroken chain, so embedding a non-[[concept.single-mesh-repo]] might be more difficult, e.g. requiring sparse checkouts. \n\n\n## 1. Git Constraints\n\n### 1.1 Weave does not interact with Git\n\nWeave is a pure filesystem operation. It:\n\n- reads current FlowShots (especially each Flow’s `_working/` directory),\n- validates and processes them,\n- writes new snapshot directories and `_default/` content,\n- updates `_working/` to match `_default/` after a successful weave.\n\nWeave **does not**:\n\n- create commits,\n- stage files,\n- modify any Git metadata.\n\nGit history is independent of weave activity.\n\n---\n\n### 1.2 Weave operates on current FlowShots, not commit state\n\nWeave consumes the **current contents of the mesh’s FlowShots in the working\ntree** (primarily `_working/` for each Flow), regardless of whether those files\nare committed to Git.\n\nTherefore:\n\n- The working tree **does not need to be clean**.\n- You may have modified, staged, or untracked files.\n- What matters is whether the Flow’s `_working/` datasets are in a state the\n  weaver can understand (valid enough to process), not whether they’re committed.\n\nGit commits are optional checkpoints, not prerequisites.\n\n---\n\n### 1.3 Flow ordinality is maintained in RDF metadata and reflected into the filesystem\n\nEach versioned Flow maintains:\n\n- a `sequenceNumber` (monotonic integer per Flow),\n- a weaveLabel (format: `YYYY-MM-DD_HHMM_SS`, e.g. `2025-11-24_0142_07`),\n- snapshot directories combining weaveLabel and sequence number (format: `YYYY-MM-DD_HHMM_SS_vN`, e.g., `2025-11-24_0142_07_v1`).\n\nThese are:\n\n- assigned by weave, based on Flow state,\n- independent of the Git commit graph.\n\n---\n\n## 2. Why Git Constraints Exist\n\nSemantic meshes assume that each Flow evolves along **one linear timeline**.\n\nIf Git introduces merges joining divergent histories, you can end up with:\n\n- multiple incompatible \"next\" snapshots for the same Flow,\n- ambiguous or duplicated sequence numbers in snapshot folder names,\n- weaveLabels that no longer line up with a single causal sequence.\n\nSo the Git constraints are solely about preventing Git from fabricating\nhistories that the FlowShot model cannot represent.\n\n---\n\n## 3. Canonical Branch Rules\n\n### 3.1 Canonical branch must remain linear\n\nEach mesh designates one **canonical branch** (typically `main`). For this branch:\n\n- **Fast-forward only**  \n  - No merge commits that join divergent histories.\n- **No overlapping-branch merges**  \n  - Branches may exist, but if they diverge from canonical and canonical advances,\n    reconciliation must use rebase, not merge.\n- **No force-push**  \n  - Canonical history must not be rewritten.\n\nThis preserves a single linear sequence for FlowShot ordinality.\n\n---\n\n### 3.2 Local branches may diverge, but reconciliation is restricted\n\nYou can create arbitrary feature/WIP branches.\n\nIf canonical has moved ahead while your branch diverged:\n\n- You must rebase onto canonical, then re-weave as needed.\n- You must **not** merge back divergent histories into canonical.\n\nIf you want to keep a divergent lineage, you treat that branch (or fork) as\na separate mesh lineage, not something that merges back into the original mesh.\n\nPossible use case: an application evolves a mesh on a branch, and the owner has a chance to approve it before merging. \n\n---\n\n## 4. Weave and Git Interactions\n\n### 4.1 You may commit at any time\n\nAll of the following patterns are valid:\n\n- **A. Weave-first**  \n  `edit → weave → test → commit`\n- **B. Checkpoint-first**  \n  `edit → commit WIP → weave → commit weave outputs`\n- **C. Heavy local WIP**  \n  `edit → weave multiple times → commit occasionally`\n\nWeave always reads the Flow’s `_working/` state in the working tree, regardless of\ncommit status.\n\n---\n\n### 4.2 Pushing requires linear history\n\nYou may push whenever you like, subject to remote branch protection:\n\n- If pushing to canonical fails (non-fast-forward), you must:\n  - `git fetch`,\n  - `git rebase` (or equivalent) onto canonical,\n  - re-weave as needed on top of the updated filesystem state,\n  - then push.\n\nNo merge commits that join two separate weave timelines.\n\n---\n\n### 4.3 Offline work is supported\n\nYou can:\n\n1. clone a mesh,\n2. edit, weave, and commit locally,\n3. later push when back online.\n\nIf canonical advanced while you were offline:\n\n- rebase your local commits onto canonical,\n- reconcile filesystem conflicts,\n- re-weave as needed,\n- then push.\n\nNo merges; only rebased linear histories.\n\n---\n\n## 5. Mesh Integrity Expectations\n\n### 5.1 Snapshots are immutable\n\nOnce a Snapshot directory (e.g. `2025-11-24_0142_07_v3/`) is created:\n\n- its contents must not be edited by hand,\n- any change to the Flow must go through `_working/` and a new weave,\n- Git diffs should only ever see new snapshots added, not old ones mutated.\n\n---\n\n### 5.2 `_default/` mirrors the latest committed dataset state\n\nAfter a successful weave:\n\n- `_default/` must reflect the current authoritative dataset for the Flow:\n  - for versioned flows, it matches the latest Snapshot,\n  - for unversioned flows, it is *the* dataset state.\n- `_working/` is reset from `_default/` to provide a clean starting point.\n\nIf Git rewrites `_default/` or `_working/` via rebase, the next weave will\nnormalize them by recomputing snapshots.\n\n---\n\n### 5.3 No parallel “next snapshots”\n\nTwo different weave runs from divergent Git histories cannot both claim to be\nthe next snapshot with the same sequence number for the same Flow in canonical.\n\n- Rebasing chooses which sequence is \"first.\"\n- The other must be re-woven atop the new canonical state.\n\n---\n\n## 6. Explicitly Allowed\n\n- Uncommitted edits during weave.  \n- Arbitrary local branches and WIP commits.  \n- Editing `_working/` FlowShots directly.  \n- Offline edit + weave + later push.  \n- Rebasing local history onto canonical, followed by re-weave.\n\n---\n\n## 7. Explicitly Forbidden\n\n- Merge commits that combine diverged histories on canonical.\n- Force-push to canonical.\n- Editing Snapshot directories in-place.\n- Any Git operation that makes Flow `sequenceNumber` or snapshot folder names ambiguous.\n- Merging two different weave timelines back into a single canonical line.\n\n---\n\n## 8. Summary\n\nSemantic meshes impose a **small, strict** set of Git rules:\n\n- One linear canonical history (fast-forward only, no force-push).  \n- Divergent work is reconciled via rebase + re-weave, not merge.  \n- Weave operates on FlowShots in the working tree, independent of commit state.  \n- Snapshot ordinality is defined by weave runs along that single timeline.\n\nThese constraints keep filesystem-based weave semantics coherent and ensure that\nevery Flow evolves along a single, interpretable timeline.\n","n":0.031}}},{"i":176,"$":{"0":{"v":"Fragment Identifiers","n":0.707},"1":{"v":"\n## Uses\n\n- [[Provenance|concept.metadata.provenance]] data and similar \"too small to be a node\" use cases\n  - e.g., things need a name, but you don't want to cause an infinitie cascade of naming pieces of metadata that need their own metadata\n- [[concept.named-graphs]]\n- handle-as-mesh-resource references\n","n":0.154}}},{"i":177,"$":{"0":{"v":"Flow Version","n":0.707},"1":{"v":"\nOnly [[mesh-resource.node-component.flow]] may be versioned. But that effectively means everything important can be versioned.\n\nEach version has an ordinal sequence number (kept in metadata as `sflo:sequenceNumber`, and also part of the snapshot folder name as `_vN`). Snapshot folders follow the format `YYYY-MM-DD_HHMM_SS_vN` (e.g., `2025-11-24_0142_07_v1`).\n\nVersioning is controlled in the [[concept.node-config]].\n","n":0.144}}},{"i":178,"$":{"0":{"v":"denotation","n":1},"1":{"v":"\nDenotation is what an [[concept.identifier]] stands for —- the resource it names in your data and reasoning.\n\n## Examples\n\n- <ns/djradon/> might denote the person known as \"dj radon\" (a \"concrete\" referent [[concept.referent]])\n- <ns/djradon/playlists/1996-11-10/> might denote a dataset which specifies the tracks played on a radio show (a more \"abstract\" referent)\n","n":0.143}}},{"i":179,"$":{"0":{"v":"Debasing","n":1},"1":{"v":"\n## Overview\n\n**De-basing** is the process of converting an imported RDF dataset into **mesh-native form** so that all local IRIs resolve relative to a node’s future serving location.  \nIt is part of **namespace adoption**: bringing external content under a Semantic Flow node’s identifier space.\n\nMesh-native files follow the rules described in  \n[[concept.identifier.intramesh.relative]].\n\n## Goals\n\n- Remove any explicit `@base` / `BASE` declarations.  \n- Rewrite local IRIs so that they become **relative IRIs**.  \n- Ensure that the file is valid when parsed with the node’s **effective base IRI** (its final public URL).  \n- Preserve external IRIs unchanged.  \n- Record the transformation in provenance.\n\n## When De-basing Occurs\n\nDe-basing is applied whenever an imported dataset is intended to become **content** of a node—either as:\n- a **PayloadFlow** dataset, or  \n- a **ReferenceFlow** dataset.\n\nDatasets placed in the **assets tree** are *not* de-based.\n\n## Algorithm (Conceptual)\n\nGiven:\n- `oldBase`: the detected namespace prefix of the imported dataset.\n- `nodeBase`: the node’s future HTTP URL (the mesh-native base).\n\nFor each IRI `I` in the dataset:\n1. If `I` begins with `oldBase`:\n   - Replace `I` with a **relative IRI**:  \n     `I_rel = I.removePrefix(oldBase)`  \n2. Else:\n   - Leave `I` unchanged (external reference).\n\nFinally:\n- Remove explicit `@base` from serialization.\n- Ensure **all local IRIs are relative IRIs**.\n- Store provenance documenting:\n  - original base (`oldBase`),\n  - target node base (`nodeBase`),\n  - the de-basing Activity.\n\n## Example\n\nImported:\n\n```turtle\n@base <https://legacy.example.org/ns/foo/> .\n<Bar> a ex:Thing .\n","n":0.068}}},{"i":180,"$":{"0":{"v":"Mesh CRUD","n":0.707},"1":{"v":"\n## Operational Modalities\n\n### Manual Manipulation\n- Pre-built node folder structures with user-editable flows and other components\n- Manual mesh resource creation (nodes; flows, snapshots, distributions and other components)\n- File-system based editing workflows\n- Validation of hand-crafted mesh structures\n\n### API-Driven Node Manipulation\n- Flow-service API endpoints for programmatic node creation\n- Support for root node initialization\n- Flow and other component management via API\n- RESTful mesh resource manipulation\n\n### Dataset Distribution Upload + Extraction\n- Upload mechanisms for payload RDF datasets (.trig, .jsonld, etc.)\n- Automatic named entity extraction from semantic data\n- System-generated reference and dataset nodes\n- Batch processing of semantic data\n- **Limitation**: Cannot handle binary file resources (audio, images, etc.) - only RDF data\n- File resources must be handled via Direct Manual Construction or API-Driven modalities\n","n":0.092}}},{"i":181,"$":{"0":{"v":"Completed Tasks","n":0.707}}},{"i":182,"$":{"0":{"v":"Merge sflo-host and sflo-config Ontologies","n":0.447},"1":{"v":"\n# Merge sflo-host and sflo-config Ontologies\n\n## Prompt\nMerge the sflo-config ontology and the sflo-host ontology, or perhaps just move some classes like ServiceConfig, PlatformServiceConfig.\n\n## TODO\n- [x] Analyze what needs to move from sflo-host to sflo-config\n- [x] Identify questions that need clarification before merge\n- [x] Plan the merge structure\n- [x] Document the planned changes\n- [x] Execute the merge\n\n## Decisions\n\n### Configuration Hierarchy\n- Three levels of config: Platform, Application/Service, and Node\n- Each level can have inheritable config that flows down to lower levels\n- Inheritable config is primarily useful at platform and application levels, but nodes can also provide it\n\n### Class Naming\n- Rename `PlatformServiceConfig` → `PlatformConfig`\n- Rename `ServiceConfig` → `ApplicationConfig`\n- Create new `HostServiceConfig` in sflo-host that extends `ApplicationConfig`\n\n## Merge Plan\n\n### What Moves to sflo-config\n\n#### Classes\n1. **PlatformConfig** (formerly PlatformServiceConfig)\n   - Represents platform-wide defaults\n   - Links to InheritableNodeConfig for platform-level defaults\n   \n2. **ApplicationConfig** (formerly ServiceConfig)\n   - Represents application/service-level configuration\n   - Links to InheritableNodeConfig for application-level overrides\n   \n3. **defaultPlatformConfig** (formerly defaultPlatformServiceConfig)\n   - The canonical platform defaults instance\n   - Contains embedded defaultInheritableNodeConfig\n\n#### Properties to Move\nNone - keep application properties in sflo-host\n\n### What Stays in sflo-host\n\n#### Classes\n1. **HostServiceConfig** (new, extends ApplicationConfig)\n   - Concrete implementation for sflo-host application\n   - Inherits configuration hierarchy capabilities from ApplicationConfig\n   \n2. **LoggingConfig**\n3. **LogChannelConfig**  \n4. **ContainedServicesConfig**\n5. **MeshPath**\n\n#### Properties (on HostServiceConfig)\n- `port`, `host`, `scheme`\n- `meshPaths`\n- `hasLoggingConfig`\n- `hasContainedServices`\n\n### Updated Class Hierarchy\n\n```mermaid\ngraph TD\n    A[ConfigurationClass] --> B[PlatformConfig]\n    A --> C[ApplicationConfig]  \n    A --> D[AbstractNodeConfig]\n    \n    C --> E[HostServiceConfig]\n    D --> F[OperationalNodeConfig]\n    D --> G[InheritableNodeConfig]\n    \n    B -.->|\"hasInheritableNodeConfig\"| G\n    C -.->|\"hasInheritableNodeConfig\"| G\n    H[Node] -.->|\"hasOperationalNodeConfig\"| F\n    H -.->|\"hasInheritableNodeConfig\"| G\n    \n    style B fill:#e1f5fe\n    style C fill:#e1f5fe\n    style E fill:#fff3e0\n    style G fill:#e1f5fe\n    \n    classDef inSfloConfig fill:#e1f5fe\n    classDef inSfloHost fill:#fff3e0\n```\n\nBlue = sflo-config ontology\nOrange = sflo-host ontology\n\n### Reference Updates\n\n#### In sflo-config\n- Update `hasInheritableNodeConfig` domain to include:\n  - `PlatformConfig` (instead of sflo-host:PlatformServiceConfig)\n  - `ApplicationConfig` (instead of sflo-host:ServiceConfig)\n  - `sflo:Handle` (unchanged)\n\n#### In sflo-host  \n- Change references from `sflo-host:ServiceConfig` to `sflo-config:ApplicationConfig`\n- Update HostServiceConfig to extend `sflo-config:ApplicationConfig`\n- Update namespace prefixes to import sflo-config\n\n### Benefits of This Approach\n\n1. **Clean Separation**: Config inheritance hierarchy in one place, application specifics in another\n2. **No Circular Dependencies**: sflo-config doesn't reference sflo-host\n3. **Extensibility**: Other applications can extend ApplicationConfig with their own specifics\n4. **Clear Inheritance Chain**: Platform → Application → Node hierarchy is explicit\n\n## Implementation Summary\n\n### Changes Made to sflo-config\n\n1. **Added new classes:**\n   - `PlatformConfig` - Platform-wide configuration with inheritable defaults\n   - `ApplicationConfig` - Application/service-level configuration\n   \n2. **Added default instance:**\n   - `defaultPlatformConfig` - Canonical platform defaults with embedded `defaultInheritableNodeConfig`\n   \n3. **Updated property domains:**\n   - `hasInheritableNodeConfig` now accepts `PlatformConfig` and `ApplicationConfig` (instead of sflo-host classes)\n\n### Changes Made to sflo-host\n\n1. **Removed classes:**\n   - `ServiceConfig` (moved to sflo-config as `ApplicationConfig`)\n   - `PlatformServiceConfig` (moved to sflo-config as `PlatformConfig`)\n   \n2. **Added new class:**\n   - `HostServiceConfig` - Extends `sflo-config:ApplicationConfig` with host-specific properties\n   \n3. **Removed instance:**\n   - `defaultPlatformServiceConfig` (moved to sflo-config as `defaultPlatformConfig`)\n   \n4. **Updated all property domains:**\n   - Changed from `ServiceConfig` to `HostServiceConfig`\n   \n5. **Added ontology import:**\n   - `owl:imports` pointing to sflo-config ontology\n\n### Result\n\nThe configuration hierarchy is now cleanly separated:\n- **sflo-config**: Contains the abstract configuration inheritance model (Platform → Application → Node)\n- **sflo-host**: Contains concrete application-specific implementation (HostServiceConfig extends ApplicationConfig)\n\nThis allows other applications to extend `ApplicationConfig` with their own specific properties while reusing the core configuration inheritance mechanism.\n","n":0.044}}},{"i":183,"$":{"0":{"v":"2025 11 08 Unified Ontologies","n":0.447},"1":{"v":"\n## Decisions\n\n\n## Prompt\n\nWe want to do another round on the unified semantic-flow-ontology.ttl and SHACL and the meta-flow-ontology. Every term IRI should end in a slash ('/') denoting it as a non-file resource. \n\n## TODO\n\n\n","n":0.171}}},{"i":184,"$":{"0":{"v":"2025 11 03 Optimizing for Agents","n":0.408},"1":{"v":"\n## Prompt\n\nI want to optimize Roo Code's use in this project by customizing modes and introducing the concept of a task-level memory-bank\n\nRoo Code must use and help maintain both. \n\n### Project-level files (all in documentation/, and will be part of published site documentation)\n\n#### Add to context for every new task\n\n- [[guide.project-brief]] : Foundation document that points to other files and discusses the memory-bank approach\n- [[guide.product-brief]] : Why this project exists, Problems it solves; what components/applications are included; how it should work, User experience goals\n- [[dev.general-guidance]] : developer/agent info\n- [[dev.memory-bank]] : ground rules for giving agents and humans a git-based shared memory\n- [[guide.status]] : where we're at, project wise; what's currently working; summarizes across \n\n#### Probably helpful on most tasks, need to be rigorously maintained regardless\n\n- [[dev.patterns]] : helpful the Architecture Mode\n- [[dev.dependencies]] : Key dependencies\n- [[dev.debugging]] : useful for QA\n- [[now]] : what's currentlly going on, big-picture\n- [[todo]] : general list of things that need doing; can include links to tasks being groomed but not started, but also that haven't been broken into tasks yet\n- [[progress]] : completed tasks, summarized\n- [[decision-log]] : important project-level decisions\n\n#### Everything else\n\n- all the other files (and there are lots) in documentation/ should get pulled into context as needed, and should periodically be groomed for pithiness, consistency, and currency. Maybe we define a \"skill\" or a \"Documentation Grooming\" Roo Mode.\n\n### Task Working Memory\n\nThis will be used to externalize Roo's short-term memory. \n\nFor each new task (in Roo Code or Cline or whatever), we'll create a new markdown file \"task.YYYY-MM-DD-task-name.md\" that will be used to keep track of the active-context, task-specific TODOs, progress, and potential updates to the general project-level memory-bank. If the task file isn't present yet, Orchestrator Roo should suggest creating it, or give the option to proceed without it.\n\nEvery task should have these second-level headings:\n\n- Prompt\n- TODO\n- Decisions\n\nThe Agent's TODO list should be mirrored into the \"TODO\" section and updated as work progresses.\n\n\n## TODO\n\n- [x] Gather context and understand requirements\n- [x] Create guide.project-brief.md - Foundation document explaining memory bank approach and pointing to other files\n- [x] Create guide.product-brief.md - Product vision, problems solved, components/applications, user experience goals\n- [x] Create guide.status.md - Current project status overview (new \"every task context\" file)\n- [x] Update dev.memory-bank.md - Core rules including \"read ALL memory bank files at start of EVERY task\"\n- [x] Update dev.patterns.md - Document patterns as they emerge (minimal initial content)\n- [x] Update dev.dependencies.md - List key dependencies (Fastify, TypeScript, pnpm, Dendron, etc.)\n- [x] Verify dev.debugging.md has proper structure (already has good content)\n- [x] Update now.md - Current work focus (not dated entries)\n- [x] Update todo.md - General task list structure\n- [x] Update progress.md - Dated entries for completed work\n- [x] Update decision-log.md - Dated entries for important decisions\n- [x] Update Roo Mode custom instructions to reference memory bank files\n- [x] Review all files for inconsistencies and repetitions \n\n\n## Decisions\n\n### Documentation Review Findings (2025-11-05)\n\nA review of all 90 documentation files in `documentation/` revealed several areas for improvement:\n\n1.  **Critical Repetition:** The file [`documentation/product-ideas.hateoas-driven-api-recipes.md`](documentation/product-ideas.hateoas-driven-api-recipes.md) contains its entire content duplicated four times, requiring immediate consolidation.\n2.  **Inconsistencies:** Several core concept files, including [`documentation/concept.mesh.md`](documentation/concept.mesh.md) and [`documentation/mesh-resource.node.md`](documentation/mesh-resource.node.md), inconsistently define the full set of node types (omitting \"reference nodes\"). Provenance documentation also shows conflicting naming conventions for internal identifiers.\n3.  **Clarity/Pithiness:** 18 files are empty or contain minimal content (e.g., [`documentation/concept.hosting.md`](documentation/concept.hosting.md), [`documentation/facet.filesystem.md`](documentation/facet.filesystem.md), and six plugin files), suggesting opportunities for consolidation or expansion to improve clarity and reduce unnecessary file clutter.\n4.  **Broken Links/Currency:** Multiple files, including [`documentation/concept.summary.md`](documentation/concept.summary.md) and [`documentation/concept.immutability.md`](documentation/concept.immutability.md), contain broken internal links or unaddressed `TODO` items, indicating outdated references or incomplete sections.\n\n\n","n":0.041}}}]}
