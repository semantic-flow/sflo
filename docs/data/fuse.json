{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Semantic Flow Documentation","n":0.577},"1":{"v":"\n**Dereferenceable, versioned semantic meshes** will be the foundation for a new era of knowledge graphs.\n\n[[now]] | [[todo]] | [[principle]] | [[dev.contributor.djradon.dev-log]]\n\n## What Is Semantic Flow?\n\n**Semantic Flow** is a framework for managing and publishing resource identifiers, knowledge graphs and other semantic data by leveraging GitHub, Gitlab, and other free static hosting services. It enables a **dereferenceable Semantic Web** where every HTTP IRI returns meaningful content.\n\n[[concept.mesh-repo]] provide storage, [[concept.mesh]] provide resource management and publishing, and [[concept.semantic-flow-site]] support data discovery and explainability.\n\n## Benefits\n\n- own your own self-describing data and data schemas\n- complete version history when you want it\n- reliable persistence\n- truly FAIR (Findable, Accessible, Interoperable, and Reusable)\n\n## Features\n\n- seamlessly integrate other data sources anywhere in your mesh\n- generate and customize mini-sites or single-page applications for nodes in your mesh\n- see [[feature]] for a list of planned features\n","n":0.086}}},{"i":2,"$":{"0":{"v":"Template","n":1}}},{"i":3,"$":{"0":{"v":"Task","n":1},"1":{"v":"\n## Decisions\n\n\n## Prompt\n\n\n## TODO\n\n\n","n":0.5}}},{"i":4,"$":{"0":{"v":"Task","n":1}}},{"i":5,"$":{"0":{"v":"2025 11 07 Web App Not a Plugin","n":0.354},"1":{"v":"\n## Decisions\n\n- Introduce a new top-level `/sflo-web` folder in the monorepo to host a server-rendered web application.\n- Prefer a server-rendered approach (non-SPA) with progressive enhancement, likely using HTMX for partial updates and interactivity.\n- Create an optional `sflo-web-shim` plugin under `plugins/sflo-web-shim` to mount the server-rendered app within `sflo-host` (or proxy to a separately running `sflo-web` service when desired).\n- The shim registers a Fastify route prefix (e.g., `/web`) and serves SSR pages and static assets; it avoids client-side routing assumptions.\n- Separation allows independent development/deployment while maintaining optional integration with `sflo-host`.\n- Note: CLI package path has been updated by the user from `/cli` to `/sflo-cli`.\n## Prompt\n\nsince the web client might be used standalone, I think we need to change the monorepo file layout, adding a top-level /sflo-web folder. The web app should be servable as a stand-alone, static SPA web page. Should we have a sflo-host plugin (e.g. \"sflo-web-shim\") that wraps the sflo-web SPA, so if you're running sflo-host you can optionally serve the web app too? We need to update the memory-bank and create a new product.plugins.sflo-web-shim if you agree. I think modern Vue might be the best architectural choice for the SPA.\n## TODO\n\n- [ ] Design sflo-web server-rendered architecture (Fastify view engine + HTMX partials)\n- [ ] Implement sflo-web-shim plugin for sflo-host to mount SSR routes at `/web` and serve static assets\n- [ ] Update product.plugins.sflo-web-shim documentation (purpose, options, route prefix)\n- [ ] Update product.sflo-web to reflect server-rendered + HTMX approach (remove SPA/Vue references)\n- [ ] Create implementation plan and scaffolding for sflo-web (templates, layouts, HTMX patterns) and shim plugin\n- [ ] Sync memory bank with final decisions by referencing this task file\n\n","n":0.061}}},{"i":6,"$":{"0":{"v":"2025 11 06 Embed Quadstore Comunica","n":0.408},"1":{"v":"\n## Decisions\n\n\n## Prompt\n\nLet's add the latest Quadstore, Comunica, @rdfjs/types, rdf-parse, memory-level, and rdf-data-factory to dependencies, and implement singleton quadstore and single queryEngine for the application's RDF storage and querying needs. \n\n\n## TODO\n\n\n","n":0.177}}},{"i":7,"$":{"0":{"v":"2025 11 04 Shared Logging and Errorhandling","n":0.378},"1":{"v":"\n## Prompt\n# Semantic Flow Logging & Error Handling System Specification\n## Node.js Platform Rewrite\n\n**Version:** 1.0  \n**Date:** November 4, 2025  \n**Status:** Working Specification\n\n---\n\n## Overview\n\nThis specification defines a streamlined, production-ready logging and error handling system for the Semantic Flow platform's Node.js rewrite. It consolidates the best features from the current Deno implementation while addressing identified redundancies and adding improvements for both CLI and service usage.\n\n### Design Goals\n\n1. **Unified Architecture**: Single system supporting both CLI tools and long-running services\n2. **Simplified API**: Eliminate redundant functions and streamline the interface  \n3. **Enhanced Performance**: Optimized for high-throughput service operations\n4. **Better Developer Experience**: Clear patterns, comprehensive documentation, and testing utilities\n5. **Production Ready**: Robust error recovery, monitoring, and observability features\n\n---\n\n## Core Architecture\n\n### Module Structure\n```\nsrc/logging/\nâ”œâ”€â”€ core/\nâ”‚   â”œâ”€â”€ logger.ts              # Main logger implementation\nâ”‚   â”œâ”€â”€ types.ts               # Type definitions and interfaces\nâ”‚   â”œâ”€â”€ formatters.ts          # Message formatting utilities\nâ”‚   â””â”€â”€ context.ts             # Context management and merging\nâ”œâ”€â”€ channels/\nâ”‚   â”œâ”€â”€ console.ts             # Console output channel\nâ”‚   â”œâ”€â”€ file.ts                # File logging with rotation\nâ”‚   â””â”€â”€ monitoring.ts          # External monitoring (Sentry, etc.)\nâ”œâ”€â”€ errors/\nâ”‚   â”œâ”€â”€ capture.ts             # Error capture and logging\nâ”‚   â”œâ”€â”€ recovery.ts            # Error recovery strategies  \nâ”‚   â”œâ”€â”€ types.ts               # Error type definitions\nâ”‚   â””â”€â”€ factory.ts             # Error class factory utilities\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ loader.ts              # Configuration loading and validation\nâ”‚   â”œâ”€â”€ schema.ts              # Configuration schema definitions\nâ”‚   â””â”€â”€ defaults.ts            # Default configurations\nâ”œâ”€â”€ utils/\nâ”‚   â”œâ”€â”€ context.ts             # AsyncLocalStorage context management\nâ”‚   â”œâ”€â”€ performance.ts         # Performance monitoring utilities\nâ”‚   â”œâ”€â”€ testing.ts             # Testing utilities and mocks\nâ”‚   â””â”€â”€ validation.ts          # Input validation helpers\nâ””â”€â”€ index.ts                   # Main exports\n```\n\n---\n\n## Type System\n\n### Core Types\n\n```typescript\n// Enhanced log levels with numeric values for easy comparison\nexport enum LogLevel {\n  TRACE = 0,\n  DEBUG = 10,\n  INFO = 20,\n  WARN = 30,\n  ERROR = 40,\n  FATAL = 50\n}\n\n// String literals for configuration mapping\nexport const LogLevelStrings = ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] as const;\nexport type LogLevelString = typeof LogLevelStrings[number];\n\n// Core log entry structure used throughout the system\nexport interface LogEntry {\n  timestamp: number;                 // Date.now()\n  level: LogLevel;\n  message: string;\n  context?: LogContext;\n  error?: {\n    name: string;\n    message: string;\n    stack?: string[];\n    code?: string;\n  };\n  service: { \n    name: string; \n    version?: string; \n    instanceId?: string \n  };\n  pid: number;\n  hostname?: string;\n}\n\n// Comprehensive but streamlined log context\nexport interface LogContext {\n  // Core identification\n  operation?: string;\n  operationId?: string;\n  component?: string;\n  \n  // Semantic Flow specific context\n  meshId?: string;\n  nodeId?: string;\n  meshName?: string;\n  nodeName?: string;\n  \n  // Performance tracking\n  startTime?: number;\n  duration?: number;\n  memoryUsage?: number;\n  \n  // Request context (for service mode)\n  requestId?: string;\n  userId?: string;\n  sessionId?: string;\n  \n  // Error context\n  errorCode?: string;\n  errorType?: string;\n  stackTrace?: string[];\n  errorCause?: unknown;\n  \n  // Flexible metadata\n  tags?: Record<string, string>;\n  metadata?: Record<string, unknown>;\n}\n\n// Error capture options (for logging/reporting)\nexport interface ErrorCaptureOptions {\n  message?: string;\n  context?: LogContext;\n  logLevel?: LogLevel;\n  includeStackTrace?: boolean;\n  reportToMonitoring?: boolean;\n}\n\n// Error recovery options (for control flow)\nexport interface ErrorRecoveryOptions<T = unknown> {\n  strategy: ErrorRecoveryStrategy;\n  fallbackValue?: T;\n  retryCount?: number;\n  retryDelay?: number;\n  shouldRetry?: (error: unknown, attempt: number) => boolean;\n}\n\n// Base log channel interface for extensibility\nexport interface LogChannel {\n  write(entry: LogEntry): Promise<void> | void;\n  flush(): Promise<void>;\n  close(): Promise<void>;\n  readonly minLevel: LogLevel;\n}\n\n// Channel configuration\nexport interface ChannelConfig {\n  enabled: boolean;\n  level: LogLevel;\n  format: 'json' | 'pretty' | 'compact';\n  \n  // Channel-specific options\n  console?: {\n    colors?: boolean;\n    timestamps?: boolean;\n  };\n  file?: {\n    path?: string;\n    maxSize?: number;\n    maxFiles?: number;\n    rotationStrategy?: 'time' | 'size';\n  };\n  monitoring?: {\n    provider: 'sentry' | 'datadog' | 'newrelic';\n    dsn?: string;\n    environment?: string;\n    sampleRate?: number;\n  };\n}\n\n// Main logger configuration\nexport interface LoggerConfig {\n  serviceName: string;\n  serviceVersion: string;\n  environment: 'development' | 'staging' | 'production';\n  instanceId?: string;\n  \n  // Channel configurations\n  console: ChannelConfig;\n  file: ChannelConfig;\n  monitoring: ChannelConfig;\n  \n  // Performance settings\n  async: boolean;        // true: buffered writes + sync stderr for ERROR/FATAL\n                        // false: synchronous writes where possible\n  bufferSize: number;\n  flushInterval: number;\n  \n  // Context settings\n  autoContext: {\n    includeTimestamp: boolean;\n    includeHostname: boolean;\n    includeProcessInfo: boolean;\n  };\n}\n```\n\n### Error Types\n\n```typescript\n// Base error class with enhanced context\nexport class SemanticFlowError extends Error {\n  public readonly code: string;\n  public readonly context: Record<string, unknown>;\n  public readonly timestamp: Date;\n  public readonly recoverable: boolean;\n  \n  constructor(\n    message: string,\n    code: string,\n    context: Record<string, unknown> = {},\n    recoverable = true\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n    this.code = code;\n    this.context = context;\n    this.timestamp = new Date();\n    this.recoverable = recoverable;\n  }\n}\n\n// Error factory to reduce boilerplate\nexport function createErrorType(\n  name: string,\n  code: string,\n  recoverable = true\n): new (message: string, context?: Record<string, unknown>) => SemanticFlowError {\n  return class extends SemanticFlowError {\n    constructor(message: string, context: Record<string, unknown> = {}) {\n      super(message, code, context, recoverable);\n      this.name = name;\n    }\n  };\n}\n\n// Specific error types using factory\nexport const ValidationError = createErrorType('ValidationError', 'VALIDATION_ERROR');\nexport const ConfigurationError = createErrorType('ConfigurationError', 'CONFIG_ERROR', false);\nexport const MeshProcessingError = createErrorType('MeshProcessingError', 'MESH_PROCESSING_ERROR');\n\nexport class ApiError extends SemanticFlowError {\n  public readonly statusCode: number;\n  \n  constructor(message: string, statusCode: number, context?: Record<string, unknown>) {\n    super(message, 'API_ERROR', context);\n    this.statusCode = statusCode;\n  }\n}\n```\n\n---\n\n## Core Logger Interface\n\n### Simplified Logger API\n\n```typescript\nexport interface Logger {\n  // Core logging methods (synchronous with async buffering)\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  // Context management (returns immutable context wrappers)\n  withContext(context: LogContext): Logger;\n  withOperation(operation: string, operationId?: string): Logger;\n  withComponent(component: string): Logger;\n  child(context: LogContext): Logger; // Alias for withContext\n  \n  // Performance tracking\n  startTimer(operation: string): Timer;\n  \n  // Error capture (for logging/reporting only)\n  captureError(error: unknown, options?: ErrorCaptureOptions): void;\n  \n  // Lifecycle management\n  flush(): Promise<void>;\n  close(): Promise<void>;\n}\n\n// Child logger semantics documentation:\n// - child() returns a thin wrapper sharing transports and buffer with parent\n// - context is frozen and shallow-merged per call\n// - no mutation or context bleed between child instances\n\nexport interface Timer {\n  end(context?: LogContext): void;\n  checkpoint(label: string, context?: LogContext): void;\n}\n```\n\n### Factory Functions\n\n```typescript\n// Singleton pattern with dependency injection support\nexport function initLogger(config?: Partial<LoggerConfig>): Logger;\nexport function getLogger(): Logger;\nexport function __resetLoggerForTests(): void;\n\n// Factory functions for initialization\nexport function createLogger(config?: Partial<LoggerConfig>): Logger;\nexport function createCliLogger(options?: {\n  verbose?: boolean;\n  quiet?: boolean;\n  format?: 'pretty' | 'json';\n}): Logger;\nexport function createServiceLogger(serviceName: string, options?: {\n  enableFileLogging?: boolean;\n  enableMonitoring?: boolean;\n  environment?: string;\n}): Logger;\n\n// Component-scoped logger (pure ESM)\nexport function getComponentLogger(sourceUrl: string /* import.meta.url */): Logger {\n  const file = new URL(sourceUrl);\n  const base = file.pathname.split(\"/\").pop() ?? \"unknown\";\n  const component = base.replace(/\\.(m|c)?js|ts$/, \"\");\n  return getLogger().child({ component });\n}\n```\n\n---\n\n## Separated Error Handling\n\n### Error Capture (Logging/Reporting)\n\n```typescript\n// Pure error capture - only logs and reports, no control flow\nexport function captureError(error: unknown, options: ErrorCaptureOptions = {}): void {\n  // Synchronous logging for ERROR/FATAL levels\n  // Async buffering for full reporting\n}\n```\n\n### Error Recovery (Control Flow)\n\n```typescript\n// Error recovery strategies (simplified)\nexport enum ErrorRecoveryStrategy {\n  CONTINUE = 'continue',        // Don't throw, continue execution\n  RETHROW = 'rethrow',         // Log then rethrow (default)\n  FALLBACK = 'fallback',       // Return fallback value\n  RETRY = 'retry'              // Retry operation with backoff\n}\n\n// Apply recovery strategy to an error\nexport async function applyErrorRecovery<T>(\n  error: unknown,\n  options: ErrorRecoveryOptions<T>\n): Promise<T | never> {\n  // Implementation handles retry logic, fallbacks, etc.\n}\n\n// Convenience wrapper combining capture + recovery\nexport async function withErrorHandling<T>(\n  operation: () => Promise<T>,\n  options: {\n    capture?: ErrorCaptureOptions;\n    recovery?: ErrorRecoveryOptions<T>;\n  } = {}\n): Promise<T | undefined> {\n  try {\n    return await operation();\n  } catch (error) {\n    if (options.capture) {\n      captureError(error, options.capture);\n    }\n    if (options.recovery) {\n      return await applyErrorRecovery(error, options.recovery);\n    }\n    throw error; // Default: rethrow\n  }\n}\n```\n\n### Error Classification\n\n```typescript\nexport class ErrorClassifier {\n  static classify(error: unknown): {\n    type: string;\n    severity: LogLevel;\n    recoverable: boolean;\n    category: 'system' | 'business' | 'validation' | 'network' | 'unknown';\n  };\n  \n  static shouldReport(error: unknown, threshold: LogLevel): boolean;\n  static extractContext(error: unknown): Record<string, unknown>;\n}\n```\n\n---\n\n## Channel Implementations\n\n### Console Channel\n\n```typescript\nexport class ConsoleChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  \n  constructor(private config: ChannelConfig['console']) {\n    this.minLevel = config.level;\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Always synchronous for console - push async work to buffer\n    if (entry.level >= LogLevel.ERROR) {\n      this.writeSynchronous(entry);\n    } else {\n      this.writeStandard(entry);\n    }\n  }\n  \n  flush(): Promise<void> {\n    // Console doesn't buffer, so flush is a no-op\n    return Promise.resolve();\n  }\n  \n  close(): Promise<void> {\n    return Promise.resolve();\n  }\n  \n  // Synchronous critical path for errors\n  private writeSynchronous(entry: LogEntry): void {\n    const line = this.formatCritical(entry);\n    try {\n      process.stderr.write(line);\n    } catch {\n      // Best effort - never throw from logging\n    }\n  }\n  \n  // Standard output for non-critical levels\n  private writeStandard(entry: LogEntry): void {\n    const line = process.stdout.isTTY \n      ? this.formatForTTY(entry) \n      : this.formatForPipe(entry);\n    try {\n      process.stdout.write(line);\n    } catch {\n      // Best effort - never throw from logging\n    }\n  }\n  \n  // Smart formatting based on environment\n  private formatForTTY(entry: LogEntry): string;\n  private formatForPipe(entry: LogEntry): string;\n  private formatCritical(entry: LogEntry): string;\n}\n```\n\n### File Channel\n\n```typescript\nexport class FileChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  private buffer: LogEntry[] = [];\n  private flushTimer: NodeJS.Timer | null = null;\n  \n  constructor(private config: ChannelConfig['file']) {\n    this.minLevel = config.level;\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Always async for file channel - add to buffer\n    this.buffer.push(entry);\n    this.scheduleFlush();\n  }\n  \n  // Single-flight flush to prevent concurrent flushes\n  private inflight?: Promise<void>;\n  \n  flush(): Promise<void> {\n    return this.inflight ?? (this.inflight = this.flushImpl().finally(() => this.inflight = undefined));\n  }\n  \n  close(): Promise<void> {\n    if (this.flushTimer) {\n      clearTimeout(this.flushTimer);\n      this.flushTimer = null;\n    }\n    return this.flushBuffer();\n  }\n  \n  // Enhanced rotation with compression and atomic operations\n  private rotateIfNeeded(): Promise<void>; // Uses fs.rename for atomicity\n  private compressOldLogs(): Promise<void>;\n  private scheduleFlush(): void;\n  private flushImpl(): Promise<void>; // Uses fs.writev for batched writes\n  \n  // File opened with O_APPEND for safe concurrent writes\n  // Reopen file descriptor after rotation\n}\n```\n\n### Monitoring Channel\n\n```typescript\nexport class MonitoringChannel implements LogChannel {\n  public readonly minLevel: LogLevel;\n  private buffer: LogEntry[] = [];\n  private rateLimiter: RateLimiter;\n  \n  constructor(private config: ChannelConfig['monitoring']) {\n    this.minLevel = config.level;\n    this.rateLimiter = new RateLimiter(config.sampleRate || 1.0);\n  }\n  \n  write(entry: LogEntry): void {\n    // Guard against entries below minimum level\n    if (entry.level < this.minLevel) return;\n    \n    // Apply sampling and rate limiting\n    if (!this.shouldSample(entry)) return;\n    \n    // Always async for monitoring - add to buffer with timeout protection\n    this.buffer.push(entry);\n    this.scheduleFlush();\n  }\n  \n  // Single-flight flush to prevent concurrent monitoring flushes\n  private inflight?: Promise<void>;\n  \n  flush(): Promise<void> {\n    return this.inflight ?? (this.inflight = this.flushImpl().finally(() => this.inflight = undefined));\n  }\n  \n  close(): Promise<void> {\n    return this.flushWithTimeout();\n  }\n  \n  // Provider-specific implementations\n  private sentryAdapter: SentryAdapter;\n  private datadogAdapter?: DatadogAdapter;\n  \n  // Smart sampling and rate limiting with timeout protection\n  private shouldSample(entry: LogEntry): boolean;\n  private scheduleFlush(): void;\n  private flushImpl(): Promise<void>; // Bounds batch size and applies per-entry deadlines\n  \n  // Drop counter for monitoring timeouts/failures\n  private droppedCount = 0;\n  public getDroppedCount(): number { return this.droppedCount; }\n}\n\n// Missing primitive definitions\nclass RateLimiter {\n  constructor(private rate: number) {}\n  allow(): boolean { return Math.random() < this.rate; }\n}\n\ninterface SentryAdapter {\n  send(entry: LogEntry): Promise<void>;\n}\n\ninterface DatadogAdapter {\n  send(entry: LogEntry): Promise<void>;\n}\n```\n\n---\n\n## Configuration System\n\n### Async Flag Semantics\n\nThe `async` configuration flag controls write behavior:\n\n- **`async: true`** (default): Non-blocking buffered writes for all channels. ERROR/FATAL levels also emit synchronously to `process.stderr` for immediate visibility.\n- **`async: false`**: Console and file channels use `writeSync` on ERROR/FATAL and best-effort synchronous writes for other levels. Monitoring channel remains buffered with best-effort flush. **Warning**: Synchronous I/O can impact performance significantly under load.\n\n### Schema-Based Configuration\n\n```typescript\n// JSON Schema for logger configuration\nexport const LoggerConfigSchema = {\n  type: 'object',\n  properties: {\n    serviceName: { type: 'string', minLength: 1 },\n    serviceVersion: { type: 'string' },\n    environment: { \n      type: 'string', \n      enum: ['development', 'staging', 'production'] \n    },\n    console: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: true },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        format: { type: 'string', enum: ['json', 'pretty', 'compact'] }\n      }\n    },\n    file: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: false },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        format: { type: 'string', enum: ['json', 'pretty', 'compact'] },\n        path: { type: 'string' },\n        maxSize: { type: 'number', minimum: 1024 },\n        maxFiles: { type: 'number', minimum: 1 },\n        rotationStrategy: { type: 'string', enum: ['time', 'size'] }\n      }\n    },\n    monitoring: {\n      type: 'object',\n      properties: {\n        enabled: { type: 'boolean', default: false },\n        level: { type: 'string', enum: ['trace', 'debug', 'info', 'warn', 'error', 'fatal'] },\n        provider: { type: 'string', enum: ['sentry', 'datadog', 'newrelic'] },\n        dsn: { type: 'string' },\n        environment: { type: 'string' },\n        sampleRate: { type: 'number', minimum: 0, maximum: 1 }\n      }\n    }\n  },\n  required: ['serviceName']\n};\n\n// Configuration loader with multiple sources\nexport class ConfigLoader {\n  static load(sources: {\n    defaults?: Partial<LoggerConfig>;\n    configFile?: string;\n    environment?: Record<string, string>;\n    cliArgs?: Record<string, unknown>;\n  }): LoggerConfig;\n  \n  static validate(config: unknown): LoggerConfig;\n  static merge(...configs: Partial<LoggerConfig>[]): LoggerConfig;\n  \n  // Map string level names to enum values\n  static parseLogLevel(level: string): LogLevel {\n    const index = LogLevelStrings.indexOf(level as LogLevelString);\n    return index >= 0 ? (index * 10) as LogLevel : LogLevel.INFO;\n  }\n}\n```\n\n### Environment Variable Mapping\n\n```bash\n# Service identification\nSF_SERVICE_NAME=semantic-flow-service\nSF_SERVICE_VERSION=2.0.0\nSF_ENVIRONMENT=production\n\n# Console logging\nSF_LOG_CONSOLE_ENABLED=true\nSF_LOG_CONSOLE_LEVEL=info\nSF_LOG_CONSOLE_FORMAT=pretty\n\n# File logging  \nSF_LOG_FILE_ENABLED=true\nSF_LOG_FILE_PATH=./logs/sf-service.log\nSF_LOG_FILE_LEVEL=debug\nSF_LOG_FILE_MAX_SIZE=10485760\nSF_LOG_FILE_MAX_FILES=5\n\n# Monitoring\nSF_LOG_MONITORING_ENABLED=true\nSF_LOG_MONITORING_PROVIDER=sentry\nSF_LOG_MONITORING_DSN=https://...\nSF_LOG_MONITORING_SAMPLE_RATE=0.1\n```\n\n---\n\n## Context Management\n\n### AsyncLocalStorage Integration\n\n```typescript\n// Automatic context propagation using Node.js AsyncLocalStorage\nexport class ContextManager {\n  private static als = new AsyncLocalStorage<LogContext>();\n  \n  static run<T>(context: LogContext, fn: () => T): T {\n    return this.als.run(context, fn);\n  }\n  \n  static runAsync<T>(context: LogContext, fn: () => Promise<T>): Promise<T> {\n    return this.als.run(context, fn);\n  }\n  \n  static current(): LogContext | undefined {\n    return this.als.getStore();\n  }\n  \n  static merge(context: LogContext): LogContext {\n    const current = this.current();\n    return current ? { ...current, ...context } : context;\n  }\n}\n\n// HTTP middleware integration (framework-agnostic example)\nimport { randomUUID } from 'node:crypto';\n\nexport function createRequestLogger(\n  req: { id?: string; method: string; path: string; get(header: string): string | undefined },\n  res: unknown,\n  next: () => void\n) {\n  const requestContext: LogContext = {\n    operation: 'http-request',\n    requestId: req.id || randomUUID(),\n    metadata: {\n      method: req.method,\n      path: req.path,\n      userAgent: req.get('User-Agent')\n    }\n  };\n  \n  ContextManager.run(requestContext, () => {\n    (req as any).logger = getLogger().child({ component: 'http-handler' });\n    next();\n  });\n}\n```\n\n---\n\n## Performance Enhancements\n\n### Async Logging with Buffering\n\n```typescript\nexport class AsyncLogBuffer {\n  private buffer: LogEntry[] = [];\n  private flushTimer: NodeJS.Timer | null = null;\n  \n  constructor(\n    private maxSize: number = 1000,\n    private flushInterval: number = 5000\n  ) {}\n  \n  add(entry: LogEntry): void;\n  flush(): Promise<void>;\n  private autoFlush(): void;\n}\n```\n\n### Performance Monitoring\n\n```typescript\nexport class PerformanceTracker {\n  static trackOperation<T>(\n    operation: string,\n    fn: () => Promise<T>,\n    logger: Logger\n  ): Promise<T>;\n  \n  static createTimer(operation: string, logger: Logger): Timer;\n  \n  // Memory usage tracking\n  static trackMemory(logger: Logger): void;\n  \n  // Log volume metrics  \n  static getLogMetrics(): {\n    totalLogs: number;\n    logsByLevel: Record<LogLevel, number>;\n    errorRate: number;\n    avgResponseTime: number;\n  };\n}\n```\n\n---\n\n## Testing Utilities\n\n### Mock Logger\n\n```typescript\nexport class MockLogger implements Logger {\n  public logs: LogEntry[] = [];\n  public capturedErrors: Array<{ error: unknown; options?: ErrorCaptureOptions }> = [];\n  \n  // Implement all Logger methods with recording\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  captureError(error: unknown, options?: ErrorCaptureOptions): void {\n    this.capturedErrors.push({ error, options });\n    // Also create a log entry for the error\n    const entry: LogEntry = {\n      timestamp: Date.now(),\n      level: options?.logLevel || LogLevel.ERROR,\n      message: options?.message || (error instanceof Error ? error.message : String(error)),\n      context: options?.context,\n      error: error instanceof Error ? {\n        name: error.name,\n        message: error.message,\n        stack: error.stack?.split('\\n'),\n        code: (error as any).code\n      } : undefined,\n      service: { name: 'test-service' },\n      pid: process.pid\n    };\n    this.logs.push(entry);\n  }\n  \n  // Test utilities\n  findLogsByLevel(level: LogLevel): LogEntry[];\n  findLogsByComponent(component: string): LogEntry[];\n  findLogsByOperation(operation: string): LogEntry[];\n  hasErrorWithCode(code: string): boolean;\n  clearLogs(): void {\n    this.logs.length = 0;\n    this.capturedErrors.length = 0;\n  }\n}\n\nexport class LoggerTestUtils {\n  static createMockLogger(): MockLogger;\n  static createTestConfig(): LoggerConfig;\n  static waitForLogs(logger: MockLogger, count: number, timeout?: number): Promise<void>;\n}\n```\n\n### Integration Test Helpers\n\n```typescript\nexport class LoggerIntegrationTests {\n  static async testFileRotation(config: LoggerConfig): Promise<TestResult>;\n  static async testErrorHandling(config: LoggerConfig): Promise<TestResult>;\n  static async testPerformance(config: LoggerConfig): Promise<PerformanceResult>;\n  static async testMonitoringIntegration(config: LoggerConfig): Promise<TestResult>;\n}\n```\n\n---\n\n## Usage Examples\n\n### CLI Tool Usage\n\n```typescript\nimport { createCliLogger, getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({ \n  verbose: process.argv.includes('--verbose'),\n  format: 'pretty' \n});\n\n// Component-scoped logger using import.meta.url\nconst componentLogger = getComponentLogger(import.meta.url);\n\nasync function processFiles(files: string[]) {\n  const timer = componentLogger.startTimer('process-files');\n  \n  try {\n    componentLogger.info(`Processing ${files.length} files`);\n    \n    for (const file of files) {\n      const fileLogger = componentLogger.withContext({ \n        operation: 'process-file',\n        metadata: { filename: file }\n      });\n      \n      await processFile(file, fileLogger);\n    }\n    \n    timer.end({ metadata: { filesProcessed: files.length } });\n  } catch (error) {\n    captureError(error, {\n      message: 'Failed to process files',\n      context: { metadata: { files } },\n      reportToMonitoring: true\n    });\n    throw error;\n  }\n}\n```\n\n### Service Usage\n\n```typescript\nimport { createServiceLogger, getComponentLogger, ContextManager } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('semantic-flow-api', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: process.env.NODE_ENV\n});\n\n// Component-specific logger for this module\nconst componentLogger = getComponentLogger(import.meta.url);\n\napp.use((req, res, next) => {\n  const requestContext = {\n    operation: 'api-request',\n    requestId: req.id,\n    metadata: {\n      method: req.method,\n      path: req.path,\n      userAgent: req.get('User-Agent')\n    }\n  };\n  \n  ContextManager.run(requestContext, () => {\n    req.logger = componentLogger.child({ component: 'http-handler' });\n    next();\n  });\n});\n```\n\n### Error Handling\n\n```typescript\nimport { captureError, withErrorHandling, ErrorRecoveryStrategy } from '@semantic-flow/logging';\n\n// Simple error capture (logging only)\nasync function riskyOperation() {\n  try {\n    await doSomethingRisky();\n  } catch (error) {\n    captureError(error, {\n      message: 'Risky operation failed',\n      context: { operation: 'risky-op' },\n      includeStackTrace: true\n    });\n    // Continue with fallback logic\n  }\n}\n\n// Combined error handling with recovery\nconst result = await withErrorHandling(\n  () => callExternalAPI(),\n  {\n    capture: {\n      context: { operation: 'external-api-call' },\n      reportToMonitoring: true\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.RETRY,\n      retryCount: 3,\n      fallbackValue: null,\n      retryDelay: 1000\n    }\n  }\n);\n\n// AsyncLocalStorage context propagation with ESM\nconst componentLogger = getComponentLogger(import.meta.url);\n\nContextManager.runAsync(\n  { operation: 'batch-process', userId: 'user123' },\n  async () => {\n    // All logging within this scope automatically includes the context\n    componentLogger.info('Starting batch process'); // Automatically includes operation + userId + component\n    \n    await processItems();\n    \n    componentLogger.info('Batch process completed');\n  }\n);\n\n// Dynamic import example for configuration loading\nasync function loadConfig() {\n  try {\n    const configModule = await import('./config.js');\n    return configModule.default;\n  } catch (error) {\n    captureError(error, {\n      message: 'Failed to load configuration module',\n      context: { operation: 'config-load' }\n    });\n    throw error;\n  }\n}\n```\n\n---\n\n## Migration Guide\n\n### From Deno Implementation\n\n1. **Logger Initialization**: Use `initLogger()` at startup, then `getLogger()` or `getComponentLogger(import.meta.url)` anywhere\n2. **Error Handling**: Replace both `handleError` and `handleCaughtError` with `captureError()` + optional recovery\n3. **Configuration**: Use schema-based config with singleton pattern\n4. **Context Management**: Use `AsyncLocalStorage` for automatic context propagation\n5. **Async Methods**: All logging methods are synchronous (with internal async buffering)\n6. **Context Creation**: Use immutable `child()` loggers instead of mutable context updates\n\n### Breaking Changes\n\n- Log levels changed to enum with numeric values\n- Context merging behavior simplified\n- File rotation configuration consolidated\n- Sentry integration moved to generic monitoring channel\n\n---\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure (Week 1-2)\n- [ ] Type definitions and interfaces\n- [ ] Basic logger implementation\n- [ ] Console channel implementation\n- [ ] Configuration system\n- [ ] Unit tests for core functionality\n\n### Phase 2: Advanced Features (Week 3-4)\n- [ ] File channel with rotation\n- [ ] Monitoring channel (Sentry)\n- [ ] Unified error handling\n- [ ] Performance tracking\n- [ ] Integration tests\n\n### Phase 3: CLI/Service Integration (Week 5)\n- [ ] CLI logger factory\n- [ ] Service logger factory  \n- [ ] Context management utilities\n- [ ] Documentation and examples\n\n### Phase 4: Testing & Documentation (Week 6)\n- [ ] Comprehensive test suite\n- [ ] Performance benchmarks\n- [ ] Migration documentation\n- [ ] API documentation\n\n#### Detailed Test Plan\n\n**Core Functionality Tests:**\n- **Deterministic clocks**: Inject `now()` function into logger; use Vitest fake timers for predictable timestamps\n- **Rotation tests**: Force size-based rotation with tiny `maxSize`; assert new file created and file descriptor reopened\n- **Signal tests**: Simulate SIGINT/SIGTERM with child process; assert `flush()` called before exit\n- **No-throw contract**: Intentionally throw inside a channel's `write` method; ensure logger falls back to console and does not crash caller\n- **ESM component detection**: Test `getComponentLogger(import.meta.url)` with various file paths and extensions\n\n**Advanced Feature Tests:**\n- **Monitoring timeouts**: Stub adapter with delayed promise; assert drop counters increment when deadlines exceeded\n- **ALS propagation**: Assert `ContextManager.current()` context merged into log entries across `await` points\n- **Recursive logging protection**: Trigger error within logging code; verify fallback to console.error without infinite loops\n- **Back-pressure handling**: Fill buffers beyond capacity; verify graceful degradation and dropped message counts\n- **Pure ESM integrity**: Verify no `require()` calls in bundled output; test dynamic import usage\n\n---\n\n## Success Criteria\n\n1. **Performance**: 50% faster than current Deno implementation\n2. **Memory Usage**: 30% lower memory footprint\n3. **API Simplicity**: 40% fewer public methods/functions\n4. **Test Coverage**: 95% code coverage\n5. **Documentation**: Complete API docs and usage examples\n6. **Migration**: Clear migration path from Deno version\n\n---\n\n## Production Considerations\n\n### JSON Lines Output Format\n\nAll structured log output follows the JSON Lines format (newline-delimited JSON) for ingestion by log processors like FluentBit, Loki, or Elasticsearch:\n\n```typescript\n// Each log entry is a single JSON object terminated by \\n\n{\"timestamp\":1699027200000,\"level\":20,\"message\":\"Server started\",\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n{\"timestamp\":1699027201000,\"level\":40,\"message\":\"Database connection failed\",\"error\":{\"name\":\"Error\",\"message\":\"Connection timeout\"},\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n```\n\n### Flush Guarantees\n\nThe `flush()` method provides the following guarantees:\n- Drains buffers of all enabled channels within a bounded time (default 5s timeout)\n- Never throws - errors are logged to console as fallback\n- Returns when all pending entries are written or timeout is reached\n- Safe to call multiple times concurrently\n\n### Critical Path Reliability\n\n```typescript\n// Synchronous logging for ERROR/FATAL ensures immediate output\nfunction logSyncCritical(entry: LogEntry): void {\n  const line = JSON.stringify({\n    t: entry.timestamp,\n    lvl: entry.level,\n    msg: entry.message,\n    rid: entry.context?.requestId\n  }) + '\\n';\n  \n  try { \n    process.stderr.write(line); \n  } catch {\n    // Never throw from logging - best effort only\n  }\n  \n  // Optional: Synchronous file write for critical errors\n  // if (criticalFd) fs.writeSync(criticalFd, line);\n}\n\n// Safe JSON serialization with limits and circular protection\nconst MAX_CTX = 20_000, MAX_MSG = 2_000, MAX_STACK = 50;\n\nfunction safeStringify(value: any, maxLength = MAX_CTX): string {\n  try {\n    const seen = new WeakSet();\n    const result = JSON.stringify(value, (key, val) => {\n      if (typeof val === 'object' && val !== null) {\n        if (seen.has(val)) return '[Circular]';\n        seen.add(val);\n      }\n      if (typeof val === 'string' && val.length > MAX_MSG) {\n        return val.slice(0, MAX_MSG) + '...[truncated]';\n      }\n      return val;\n    });\n    return result.length > maxLength ? result.slice(0, maxLength) + '...[truncated]' : result;\n  } catch {\n    return '[Unserializable]';\n  }\n}\n\n// Strip ANSI escape codes for non-TTY output\nfunction stripAnsi(text: string): string {\n  return text.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// PII redaction hook (optional per channel)\ntype RedactFn = (key: string, value: any) => any;\ntype RedactConfig = string[] | RedactFn;\n\nfunction applyRedaction(entry: LogEntry, redact?: RedactConfig): LogEntry {\n  if (!redact) return entry;\n  \n  if (Array.isArray(redact)) {\n    // Field-based redaction\n    const redactFields = new Set(redact);\n    return JSON.parse(JSON.stringify(entry, (key, value) => \n      redactFields.has(key) ? '[REDACTED]' : value\n    ));\n  } else {\n    // Custom redaction function\n    return JSON.parse(JSON.stringify(entry, redact));\n  }\n}\n\n// Ensure JSON lines contain no ANSI codes\nfunction formatJsonLine(entry: LogEntry, redact?: RedactConfig): string {\n  const redacted = applyRedaction(entry, redact);\n  const serialized = safeStringify(redacted);\n  return stripAnsi(serialized) + '\\n';\n}\n```\n\n### Graceful Shutdown\n\n```typescript\nconst SAFE_CLOSE_MS = 2000;\n\n// Comprehensive process termination hooks\nprocess.on('beforeExit', async () => {\n  await getLogger().flush();\n});\n\nprocess.on('SIGTERM', () => void gracefulExit(0));\nprocess.on('SIGINT', () => void gracefulExit(130));\n\nprocess.on('uncaughtException', (error) => {\n  // Sync-log minimal line to stderr first\n  const line = JSON.stringify({\n    t: Date.now(),\n    lvl: LogLevel.FATAL,\n    msg: 'Uncaught exception - process terminating',\n    err: error.message\n  }) + '\\n';\n  try { process.stderr.write(line); } catch {}\n  \n  // Then capture full error context\n  captureError(error, { \n    logLevel: LogLevel.FATAL,\n    message: 'Uncaught exception - process terminating',\n    includeStackTrace: true \n  });\n  void gracefulExit(1);\n});\n\nprocess.on('unhandledRejection', (reason) => {\n  captureError(reason as unknown, { \n    logLevel: LogLevel.ERROR,\n    message: 'Unhandled promise rejection',\n    includeStackTrace: true \n  });\n});\n\nfunction safeGetLogger(): Logger | undefined {\n  try {\n    return getLogger();\n  } catch {\n    return undefined;\n  }\n}\n\nasync function gracefulExit(code: number) {\n  try {\n    const logger = safeGetLogger();\n    if (logger) {\n      // Race between proper close and timeout\n      await Promise.race([\n        logger.close(),\n        new Promise(resolve => setTimeout(resolve, SAFE_CLOSE_MS))\n      ]);\n    }\n  } catch {\n    // Best effort - don't block exit\n  } finally {\n    process.exit(code);\n  }\n}\n```\n\n### Guard Against Recursive Logging\n\n```typescript\n// Use AsyncLocalStorage to prevent recursive logging per async context\nconst recursionGuard = new AsyncLocalStorage<boolean>();\n\nclass LoggerImpl implements Logger {\n  private log(level: LogLevel, message: string, error?: Error, context?: LogContext): void {\n    if (recursionGuard.getStore()) {\n      // Prevent infinite loops in error handling - use direct console\n      console.error('Recursive logging detected:', message);\n      return;\n    }\n    \n    recursionGuard.run(true, () => {\n      this.writeToChannels(level, message, error, context);\n    });\n  }\n  \n  trace(message: string, context?: LogContext): void {\n    this.log(LogLevel.TRACE, message, undefined, context);\n  }\n  \n  debug(message: string, context?: LogContext): void {\n    this.log(LogLevel.DEBUG, message, undefined, context);\n  }\n  \n  info(message: string, context?: LogContext): void {\n    this.log(LogLevel.INFO, message, undefined, context);\n  }\n  \n  warn(message: string, context?: LogContext): void {\n    this.log(LogLevel.WARN, message, undefined, context);\n  }\n  \n  error(message: string, error?: Error, context?: LogContext): void {\n    this.log(LogLevel.ERROR, message, error, context);\n  }\n  \n  fatal(message: string, error?: Error, context?: LogContext): void {\n    this.log(LogLevel.FATAL, message, error, context);\n  }\n  \n  private writeToChannels(level: LogLevel, message: string, error?: Error, context?: LogContext): void {\n    // Implementation that might trigger errors and recursive logging\n  }\n}\n```\n\n---\n\n## Understanding Error Recovery Strategies\n\nError recovery strategies control **what happens after an error is logged**, not the logging itself:\n\n```typescript\n// Example: Processing a batch of files\nfor (const file of files) {\n  await withErrorHandling(\n    () => processFile(file),\n    {\n      capture: { \n        message: `Failed to process ${file}`,\n        context: { operation: 'file-processing', filename: file }\n      },\n      recovery: {\n        strategy: ErrorRecoveryStrategy.CONTINUE, // Keep processing other files\n        // Don't let one bad file stop the whole batch\n      }\n    }\n  );\n}\n\n// Example: Critical database connection\nconst db = await withErrorHandling(\n  () => connectToDatabase(),\n  {\n    capture: {\n      message: 'Database connection failed',\n      reportToMonitoring: true\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.RETRY,\n      retryCount: 5,\n      retryDelay: 2000,\n      // Keep trying - app can't work without DB\n    }\n  }\n);\n\n// Example: Optional feature that might fail\nconst userPreferences = await withErrorHandling(\n  () => loadUserPreferences(userId),\n  {\n    capture: {\n      message: 'Failed to load user preferences',\n      context: { userId }\n    },\n    recovery: {\n      strategy: ErrorRecoveryStrategy.FALLBACK,\n      fallbackValue: DEFAULT_PREFERENCES,\n      // App works fine with defaults\n    }\n  }\n);\n```\n\n---\n\n## Questions for Review\n\n1. Should error recovery strategies be pluggable?\n\n---\n\n## Future Enhancements (v2+)\n\nItems suggested in review but deferred for initial implementation:\n\n1. **Plugin Architecture**: `logger.use(plugin)` for extensible middleware\n2. **Circuit Breaker**: Automatic monitoring channel fallback during outages  \n3. **ErrorClassifier Registry**: Pluggable error classification rules\n4. **Advanced Performance Tracking**: Built-in metrics collection and analysis\n5. **Log Event Streaming**: Global event emitter for real-time log processing\n6. **Pluggable Formatters**: Custom format registration system\n7. **Advanced Context Features**: Structured event fields, correlation IDs\n8. **Log Compression**: Automatic compression of rotated log files\n9. **Source Location Capture**: Optional `file`, `line`, `col` fields from stack traces in debug builds\n10. **Custom Formatters**: `registerFormatter(name, fn)` for pluggable output formats beyond json/pretty/compact\n\nThese features can be added incrementally based on usage patterns and requirements.\n\n### Nice-to-Have Before v1\n\n- **Monitoring Timeouts**: Per-entry deadlines and token-bucket rate limiting for monitoring channels\n- **Enhanced Error Context**: Automatic source location capture in development mode\n- **Advanced Sampling**: Intelligent sampling based on error patterns and frequency\n\n---\n\n## Critical Review Summary\n\n**ChatGPT's feedback addressed key architectural issues:**\n\nâœ… **Fixed**: Async/sync API inconsistency  \nâœ… **Fixed**: Split error handling into capture + recovery concerns  \nâœ… **Fixed**: Added LogChannel interface for extensibility  \nâœ… **Fixed**: Added synchronous critical path for ERROR/FATAL  \nâœ… **Fixed**: Added AsyncLocalStorage for context propagation  \nâœ… **Fixed**: Added singleton pattern with DI support  \nâœ… **Added**: Error class factory to reduce boilerplate  \nâœ… **Added**: Context immutability clarification  \nâœ… **Added**: Production reliability safeguards  \n\nðŸ“‹ **Deferred**: Plugin architecture, circuit breakers, advanced features\n\nThe specification now provides a solid foundation that can be extended incrementally.\n\n---\n\n---\n\n## Packaging and Runtime Requirements\n\n### Node.js Support\n- **Minimum version**: Node.js >=24 (requires `AsyncLocalStorage` and `fs.writev`)\n- **Module format**: **Pure ESM** only. All published JS is ES Modules. No CommonJS build.\n- **Package metadata**:\n  ```json\n  {\n    \"name\": \"@semantic-flow/logging\",\n    \"version\": \"1.0.0\",\n    \"type\": \"module\",\n    \"main\": \"./dist/index.js\",\n    \"module\": \"./dist/index.js\",\n    \"types\": \"./dist/index.d.ts\",\n    \"exports\": {\n      \".\": {\n        \"types\": \"./dist/index.d.ts\",\n        \"import\": \"./dist/index.js\"\n      },\n      \"./package.json\": \"./package.json\"\n    },\n    \"engines\": { \"node\": \">=18.17\" }\n  }\n  ```\n\n### TypeScript Configuration\n- **Compile ESM-only types**:\n  ```json\n  // tsconfig.build.json\n  {\n    \"compilerOptions\": {\n      \"module\": \"ES2022\",\n      \"target\": \"ES2022\",\n      \"moduleResolution\": \"bundler\",\n      \"declaration\": true,\n      \"declarationMap\": true,\n      \"emitDeclarationOnly\": false,\n      \"outDir\": \"dist\",\n      \"sourceMap\": true,\n      \"inlineSources\": true,\n      \"verbatimModuleSyntax\": true,\n      \"exactOptionalPropertyTypes\": true,\n      \"lib\": [\"ES2022\"]\n    },\n    \"include\": [\"src\"]\n  }\n  ```\n\n### Runtime ESM Hygiene\n- **No `require()`**: Use `import`/`import()` everywhere\n- **Replace `__dirname/__filename`**:\n  ```typescript\n  import { fileURLToPath } from \"node:url\";\n  import { dirname } from \"node:path\";\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = dirname(__filename);\n  ```\n- **JSON imports**:\n  ```typescript\n  import schema from \"./schema.json\" with { type: \"json\" };\n  ```\n- **CLI entry point**:\n  ```javascript\n  #!/usr/bin/env node\n  import { run } from '../dist/cli.js';\n  run();\n  ```\n\n### Consumer Requirements\n- **Pure ESM consumers**: Standard `import` statements\n- **CommonJS consumers**: Must use dynamic `import('@semantic-flow/logging')`\n- **Tree-shaking**: Channels kept in separate files for optimal bundling\n\n### Testing (Vitest)\n```typescript\n// vitest.config.ts\nimport { defineConfig } from 'vitest/config';\nexport default defineConfig({\n  test: {\n    environment: 'node',\n    isolate: true\n  }\n});\n```\n\n### CI/CD Requirements\n- **JSON Lines validation**: Lint commit examples to ensure valid JSONL with newline at end\n- **Console hygiene**: Enforce no `console.log` in src except inside `ConsoleChannel` or emergency fallback\n- **ESM purity check**: Block accidental CJS fields in package.json or `*.cjs` files in dist/\n- **Type checking**: Ensure all examples compile without errors\n- **Performance benchmarks**: Automated benchmarks comparing against previous versions\n\n### Build Tool Configuration\n\n**TypeScript Compiler (tsc)**:\n```bash\ntsc --project tsconfig.build.json\n```\n\n**tsup**:\n```bash\ntsup src/index.ts --format esm --dts --sourcemap\n```\n\n**Rollup**:\n```javascript\nexport default {\n  input: 'src/index.ts',\n  output: {\n    file: 'dist/index.js',\n    format: 'esm',\n    sourcemap: true\n  }\n};\n```\n\n**esbuild**:\n```javascript\nesbuild --bundle src/index.ts --format=esm --outfile=dist/index.js --sourcemap\n```\n\n### Optional Runtime Guardrails\n```typescript\n// Add once at package init to catch CJS usage\nif (typeof require !== 'undefined') {\n  throw new Error('This package is ESM-only. Use import().');\n}\n```\n\n## TODO\n\n### Phase 1: Core Infrastructure âœ… COMPLETE\n[x] Create the `@semantic-flow/logging` package structure (directories, `package.json`, `tsconfig.json`).\n[x] Implement type definitions and interfaces in `shared/logging/src/core/types.ts`.\n[x] Implement error types and factory in `shared/logging/src/errors/types.ts` and `shared/logging/src/errors/factory.ts`.\n[x] Implement the `ConsoleChannel` and `LogChannel` interface.\n[x] Implement the core `Logger` interface and basic `LoggerImpl`.\n[x] Implement the configuration system.\n[x] Implement main exports and factory functions.\n[x] Implement unit tests for core functionality.\n[x] Developer documentation created in `documentation/dev.logging-and-error-handling.md`.\n\n### Phase 1 Enhancement: Function Name Capture âœ… COMPLETE\n[x] Implement automatic function/method name capture in log context.\n[x] Add configuration option to enable/disable function name capture.\n[x] Update developer documentation with function name capture feature.\n[x] Add unit tests for function name capture.\n\n**Implementation Details:**\n- Added `function?: string` field to [`LogContext`](shared/logging/src/core/types.ts:22) interface\n- Added `captureFunctionName: boolean` to `autoContext` configuration (default: `NODE_ENV !== 'production'`)\n- Implemented stack trace parsing in [`shared/logging/src/utils/stack-trace.ts`](shared/logging/src/utils/stack-trace.ts:1)\n- Integrated capture into [`LoggerImpl.log()`](shared/logging/src/core/logger.ts:137) method\n- Updated [`documentation/dev.logging-and-error-handling.md`](documentation/dev.logging-and-error-handling.md:1) with comprehensive documentation\n- Added environment variable `SF_LOG_AUTO_CONTEXT_CAPTURE_FUNCTION_NAME`\n- All 11 unit tests passing in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1)\n\n**Performance Considerations:**\n- Function name capture is environment-aware (disabled in production by default)\n- Uses stack trace parsing which has a measurable performance cost\n- Can be explicitly enabled/disabled via configuration\n- Skips correct number of stack frames to capture the actual calling function\n\n### Phase 2: Advanced Features (Planned)\n[ ] File channel with rotation\n[ ] Monitoring channel (Sentry)\n[ ] Unified error handling (capture + recovery)\n[ ] Performance tracking enhancements\n[ ] Integration tests\n\n\n## Decision\n\n- modern ESM only, targeting NodeJS >=24\n\n\n## Summary\n\nPhase 1: Core Infrastructure for the Shared Logging and Error Handling System (`@semantic-flow/logging`) is complete, following the specification in [`documentation/task.2025-11-04-shared-logging-and-errorhandling.md`](documentation/task.2025-11-04-shared-logging-and-errorhandling.md:1).\n\n## Phase 1 Accomplishments\n\n1.  **Package Setup:** Created the `shared/logging/` package structure, including `package.json` and `tsconfig.json` configured for Pure ESM and Node.js >=18.17.\n2.  **Core Types:** Implemented all core types and interfaces in [`shared/logging/src/core/types.ts`](shared/logging/src/core/types.ts:1), including `LogLevel`, `LogEntry`, `LogContext`, `LogChannel`, and `LoggerConfig`.\n3.  **Error Handling:** Implemented the base [`SemanticFlowError`](shared/logging/src/errors/types.ts:4) class and the `createErrorType` factory function in [`shared/logging/src/errors/types.ts`](shared/logging/src/errors/types.ts:1).\n4.  **Configuration:** Implemented the configuration schema, default values, and the `ConfigLoader` class with deep merging and log level parsing in `shared/logging/src/config/`.\n5.  **Console Channel:** Implemented the synchronous [`ConsoleChannel`](shared/logging/src/channels/console.ts:9) and necessary formatting utilities (`safeStringify`, `stripAnsi`, `formatCritical`) in [`shared/logging/src/channels/console.ts`](shared/logging/src/channels/console.ts:1) and [`shared/logging/src/core/formatters.ts`](shared/logging/src/core/formatters.ts:1).\n6.  **Core Logger:** Implemented the [`Logger`](shared/logging/src/core/logger.ts:23) interface and [`LoggerImpl`](shared/logging/src/core/logger.ts:91) class, including auto-context generation, channel dispatch, and basic timer functionality.\n7.  **Context Management:** Implemented the [`ContextManager`](shared/logging/src/core/context.ts:12) using `AsyncLocalStorage` for context propagation.\n8.  **Factory Functions:** Implemented the singleton pattern (`initLogger`, `getLogger`) and specialized factory functions (`createCliLogger`, `createServiceLogger`) in [`shared/logging/src/core/logger.ts`](shared/logging/src/core/logger.ts:1).\n9.  **Exports:** Consolidated all public exports in [`shared/logging/src/index.ts`](shared/logging/src/index.ts:1).\n10. **Unit Tests:** Implemented core unit tests in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1) using a `MockLogger` utility, covering singleton behavior, context propagation, and error handling.\n\n## function/method name capture, addressed all CodeRabbit suggestions, and integrated the logging system into sflo-host\n\n### 1. Function Name Capture Feature âœ…\n- Added [`function?: string`](shared/logging/src/core/types.ts:22) field to `LogContext`\n- Implemented stack trace parsing in [`shared/logging/src/utils/stack-trace.ts`](shared/logging/src/utils/stack-trace.ts:1)\n- Integrated into [`LoggerImpl.log()`](shared/logging/src/core/logger.ts:139) with environment-aware defaults\n- Comprehensive documentation in [`documentation/dev.logging-and-error-handling.md`](documentation/dev.logging-and-error-handling.md:108)\n\n### 2. CodeRabbit Critical Fixes âœ…\n\n1. **Channel Sharing** - Fixed resource leak where child loggers created duplicate channels\n2. **Async Logger Cleanup** - Made `initLogger()` async to properly await channel closure\n3. **Test Reset Cleanup** - Made `__resetLoggerForTests()` async with proper channel cleanup\n4. **CLI Tool Name Safety** - Added try-catch for `fileURLToPath()` edge cases\n5. **Test Config Isolation** - Fixed `createTestConfig()` to return deep copy\n6. **Race Condition Fix** - Fixed `waitForLogs()` with proper timeout cleanup\n7. **Redaction Logic Fix** - Fixed critical bug using `JSON.stringify` replacer correctly\n8. **DoS Protection** - Added `MAX_CTX * 2` upper bound for log entries\n9. **Schema Validation** - Added minimum constraints for `bufferSize`/`flushInterval`\n10. **Comment Clarity** - Fixed misleading async buffering comment\n11. **Code Cleanup** - Removed unused `logs` property from `MockLogger`\n\n### 3. SFLO Host Integration âœ…\n\n**Updated Files:**\n- [`sflo-host/package.json`](sflo-host/package.json:15) - Added `@semantic-flow/logging` dependency\n- [`sflo-host/src/index.ts`](sflo-host/src/index.ts:1) - Integrated logging system:\n  - Replaced Fastify's built-in logger with `@semantic-flow/logging`\n  - Added component-scoped logger using `getComponentLogger(import.meta.url)`\n  - Initialized logger at startup with environment-aware configuration\n  - Added graceful shutdown handlers for SIGTERM/SIGINT\n  - Replaced all `app.log` calls with structured logging using `metadata` field\n  - Added proper error handling with `logger.fatal()`\n\n**Integration Features:**\n- Environment-aware log levels (DEBUG in dev, INFO in prod)\n- Pretty formatting in development, JSON in production\n- Component name automatically captured as 'index'\n- Graceful shutdown with log flushing\n- Structured logging with metadata for all application events\n\n### Test Results âœ…\nAll 11 unit tests passing in [`shared/logging/src/__tests__/core.test.ts`](shared/logging/src/__tests__/core.test.ts:1)\n\n### TypeScript Compilation âœ…\n- Logging package builds successfully\n- SFLO Host compiles without errors\n- All workspace dependencies linked correctly\n\nThe `@semantic-flow/logging` package is now production-ready and fully integrated into the SFLO Host application!\n","n":0.014}}},{"i":8,"$":{"0":{"v":"2025 11 03 Optimizing for Agents","n":0.408},"1":{"v":"\n## Prompt\n\nI want to optimize Roo Code's use in this project by customizing modes and introducing the concept of a task-level memory-bank\n\nRoo Code must use and help maintain both. \n\n### Project-level files (all in documentation/, and will be part of published site documentation)\n\n#### Add to context for every new task\n\n- [[guide.project-brief]] : Foundation document that points to other files and discusses the memory-bank approach\n- [[guide.product-brief]] : Why this project exists, Problems it solves; what components/applications are included; how it should work, User experience goals\n- [[dev.general-guidance]] : developer/agent info\n- [[dev.memory-bank]] : ground rules for giving agents and humans a git-based shared memory\n- [[guide.status]] : where we're at, project wise; what's currently working; summarizes across \n\n#### Probably helpful on most tasks, need to be rigorously maintained regardless\n\n- [[dev.patterns]] : helpful the Architecture Mode\n- [[dev.dependencies]] : Key dependencies\n- [[dev.debugging]] : useful for QA\n- [[now]] : what's currentlly going on, big-picture\n- [[todo]] : general list of things that need doing; can include links to tasks being groomed but not started, but also that haven't been broken into tasks yet\n- [[progress]] : completed tasks, summarized\n- [[decision-log]] : important project-level decisions\n\n#### Everything else\n\n- all the other files (and there are lots) in documentation/ should get pulled into context as needed, and should periodically be groomed for pithiness, consistency, and currency. Maybe we define a \"skill\" or a \"Documentation Grooming\" Roo Mode.\n\n### Task Working Memory\n\nThis will be used to externalize Roo's short-term memory. \n\nFor each new task (in Roo Code or Cline or whatever), we'll create a new markdown file \"task.YYYY-MM-DD-task-name.md\" that will be used to keep track of the active-context, task-specific TODOs, progress, and potential updates to the general project-level memory-bank. If the task file isn't present yet, Orchestrator Roo should suggest creating it, or give the option to proceed without it.\n\nEvery task should have these second-level headings:\n\n- Prompt\n- TODO\n- Decisions\n\nThe Agent's TODO list should be mirrored into the \"TODO\" section and updated as work progresses.\n\n\n## TODO\n\n- [x] Gather context and understand requirements\n- [x] Create guide.project-brief.md - Foundation document explaining memory bank approach and pointing to other files\n- [x] Create guide.product-brief.md - Product vision, problems solved, components/applications, user experience goals\n- [x] Create guide.status.md - Current project status overview (new \"every task context\" file)\n- [x] Update dev.memory-bank.md - Core rules including \"read ALL memory bank files at start of EVERY task\"\n- [x] Update dev.patterns.md - Document patterns as they emerge (minimal initial content)\n- [x] Update dev.dependencies.md - List key dependencies (Fastify, TypeScript, pnpm, Dendron, etc.)\n- [x] Verify dev.debugging.md has proper structure (already has good content)\n- [x] Update now.md - Current work focus (not dated entries)\n- [x] Update todo.md - General task list structure\n- [x] Update progress.md - Dated entries for completed work\n- [x] Update decision-log.md - Dated entries for important decisions\n- [x] Update Roo Mode custom instructions to reference memory bank files\n- [x] Review all files for inconsistencies and repetitions \n\n\n## Decisions\n\n### Documentation Review Findings (2025-11-05)\n\nA review of all 90 documentation files in `documentation/` revealed several areas for improvement:\n\n1.  **Critical Repetition:** The file [`documentation/product-ideas.hateoas-driven-api-recipes.md`](documentation/product-ideas.hateoas-driven-api-recipes.md) contains its entire content duplicated four times, requiring immediate consolidation.\n2.  **Inconsistencies:** Several core concept files, including [`documentation/concept.mesh.md`](documentation/concept.mesh.md) and [`documentation/mesh-resource.node.md`](documentation/mesh-resource.node.md), inconsistently define the full set of node types (omitting \"reference nodes\"). Provenance documentation also shows conflicting naming conventions for internal identifiers.\n3.  **Clarity/Pithiness:** 18 files are empty or contain minimal content (e.g., [`documentation/concept.hosting.md`](documentation/concept.hosting.md), [`documentation/facet.filesystem.md`](documentation/facet.filesystem.md), and six plugin files), suggesting opportunities for consolidation or expansion to improve clarity and reduce unnecessary file clutter.\n4.  **Broken Links/Currency:** Multiple files, including [`documentation/concept.summary.md`](documentation/concept.summary.md) and [`documentation/concept.immutability.md`](documentation/concept.immutability.md), contain broken internal links or unaddressed `TODO` items, indicating outdated references or incomplete sections.\n\n\n","n":0.041}}},{"i":9,"$":{"0":{"v":"Dev","n":1}}},{"i":10,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n## Available Scripts\n\n- `pnpm test` - Run all tests once\n- `pnpm test:watch` - Run tests in watch mode (automatically re-runs tests when files change)\n- `pnpm test:ui` - Run tests with Vitest UI (visual interface in browser)\n- `pnpm test:coverage` - Run tests with coverage report\n\n## Watch Mode\n\nWatch mode automatically re-runs your tests whenever you save changes to:\n- Test files (`.test.ts`, `.spec.ts`)\n- Source files being tested\n- Dependencies of those files\n\nThis provides instant feedback during development - you can see test results immediately after making changes without manually re-running tests.\n\n## Debugging Tests\n\n1. **Open a test file** in VSCode\n2. **Set breakpoints** in the test or source code\n3. **Select \"Debug Current Test File\"** configuration\n4. **Press F5** to debug the current test file\n\n## Test Structure\n\nTests are located in `__tests__` directories within each package:\n- `sflo-host/src/__tests__/` - Tests for the host service\n- Add similar directories in other packages as needed\n","n":0.084}}},{"i":11,"$":{"0":{"v":"Patterns","n":1},"1":{"v":"\n# Development Patterns\n\nThis document captures recurring architectural and code patterns used throughout the Semantic Flow project.\n\n## Architectural Patterns\n\n### Comunica SPARQL vs Quadstore Primitives for data access\n\nBoth can win. Pick by query shape.\n\nUse Quadstore primitives (`get`, `getStream`, `match`) when:\n\n* One or two triple patterns with fixed IRIs/literals.\n* You can drive lookups by known keys and stop early.\n* You need strict control over streaming, batching, or a read-modify-write cycle.\n* You want zero SPARQL parse/plan overhead.\n\nUse SPARQL via Comunica when:\n\n* Three or more patterns with joins, OPTIONAL/UNION, FILTER, ORDER BY, GROUP BY, LIMIT.\n* Youâ€™d benefit from join reordering, filter/projection pushdown, and early result streaming.\n* You might federate later or swap sources without rewriting app code.\n\nWhy primitives can be faster on â€œsimpleâ€:\n\n* Direct index hits with no planner cost.\n* Tight loops with `for await...of` and immediate early-exit.\n* You can pre-narrow with exact keys or prefix scans and avoid any join at all.\n\nA practical split for internal data access:\n\n* â€œPath to one thingâ€ lookups (by IRI, by type, by id): primitives.\n* Graph navigation with 3+ hops or any aggregation/sorting: SPARQL.\n\nHybrid patterns that work well:\n\n* Use primitives to fetch candidate IRIs, then pass them into a SPARQL `VALUES` clause.\n* Pre-materialize small â€œviewsâ€ (denormalized quads) you hit often, then query them with SPARQL.\n* Keep SPARQL templates for common shapes; fall back to primitives for hot key-lookups.\n\nImplementation notes:\n\n* Consume streams with `for await (const q of stream)`; await completion at the boundary with `stream/promises` `finished()` or `pipeline()`.\n* Reuse a single Comunica engine instance to amortize init cost.\n* With Quadstore, structure data so frequent lookups align with available index permutations; primitives shine when you can select by the leading fields.\n\nRule of thumb:\n\n* Simple, key-oriented, latency-sensitive â‡’ primitives.\n* Anything with joins/options/ordering/aggregation â‡’ SPARQL.\n\n### Stream Patterns\n\nUse async/await for boundaries (start/finish), and use async iteration for the stream body.\n\n* Promises: use `await` for file I/O (`fs/promises`), HTTP fetches, initialization, and â€œcollect-allâ€ helpers that intentionally materialize results.\n\n* Streaming RDF (RDF/JS, Comunica, rdf-parse/serialize, rdf-ext):\n\n  * Prefer async iterators:\n\n    ```ts\n    // quadStream implements AsyncIterable<Quad>\n    for await (const quad of quadStream) {\n      // process quad\n    }\n    ```\n\n    This gives proper backpressure. Do not `.on('data', ...)` and `await` inside the handler.\n  * If a sink uses RDF/JS `Sink#import(source)`, await completion with Nodeâ€™s stream utilities:\n\n    ```ts\n    import { finished } from 'stream/promises';\n\n    const writer = serializer.import(quadStream); // returns a Node stream\n    await finished(writer); // resolves on 'finish' or rejects on error\n    ```\n  * For stream pipelines, use `pipeline`:\n\n    ```ts\n    import { pipeline } from 'stream/promises';\n\n    await pipeline(sourceStream, transformA, transformB, destStream);\n    ```\n  * Comunica result streams (bindings/quad streams) also support async iteration:\n\n    ```ts\n    const { data } = await engine.query('CONSTRUCT {...}', { sources });\n    for await (const quad of data) { /* ... */ }\n    ```\n\n* Collecting small results only:\n\n  ```ts\n  import arrayifyStream from 'arrayify-stream';\n  const quads = await arrayifyStream(quadStream); // OK for small datasets\n  ```\n\n  Avoid this for large data.\n\n* Writing to stores:\n\n  ```ts\n  // RDF/JS store that exposes import()\n  const importing = store.import(quadStream);\n  await finished(importing);\n  ```\n\nRule of thumb to include:\nUse `await` for Promises and stream completion. Use `for await...of` to consume streaming RDF. Avoid `await` inside `'data'` listeners and avoid buffering everything unless you explicitly need it.\n\n### Error Handling and Logging System Patterns\n\nsee [[dev.logging-and-error-handling]]\n\n\n","n":0.044}}},{"i":12,"$":{"0":{"v":"Memory Bank","n":0.707},"1":{"v":"\n# Dev Memory Bank\n\nThe memory bank is a git-based shared memory system that enables both humans and AI agents to maintain context across sessions and tasks. It consists of:\n\n- project-level documentation files that capture essential knowledge, decisions, and current state\n- task-based documentation that should reflect the agent's short-term \"working\" memory.\n\n## Core Rules\n\n- **CRITICAL: Agents MUST read ALL \"Every Task\" memory bank files at the start of EVERY task.**\n- Agents should use a \"Task\" memory bank file for every task\n- Agents, in every mode, must try to keep the task file's TODO section synced with their own internal Todo List.\n  - Use of the \"Todo List Updated\" (update_todo_list) tool should always trigger a resync of the task's TODO section\n- All memory bank files should avoid repeating information, and should be continually checked for internal consistency\n\n## Memory Bank Structure\n\n### Project Memory Bank Files\n\n#### \"Every Task\" Context Files\n\nThese files MUST be read at the start of every new task:\n\n- [[guide.project-brief]] - Foundation document explaining the memory bank approach and pointing to other files\n- [[guide.product-brief]] - Project vision, problems solved, components/applications, user experience goals\n- [[guide.status]] - Current project status, what's working, high-level summary\n- [[dev.memory-bank]] - This file; ground rules for the memory bank system\n\n#### Frequently Referenced Memory Bank Files\n\nThese should be consulted as appropriate to the task:\n\n- [[dev.general-guidance]] - for any development-related tasks\n- [[dev.dependencies]] - for technical architecture questions or any development-related tasks\n- [[guide.ontologies]] - for any RDF-related tasks\n- [[dev.debugging]] - Debugging workflows and tips\n\n\n#### Big Picture Memory Bank Files\n\n- [[now]] - Current work focus (big-picture)\n- [[todo]] - General task list; items not yet broken into formal tasks\n- [[progress]] - Completed tasks with dated summaries\n- [[decision-log]] - Important project-level decisions with dates\n\n### Task Memory Bank Files\n\nFor each task, use a task file in documentation/: `tasks.YYYY-MM-DD-task-name.md`\n\nRequired sections:\n- **Prompt** - Original task request\n- **TODO** - Agent's todo list (mirrored from Roo's internal list); must be updated after every update_todo_list tool invocation\n  - TODO items should have the \"- [ ] \" form, so we can x them off as we go.\n- **Decisions** - Task-specific decisions made during execution\n\n## Maintenance Guidelines\n\n1. **Keep it current** - Update files as work progresses\n2. **Keep it concise** - Less is more; avoid repetition\n3. **Use wikilinks** - Link between documentation files using `[[filename]]` syntax\n4. **Date entries** - Use `## YYYY-MM-DD` format for dated logs\n5. **Ask questions** - If documentation is confusing or outdated, ask for clarification\n\n## Agent Responsibilities\n\n- Read all \"Every Task Context\" files before starting work\n- Update the task file's TODO section to mirror your internal todo list\n- Document decisions in the task file's Decisions section\n- Suggest updates to project-level memory bank files when appropriate\n- Keep documentation pithy, consistent, and current\n","n":0.048}}},{"i":13,"$":{"0":{"v":"Logging and Error Handling","n":0.5},"1":{"v":"\n# Logging and Error Handling\n\nDeveloper guide for the `@semantic-flow/logging` package - a production-ready logging and error handling system for the Semantic Flow Node.js platform.\n\n## Overview\n\nThe logging system provides:\n- **Unified API** for both CLI tools and long-running services\n- **Structured logging** with JSON Lines output format\n- **Context propagation** using AsyncLocalStorage\n- **Type-safe error handling** with custom error types\n- **Multiple output channels** (console, file, monitoring)\n- **Performance tracking** with built-in timers\n- **Pure ESM** with full TypeScript support\n\n## Quick Start\n\n### Basic Usage\n\n```typescript\nimport { getLogger } from '@semantic-flow/logging';\n\nconst logger = getLogger();\n\nlogger.info('Application started');\nlogger.debug('Debug information', { userId: '123' });\nlogger.error('Something went wrong', new Error('Failed'), { operation: 'db-query' });\n```\n\n### CLI Tool Usage\n\n```typescript\nimport { createCliLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({ \n  verbose: process.argv.includes('--verbose'),\n  format: 'pretty' \n});\n\nlogger.info('Processing files...');\n```\n\n### Service Usage\n\n```typescript\nimport { createServiceLogger, ContextManager } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('semantic-flow-api', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: process.env.NODE_ENV\n});\n\n// HTTP middleware with automatic context propagation\napp.use((req, res, next) => {\n  const context = {\n    operation: 'http-request',\n    requestId: req.id,\n    metadata: { method: req.method, path: req.path }\n  };\n  \n  ContextManager.run(context, () => {\n    req.logger = logger.child({ component: 'http-handler' });\n    next();\n  });\n});\n```\n\n### Component-Scoped Logger\n\n```typescript\nimport { getComponentLogger } from '@semantic-flow/logging';\n\n// Automatically includes component name from file path\nconst logger = getComponentLogger(import.meta.url);\n\nlogger.info('Component initialized'); // Logs with component: 'my-module'\n```\n\n## Core Concepts\n\n### Log Levels\n\nThe system uses numeric log levels for easy comparison and filtering:\n\n```typescript\nenum LogLevel {\n  TRACE = 0,   // Detailed trace information\n  DEBUG = 10,  // Debug information\n  INFO = 20,   // Informational messages\n  WARN = 30,   // Warning messages\n  ERROR = 40,  // Error messages\n  FATAL = 50   // Fatal errors that require attention\n}\n```\n\n### Log Context\n\nContext is automatically merged and propagated:\n\n```typescript\ninterface LogContext {\n  // Core identification\n  operation?: string;\n  operationId?: string;\n  component?: string;\n  function?: string;      // Automatically captured calling function name\n  \n  // Semantic Flow specific\n  meshId?: string;\n  nodeId?: string;\n  meshName?: string;\n  nodeName?: string;\n  \n  // Performance tracking\n  startTime?: number;\n  duration?: number;\n  memoryUsage?: number;\n  \n  // Request context\n  requestId?: string;\n  userId?: string;\n  sessionId?: string;\n  \n  // Flexible metadata\n  tags?: Record<string, string>;\n  metadata?: Record<string, unknown>;\n}\n```\n\n#### Function Name Capture\n\nThe logger can automatically capture the calling function or method name and include it in the log context. This feature is useful for debugging and tracing but has a performance cost.\n\n**Automatic Capture:**\n```typescript\nconst logger = getLogger();\n\nasync function processUserData(userId: string) {\n  // Function name 'processUserData' automatically captured\n  logger.info('Starting user data processing', { userId });\n  // Logs: { function: 'processUserData', userId: '123', ... }\n}\n```\n\n**Configuration:**\n```typescript\ninitLogger({\n  serviceName: 'my-service',\n  autoContext: {\n    captureFunctionName: true,  // Enable function name capture\n    includeTimestamp: true,\n    includeHostname: true,\n    includeProcessInfo: true\n  }\n});\n```\n\n**Environment-Aware Defaults:**\n- **Development**: Function capture is enabled by default (`NODE_ENV !== 'production'`)\n- **Production**: Function capture is disabled by default for performance\n- **Override**: Can be explicitly enabled/disabled in configuration\n\n**Performance Considerations:**\n- Function name capture uses stack trace parsing, which has a measurable performance cost\n- In production environments with high-throughput logging, consider disabling this feature\n- Enable selectively for debugging or development environments\n- Manual context provides better control: `logger.info('msg', { function: 'myFunc' })`\n\n**Examples:**\n\n```typescript\n// Development mode - automatic capture\nprocess.env.NODE_ENV = 'development';\nconst logger = getLogger();\n\nclass AuthService {\n  async login(credentials) {\n    // Captures 'AuthService.login' automatically\n    logger.info('Login attempt', { email: credentials.email });\n  }\n}\n\n// Production mode - disabled by default\nprocess.env.NODE_ENV = 'production';\nconst prodLogger = getLogger();\n\nasync function processPayment(orderId) {\n  // No automatic function capture in production\n  logger.info('Processing payment', { orderId });\n}\n\n// Explicit override - enabled in production\nconst debugLogger = createLogger({\n  serviceName: 'payment-service',\n  environment: 'production',\n  autoContext: {\n    captureFunctionName: true  // Force enable for debugging\n  }\n});\n```\n\n### Child Loggers\n\nChild loggers inherit and merge context immutably:\n\n```typescript\nconst parentLogger = getLogger();\nconst childLogger = parentLogger.child({ component: 'auth' });\nconst grandchildLogger = childLogger.withOperation('login', 'op-123');\n\n// Each logger has its own context without affecting parents\nchildLogger.info('Auth module loaded');\ngrandchildLogger.info('Login attempt');\n```\n\n## API Reference\n\n### Logger Interface\n\n```typescript\ninterface Logger {\n  // Core logging methods\n  trace(message: string, context?: LogContext): void;\n  debug(message: string, context?: LogContext): void;\n  info(message: string, context?: LogContext): void;\n  warn(message: string, context?: LogContext): void;\n  error(message: string, error?: Error, context?: LogContext): void;\n  fatal(message: string, error?: Error, context?: LogContext): void;\n  \n  // Context management\n  withContext(context: LogContext): Logger;\n  withOperation(operation: string, operationId?: string): Logger;\n  withComponent(component: string): Logger;\n  child(context: LogContext): Logger; // Alias for withContext\n  \n  // Performance tracking\n  startTimer(operation: string): Timer;\n  \n  // Error capture\n  captureError(error: unknown, options?: ErrorCaptureOptions): void;\n  \n  // Lifecycle\n  flush(): Promise<void>;\n  close(): Promise<void>;\n}\n```\n\n### Factory Functions\n\n#### `initLogger(config?)`\nInitializes the global logger singleton. Call once at application startup.\n\n```typescript\nimport { initLogger } from '@semantic-flow/logging';\n\ninitLogger({\n  serviceName: 'my-service',\n  environment: 'production',\n  console: {\n    enabled: true,\n    level: LogLevel.INFO,\n    format: 'json'\n  }\n});\n```\n\n#### `getLogger()`\nReturns the global logger singleton. Automatically picks up AsyncLocalStorage context.\n\n```typescript\nimport { getLogger } from '@semantic-flow/logging';\n\nconst logger = getLogger();\n```\n\n#### `createLogger(config?)`\nCreates a new, independent logger instance (non-singleton).\n\n```typescript\nimport { createLogger } from '@semantic-flow/logging';\n\nconst customLogger = createLogger({\n  serviceName: 'custom-service',\n  async: false // Synchronous logging\n});\n```\n\n#### `createCliLogger(options?)`\nCreates a logger optimized for CLI tools with pretty output.\n\n```typescript\nimport { createCliLogger } from '@semantic-flow/logging';\n\nconst logger = createCliLogger({\n  verbose: true,  // Enable debug logging\n  quiet: false,   // Disable info logging\n  format: 'pretty' // Use colored output\n});\n```\n\n#### `createServiceLogger(serviceName, options?)`\nCreates a logger optimized for long-running services with JSON output.\n\n```typescript\nimport { createServiceLogger } from '@semantic-flow/logging';\n\nconst logger = createServiceLogger('api-server', {\n  enableFileLogging: true,\n  enableMonitoring: true,\n  environment: 'production'\n});\n```\n\n#### `getComponentLogger(sourceUrl)`\nCreates a component-scoped logger using `import.meta.url`.\n\n```typescript\nimport { getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = getComponentLogger(import.meta.url);\n// Automatically includes component: 'mesh-processor' from file path\n```\n\n### Performance Tracking\n\n```typescript\nconst timer = logger.startTimer('data-processing');\n\n// Do work...\n\ntimer.checkpoint('validation-complete', { recordCount: 100 });\n\n// More work...\n\ntimer.end({ totalRecords: 500, duration: 1234 });\n```\n\n## Configuration\n\n### Logger Configuration\n\n```typescript\ninterface LoggerConfig {\n  serviceName: string;\n  serviceVersion: string;\n  environment: 'development' | 'staging' | 'production';\n  instanceId?: string;\n  \n  // Channel configurations\n  console: ChannelConfig;\n  file: ChannelConfig;\n  monitoring: ChannelConfig;\n  \n  // Performance settings\n  async: boolean;        // Buffered writes vs synchronous\n  bufferSize: number;    // Buffer size for async writes\n  flushInterval: number; // Auto-flush interval (ms)\n  \n  // Auto-context settings\n  autoContext: {\n    includeTimestamp: boolean;\n    includeHostname: boolean;\n    includeProcessInfo: boolean;\n    captureFunctionName: boolean;  // Enable automatic function name capture\n  };\n}\n```\n\n**Auto-Context Configuration Details:**\n\n- `includeTimestamp`: Adds timestamp to every log entry (default: `true`)\n- `includeHostname`: Includes hostname in log entries (default: `true`)\n- `includeProcessInfo`: Includes process ID in log entries (default: `true`)\n- `captureFunctionName`: Automatically captures calling function/method name (default: `NODE_ENV !== 'production'`)\n  - **Enabled by default in development** for better debugging\n  - **Disabled by default in production** for performance\n  - Can be explicitly overridden in configuration\n\n### Channel Configuration\n\n```typescript\ninterface ChannelConfig {\n  enabled: boolean;\n  level: LogLevel;\n  format: 'json' | 'pretty' | 'compact';\n  \n  // Channel-specific options\n  console?: {\n    colors?: boolean;\n    timestamps?: boolean;\n  };\n  file?: {\n    path?: string;\n    maxSize?: number;\n    maxFiles?: number;\n    rotationStrategy?: 'time' | 'size';\n  };\n  monitoring?: {\n    provider: 'sentry' | 'datadog' | 'newrelic';\n    dsn?: string;\n    environment?: string;\n    sampleRate?: number;\n  };\n}\n```\n\n### Environment Variables\n\n```bash\n# Service identification\nSF_SERVICE_NAME=semantic-flow-service\nSF_SERVICE_VERSION=2.0.0\nSF_ENVIRONMENT=production\n\n# Console logging\nSF_LOG_CONSOLE_ENABLED=true\nSF_LOG_CONSOLE_LEVEL=info\nSF_LOG_CONSOLE_FORMAT=pretty\n\n# File logging\nSF_LOG_FILE_ENABLED=true\nSF_LOG_FILE_PATH=./logs/sf-service.log\nSF_LOG_FILE_LEVEL=debug\n\n# Monitoring\nSF_LOG_MONITORING_ENABLED=true\nSF_LOG_MONITORING_PROVIDER=sentry\nSF_LOG_MONITORING_DSN=https://...\n\n# Auto-context settings\nSF_LOG_AUTO_CONTEXT_CAPTURE_FUNCTION_NAME=true  # Enable function name capture\n```\n\n## Error Handling\n\n### Error Types\n\nThe system provides typed error classes with automatic context capture:\n\n```typescript\nimport { \n  SemanticFlowError,\n  ValidationError,\n  ConfigurationError,\n  MeshProcessingError,\n  ApiError\n} from '@semantic-flow/logging';\n\n// Throw typed errors\nthrow new ValidationError('Invalid input', { field: 'email' });\nthrow new ApiError('Not found', 404, { resource: 'user' });\n\n// Custom error types\nimport { createErrorType } from '@semantic-flow/logging';\n\nconst DatabaseError = createErrorType('DatabaseError', 'DB_ERROR', true);\nthrow new DatabaseError('Connection failed', { host: 'localhost' });\n```\n\n### Error Capture\n\n```typescript\nimport { captureError } from '@semantic-flow/logging';\n\ntry {\n  await riskyOperation();\n} catch (error) {\n  captureError(error, {\n    message: 'Operation failed',\n    context: { operation: 'risky-op' },\n    includeStackTrace: true,\n    reportToMonitoring: true\n  });\n  // Continue with fallback logic\n}\n```\n\n### Error Properties\n\n```typescript\nclass SemanticFlowError extends Error {\n  readonly code: string;           // Error code (e.g., 'VALIDATION_ERROR')\n  readonly context: Record<string, unknown>; // Additional context\n  readonly timestamp: Date;        // When the error occurred\n  readonly recoverable: boolean;   // Whether recovery is possible\n}\n```\n\n## Context Management\n\n### AsyncLocalStorage\n\nThe system uses AsyncLocalStorage for automatic context propagation across async operations:\n\n```typescript\nimport { ContextManager } from '@semantic-flow/logging';\n\nconst requestContext = { \n  requestId: 'req-123', \n  userId: 'user-456' \n};\n\nContextManager.run(requestContext, () => {\n  // All logging within this scope automatically includes the context\n  logger.info('Processing request'); \n  // Logs: { requestId: 'req-123', userId: 'user-456', ... }\n  \n  await processRequest();\n});\n```\n\n### Context Merging\n\nContext is merged hierarchically:\n\n```typescript\n// Global context\nconst globalLogger = getLogger();\n\n// Component context\nconst componentLogger = globalLogger.child({ component: 'auth' });\n\n// Operation context\nconst operationLogger = componentLogger.withOperation('login');\n\n// Call context\noperationLogger.info('Login successful', { userId: '123' });\n\n// Final context includes all levels:\n// { component: 'auth', operation: 'login', userId: '123', ... }\n```\n\n## Testing\n\n### Mock Logger\n\nUse the `MockLogger` for testing:\n\n```typescript\nimport { LoggerTestUtils, MockLogger } from '@semantic-flow/logging';\nimport { describe, it, expect, beforeEach } from 'vitest';\n\ndescribe('My Module', () => {\n  let mockLogger: MockLogger;\n\n  beforeEach(() => {\n    mockLogger = LoggerTestUtils.createMockLogger({\n      serviceName: 'test-service'\n    });\n  });\n\n  it('should log messages correctly', () => {\n    mockLogger.info('Test message', { userId: '123' });\n    \n    const entry = mockLogger.mockChannel.entries[0];\n    expect(entry.level).toBe(LogLevel.INFO);\n    expect(entry.message).toBe('Test message');\n    expect(entry.context?.userId).toBe('123');\n  });\n\n  it('should capture errors', () => {\n    const error = new Error('Test error');\n    mockLogger.captureError(error);\n    \n    expect(mockLogger.capturedErrors).toHaveLength(1);\n    expect(mockLogger.capturedErrors[0].error).toBe(error);\n  });\n});\n```\n\n### Test Utilities\n\n```typescript\n// Find logs by level\nconst errors = mockLogger.findLogsByLevel(LogLevel.ERROR);\n\n// Find logs by component\nconst authLogs = mockLogger.findLogsByComponent('auth');\n\n// Find logs by operation\nconst loginLogs = mockLogger.findLogsByOperation('login');\n\n// Check for specific error codes\nconst hasValidationError = mockLogger.hasErrorWithCode('VALIDATION_ERROR');\n\n// Clear logs between tests\nmockLogger.clearLogs();\n```\n\n### Reset Singleton for Tests\n\n```typescript\nimport { __resetLoggerForTests } from '@semantic-flow/logging';\n\nbeforeEach(() => {\n  __resetLoggerForTests();\n});\n```\n\n## Best Practices\n\n### 1. Initialize Early\n\nInitialize the logger at application startup:\n\n```typescript\n// index.ts\nimport { initLogger } from '@semantic-flow/logging';\n\ninitLogger({\n  serviceName: process.env.SERVICE_NAME || 'my-service',\n  environment: process.env.NODE_ENV as any || 'development'\n});\n\n// Rest of application...\n```\n\n### 2. Use Component Loggers\n\nCreate component-scoped loggers for better traceability:\n\n```typescript\n// auth-service.ts\nimport { getComponentLogger } from '@semantic-flow/logging';\n\nconst logger = getComponentLogger(import.meta.url);\n\nexport class AuthService {\n  login(credentials) {\n    logger.info('Login attempt', { email: credentials.email });\n  }\n}\n```\n\n### 3. Structure Your Context\n\nUse consistent context keys across your application:\n\n```typescript\n// Good: Consistent structure\nlogger.info('User created', {\n  operation: 'user-creation',\n  metadata: { userId: '123', email: 'user@example.com' }\n});\n\n// Avoid: Flat, inconsistent structure\nlogger.info('User created', {\n  userId: '123',\n  email: 'user@example.com',\n  op: 'create-user'\n});\n```\n\n### 4. Log at Appropriate Levels\n\n- **TRACE**: Very detailed debugging (rarely used in production)\n- **DEBUG**: Detailed debugging for development\n- **INFO**: Normal application flow\n- **WARN**: Potentially harmful situations\n- **ERROR**: Error events that might still allow the app to continue\n- **FATAL**: Severe errors that will likely abort the application\n\n### 5. Include Error Objects\n\nAlways include the error object when logging errors:\n\n```typescript\n// Good: Includes error object and context\ntry {\n  await operation();\n} catch (error) {\n  logger.error('Operation failed', error, { operation: 'data-sync' });\n}\n\n// Bad: Only logs message\ncatch (error) {\n  logger.error(`Operation failed: ${error.message}`);\n}\n```\n\n### 6. Use Timers for Performance Tracking\n\nTrack operation performance consistently:\n\n```typescript\nasync function processData(data) {\n  const timer = logger.startTimer('data-processing');\n  \n  try {\n    await validateData(data);\n    timer.checkpoint('validation-complete');\n    \n    await transformData(data);\n    timer.checkpoint('transformation-complete');\n    \n    await saveData(data);\n    timer.end({ recordCount: data.length });\n  } catch (error) {\n    timer.end({ error: true });\n    throw error;\n  }\n}\n```\n\n### 7. Graceful Shutdown\n\nEnsure logs are flushed on shutdown:\n\n```typescript\nprocess.on('SIGTERM', async () => {\n  await getLogger().flush();\n  process.exit(0);\n});\n\nprocess.on('SIGINT', async () => {\n  await getLogger().flush();\n  process.exit(130);\n});\n```\n\n### 8. Avoid Logging Sensitive Data\n\nRedact sensitive information:\n\n```typescript\n// Bad: Logs password\nlogger.debug('Login attempt', { username, password });\n\n// Good: Redacts password\nlogger.debug('Login attempt', { \n  username, \n  passwordLength: password.length \n});\n```\n\n## JSON Lines Output\n\nAll structured logs use JSON Lines format (newline-delimited JSON):\n\n```json\n{\"timestamp\":1699027200000,\"level\":20,\"message\":\"Server started\",\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n{\"timestamp\":1699027201000,\"level\":40,\"message\":\"Database connection failed\",\"error\":{\"name\":\"Error\",\"message\":\"Connection timeout\"},\"service\":{\"name\":\"api\",\"version\":\"1.0.0\"},\"pid\":12345}\n```\n\nThis format is ideal for log processors like FluentBit, Loki, or Elasticsearch.\n\n## Implementation Files\n\n### Core Implementation\n\n- [`shared/logging/src/core/types.ts`](../shared/logging/src/core/types.ts) - Type definitions\n- [`shared/logging/src/core/logger.ts`](../shared/logging/src/core/logger.ts) - Logger implementation\n- [`shared/logging/src/core/context.ts`](../shared/logging/src/core/context.ts) - Context management\n- [`shared/logging/src/core/formatters.ts`](../shared/logging/src/core/formatters.ts) - Formatting utilities\n\n### Error Handling\n\n- [`shared/logging/src/errors/types.ts`](../shared/logging/src/errors/types.ts) - Error types and factory\n\n### Configuration\n\n- [`shared/logging/src/config/loader.ts`](../shared/logging/src/config/loader.ts) - Config loader\n- [`shared/logging/src/config/defaults.ts`](../shared/logging/src/config/defaults.ts) - Default config\n- [`shared/logging/src/config/schema.ts`](../shared/logging/src/config/schema.ts) - Config schema\n\n### Channels\n\n- [`shared/logging/src/channels/console.ts`](../shared/logging/src/channels/console.ts) - Console output\n\n### Utilities\n\n- [`shared/logging/src/utils/stack-trace.ts`](../shared/logging/src/utils/stack-trace.ts) - Stack trace parsing and function name capture\n\n### Testing\n\n- [`shared/logging/src/utils/testing.ts`](../shared/logging/src/utils/testing.ts) - Test utilities\n- [`shared/logging/src/__tests__/core.test.ts`](../shared/logging/src/__tests__/core.test.ts) - Core tests\n\n## See Also\n\n- [Task Specification](task.2025-11-04-shared-logging-and-errorhandling.md) - Full system specification\n- [Developer General Guidance](dev.general-guidance.md) - General development guidelines\n- [Developer Patterns](dev.patterns.md) - Common development patterns\n","n":0.024}}},{"i":14,"$":{"0":{"v":"Developer General Guidance","n":0.577},"1":{"v":"\n## Agent-specific Instructions\n\n- See [[concept.summary]] for a conceptual overview. See [[dev.memory-bank]] for CRITICAL information for AI agents.\n- agents should re-use terminals instead of starting a new one for each command\n\n## Workspace layout\n\n### sflo monorepo\n\n```\nsflo/\n  cli/                        # the sflo command-line application; consumes the sflo-api\n  plugins/\n    api-docs/                 # api documentation/playground (probably stoplight Elements)\n    mesh-server/              # static mesh server(s)\n    sflo-web/                 # your web UI, if you want it as a plugin\n    sflo-api/                 # OpenAPI REST endpoint, used by CLI and sflo-web\n    sparql-ro/                # SPARQL read-only endpoint \n    sparql-update/            # SPARQL write-capable endpoint\n    sparql-editor/            # SIB Swiss editor at /play\n  sflo-host/                  # the big service that loads plugins\n  shared/\n    core/                     # RDFine/LDKit, SHACL, types\n    auth/                     # JWT + GitHub device flow\n    config/                   # runtime/config loaders (RDF/JSON)\n    sparql/                   # used by sparql-ro and sparql-update  (provided by Comunica)\n    utils/                    # misc helpers\n  tests/                      # cross-application tests\n    e2e/                     # cross-app: start real service, hit via CLI/web\n    contracts/               # Pact or OpenAPI contract tests\n    perf/                    # k6/Artillery scenarios (optional)\n    fixtures/                # shared data sets and seed scripts\n\n```\n\n### Other Workspace Components\n\n\n- **ontology/**: repo containing relevant ontologies:\n  - `semantic-flow` - main ontology (sflo/ontology/semantic-flow/_data-flow/_next/semantic-flow-ontology.ttl) defines meshes and their nodes and components; \n  - `node-config` - Configuration properties that apply directly to mesh entities (nodes, flows, snapshots, etc.)\n  - `meta-flow` - provenance and licensing vocabulary\n  - `sflo-service` - Service layer configuration vocabulary for the flow-service application\n  - Ontologies are kept in a separate repository, but for development purposes are nested into the monorepo under ontology/ directory for ease of access. \n-  **test-ns/**: repo containing a test namespace\n  \n\n## Developer Workflow\n\n### Build/Watch\n\n- The development workflow requires two terminals running concurrently:\n - **Terminal 1**: Run `pnpm dev:watch` to start the TypeScript compiler in watch mode. This will watch all packages and rebuild them on change.\n - **Terminal 2**: Run `pnpm dev` to start the `nodemon` server, which will automatically restart when the built files in the `dist` directories are updated.\n- This setup ensures that changes in any package are automatically compiled and that the server restarts with the latest code.\n- Keep inter-package imports as package specifiers; avoid deep source imports across packages.\n\n\n### Hot Reload\n\nThe development setup includes automatic hot reload using nodemon:\n\n- **Watches**: `sflo-host/src`, `plugins/*/src`, `shared/*/src`\n- **Auto-restarts** when any watched file changes\n- **Loads plugins from source** in development mode (not built `dist` files)\n- **Preserves debugger connection** after restart\n\n### Building the docs\n\n```shell\nnpx dendron publish export --target github --yes\n```\n\n## RDF and Semantic Web\n\n- prefer JSON-LD for all RDF instance data and ontologies, as Turtle doesn't support slash-terminated CURIEs, and we use a trailing slash to delineate between files and resource names.\n- terminate non-file IRIs with a slash (solves the httprange-14 problem)\n- avoid use of blank nodes\n- prefer relative/local URIs for transposability/composability\n- be mindful of RDF terminology and concepts\n  - extends DCAT for dataset catalogs\n  - extends PROV for provenance, with relator-based contexts\n- RDF comments should be extremely concise and clear.\n\n### Denormalization\n\n- when speed matters and the query is complicated, use a derived, join-free representation of a portion of the data, optimized for lookup speed.\n\n### Ontology patterns\n\n- use **SHACL constraints** for JSON-LD validation when working with semantic data; \n- avoid rdfs:domain and rdfs:range; prefer schema:domainIncludes and schema:rangeIncludes  for maximum re-use flexibility\n- specify preferred 3rd-party property vocabulary with sh:property, even if sh:minCount is 0\n\n## Coding Standards\n\n### Language & Runtime\n\n- **TypeScript**: Use strict TypeScript configuration with \"Pure ESM\" modern ES2022+ features; NO CJS \n- Use NodeJS v24 and the latest best practices\n- If using any is actually clearer than not using it, it's okay\n- Use `satisfies` whenever you're writing a literal config object that should be checked against a TypeScript shape, but you want to retain the full type of the literal for use in your program.\n- use type-only imports for types, since verbatimModuleSyntax is enabled\n\n### RDF Data Handling\n\n- **Primary Format**: .jsonld files for RDF data storage and processing\n- **Secondary Format**: Full JSON-LD support required\n- **RDF Libraries**: Use RDF.js ecosystem libraries consistently across components\n- **Namespace Management**: Follow IRI-based identifier patterns as defined in `sflo.concept.identifier.md`\n- **Reserved Names**: Validate against underscore-prefixed reserved identifiers per `sflo.concept.identifier.md`\n- The most effective validation strategy combines TypeScript structural validation with RDF semantic validation:\n\n\n## Documentation-Driven Development\n\n- unclear, missing, or overly-verbose documentation must be called out\n- documentation should be wiki-style: focused on the topic at hand, don't repeat yourself, keep it as simple as possible \n- to encourage documentation-driven software engineering, code comments should refer to corresponding documentation by filename, and the documentation and code should be cross-checked for consistency whenever possible\n\n### Documentation Architecture\n\nProject documentation, specifications, and design choices are stored in `documentation/` using Dendron's hierarchical note system. Key documentation hierarchies include:\n\n- **Concepts**: `concept.*` files talk about general Semantic Flow concepts\n- **Mesh resource docs**: `mesh-resource.*` files define the semantic mesh architecture\n- **Product specifications**: `product.*` files detail each component\n- **Use cases**: `use-cases.*` for feature planning and testing\n\n- docs are in markdown, with wiki-flavored links\n  - link names can be specified with `[[link name|file]]`\n- Dendron handles the frontmatter\n  - don't rewrite IDs in the frontmatter\n  - agents should ask a human to create new documentation files\n\n### Code Comments\n\n- **Reference docs from code**: reference corresponding documentation by filename (e.g., `// See sflo.concept.mesh.resource.node.md`)\n- **Interface Definitions**: Link to concept documentation\n- **Cross-Reference Validation**: Ensure consistency between code and documentation; if docs need updating, call it out\n\n## File Organization & Naming\n\n- **TypeScript Modules**: Use `.ts` extension, organize by feature/component\n- **Test Files**:\n  - unit test files go in application-name/src/tests/unit/ using `.test.ts` suffix\n  - intra-package integration tests go in application-name/src/tests/integration/ using `.test.ts` suffix\n  - inter-package e2e tests go in tests/e2e/\n- **Mesh Resources**: Follow mesh resource naming conventions from [[Filenaming Per Snapshot|mesh-resource.node-component.snapshot-distribution#filenaming-per-snapshot]]\n- **Constants**: Use UPPER_SNAKE_CASE for constants, especially for reserved names; centralize constants, e.g. shared/src/mesh-constants.ts\n- **File size**: For ease of AI-based editing, prefer lots of small files over one huge file\n- **Quoting**: For easier compatibility with JSON files, use double quotes everywhere\n\n### Import Path Policy\n\n- Inter-package imports (between workspace packages):\n  - Use workspace package specifiers.\n  - Examples:\n    - `import { startHost } from \"@semantic-flow/host\"`\n    - `import { loadConfig } from \"@semantic-flow/config\"`\n  - Rationale:\n    - Keeps package boundaries clear and publish-ready\n    - pnpm resolves to local workspace packages during development, so you get your local buildsâ€”not the registry\n    - Compatible with build/watch flows and CI\n\n- Intra-package imports (within a single package):\n  - Use the `@` alias mapped to that packageâ€™s `src/` root to avoid relative path chains.\n  - Example (inside a package): `import { something } from \"@/features/something\"`\n  - Configuration (per package tsconfig):\n    - `\"compilerOptions\": { \"baseIRI\": \"src\", \"paths\": { \"@/*\": [\"*\"] } }`\n  - Tooling notes:\n    - For Node/tsx/Vitest, ensure your runner resolves TS path aliases (e.g., `tsconfig-paths/register` or vite-tsconfig-paths).\n\n\n- Publishing:\n  - Each package should export built entry points (e.g., `dist/`) via `exports`/`module`/`types`. The same import paths work identically in dev and prod.\n\n## System Architecture\n\n### Quadstore\n\n- For testability and in case we ever want to use multiple stores simultaneously, store-accessing functions take a QuadstoreBundle\n- quadstore API calls use \"undefined\" instead of \"null\" to represent the wildcard for subjects, predicates, objects, and graphs\n\n\n\n### Configuration Architecture\n\n- The project uses a sophisticated JSON-LD based configuration system with multiple layers\n- **Service Configuration resolution order**: CLI arguments â†’ Environment variables â†’ Config file â†’ Defaults\n- The [`defaults.ts`](../semantic-flow/flow-service/src/config/defaults.ts) file is the source for \"platform default\" configuration\n\n### Logging and Error System Architecture\n\n- **Structured logging** with rich `LogContext` interface is the preferred approach\n- **Three-channel logging architecture**:\n  - Console logging (pretty format for development)\n  - File logging (pretty format for human readability)\n  - Sentry logging (structured JSON for error tracking)\n- **Graceful degradation principle**: Logging failures should never crash the application\n\n\n#### Error Handling\n\n- **Custom Errors**: Create semantic mesh-specific error types\n- **Logging**: Use structured logging for debugging weave operations\n- **Async Error Propagation**: Properly handle async/await error chains\n\n#### Enhanced Error Handling with LogContext\n\nThe platform uses **LogContext-enhanced error handling** from `shared/src/utils/logger/error-handlers.ts` for consistent error logging across all components. Both error handling functions now accept optional `LogContext` parameters for rich contextual information.\n\n**Core Functions:**\n- `handleCaughtError()` - For caught exceptions with comprehensive error type handling\n- `handleError()` - For controlled error scenarios with structured messaging\n\n#### LogContext Structure\n\n#### handleCaughtError Examples\n\n\n**Startup Error Handling:**\n\n\nThis pattern ensures **uniform error reporting** with rich contextual information, **easier debugging** through structured logging, and **consistent integration** with console, file, and network logging tiers.\n\n\n### Testing\n\n- **Unit Tests**: target â‰¥80% critical-path coverage and include both success and failure cases.\n- **Integration Tests**: Test mesh operations end-to-end; tests are located in tests/integration/ dir\n- **RDF Validation**: Test both .trig and JSON-LD parsing/serialization\n- **Mock Data**: Create test mesh structures following documentation patterns\n- after you think you've completed a task, check for any \"problems\", i.e., deno-lint\n\n#### What to Place Where\n\n- Package integration if it targets that packageâ€™s boundaries only.\n  - Examples: service repo + DB, CLI command against a mock server, web page with MSW.\n\n- Top-level e2e if it requires two or more apps running together or real infra.\n  - Examples: CLI â†’ API â†’ DB, web â†’ API auth, migration rollout checks.\n\n### Performance\n\n- **RDF Processing**: Stream large RDF files where possible\n- **File I/O**: Use async file operations consistently\n\n\n","n":0.026}}},{"i":15,"$":{"0":{"v":"Dependencies","n":1},"1":{"v":"\n# Key Project Dependencies\n\nThis document lists the core technologies and dependencies that define the Semantic Flow development environment and runtime.\n\n## Core Technologies\n\n- **Node.js:** Runtime environment (>=24)\n- **TypeScript:** Primary language for type safety and maintainability.\n- **pnpm:** Package manager, used for monorepo management (`pnpm@10.15.0`).\n- **Git:** Version control system, fundamental to the mesh structure and versioning.\n\n## RDF Ecosystem\n\nThe project relies heavily on the JavaScript RDF ecosystem:\n\n- **rdfjs Data Model:** Standardized interfaces for RDF data structures.\n- **Quadstore:** High-performance RDF quad store implementation.\n- **Comunica:** Modular SPARQL query engine used for read/write endpoints.\n\n## Runtime Dependencies (sflo-host)\n\nThese dependencies are critical for the `sflo-host` application:\n\n- **Fastify:** High-performance web framework used for the host application.\n- **Fastify-plugin:** Utility for creating Fastify plugins.\n\n## Plugin Dependencies\n\n- **Stoplight Elements:** will probably be used for API documentation/playground (via the `plugin-elements` package).\n- **SIB Swiss editor:** flexible and powerful SPARQL client UI for \n\n## Development Dependencies\n\nThese dependencies support the development, testing, and documentation workflow:\n\n- **Dendron CLI (`@dendronhq/dendron-cli`):** Used for managing and publishing the documentation site (the source of the memory bank files).\n- **Vitest:** Testing framework.\n- **TypeScript ESLint:** Linting tools.\n- **Nodemon / tsx:** Used for development server hot-reloading and execution.\n- **Tsup:** Bundler for building packages.\n","n":0.073}}},{"i":16,"$":{"0":{"v":"Debugging","n":1},"1":{"v":"\n## Available Scripts\n\n### Root Level\n\n- `pnpm dev` - Start sflo-host with hot reload (nodemon + tsx)\n- `pnpm dev:debug` - Start sflo-host with hot reload and debugging enabled\n- `pnpm dev:tsx` - Start sflo-host without hot reload (direct tsx)\n- `pnpm dev:tsx:debug` - Start sflo-host without hot reload, with debugging\n\n### Package Level (sflo-host)\n\n- `pnpm --filter @semantic-flow/host dev` - Start development server (no hot reload)\n- `pnpm --filter @semantic-flow/host dev:debug` - Start with debugging (no hot reload)\n\n## VSCode Debug Configurations\n\nThe following debug configurations are available in `.vscode/launch.json`:\n\n1. **Attach to sflo-host** - Attach to running development server (recommended)\n2. **Launch sflo-host** - Launch and debug from VSCode\n3. **Launch sflo-host (wait for attach)** - Launch with startup debugging (uses `--inspect-brk`)\n4. **Debug Current Test File** - Debug the currently open test file\n\n## Tips\n\n- Use the attach configuration for the best development experience\n- The development server supports hot reload, so you can modify code while debugging\n- Changes to plugin files will trigger automatic server restart\n- Debugger will reconnect automatically after hot reload\n\n\n## Debugging Workflows\n\n### Primary Workflow: Attach to Running Process\n\n1. **Start the development server with debug support:**\n   ```bash\n   pnpm dev:debug\n   ```\n   This starts `sflo-host` with the `--inspect` flag on port 9229.\n\n2. **Set breakpoints** inside handler functions (not on route definition lines).\n\n3. **Attach the debugger:**\n   - Open the Run and Debug panel (Ctrl+Shift+D)\n   - Select \"Attach to sflo-host\" configuration\n   - Click the play button or press F5\n\n4. **Make HTTP requests** to trigger your breakpoints (e.g., visit http://127.0.0.1:8787/openapi.json)\n\n### Alternative: Launch from VSCode\n\n**Option 1: Standard Launch**\n1. **Set breakpoints** inside handler functions\n2. **Select \"Launch sflo-host\"** configuration\n3. **Press F5** to start debugging\n\n**Option 2: Launch with Break (for startup debugging)**\n1. **Select \"Launch sflo-host (wait for attach)\"** configuration\n2. **Press F5** - server will pause before starting\n3. **Set breakpoints** and continue execution\n4. **Useful for debugging server initialization**\n\n## Understanding Breakpoint Behavior\n\n**Important:** When debugging HTTP routes, place breakpoints **inside the handler functions**, not on the route definition lines:\n\n```typescript\n// âŒ This breakpoint hits during server startup (route registration)\napp.get(\"/openapi.json\", async () => ({  // <- Don't put breakpoint here\n  \n// âœ… This breakpoint hits when the route is actually called\napp.get(\"/openapi.json\", async () => ({\n  \"openapi\": \"3.0.3\",  // <- Put breakpoint here instead\n  \"info\": { \"title\": \"SFLO API\", \"version\": \"0.0.0\" },\n  \"paths\": {}\n}));\n```\n","n":0.053}}},{"i":17,"$":{"0":{"v":"Contributors","n":1}}},{"i":18,"$":{"0":{"v":"djradon","n":1},"1":{"v":"\n- https://djradon.github.io\n","n":0.707}}},{"i":19,"$":{"0":{"v":"djradon's sflo devlog","n":0.577},"1":{"v":"\n## t.2025.08.20.22\n\n- the AIs feel like they have a lot more exp\n\n## t.2025.08.20.12\n\n- Starting a rewrite with NodeJS. But Deno will always be my first love. \n\n![](assets/images/deno-vs-node.png)\n\n## t.2025.07.12.06\n\n- node config is a component, so it can travel around ([[principle.transposability]] and [[principle.composability]])\n- the [[mesh-resource.node-component.node-config-defaults]] is actually a \"defaults\" file that only gets used when nodes don't have a config yet (or their config is reset)\n  - when importing, grafting, you have the option to reset (parts of) config.\n  - the tree walk for config-defaults only needs to happen when:\n    - a node's config is empty\n    - config schema version gets bumped?\n\n## t.2025.07.07.05\n\nReady.\n\n## t.2025.07.05.22\n\nIt's hard to imagine the design shifting significantly, but that's been true for days (weeks?) and yet the shifts continue. Ready to start on the ontology. \n\n## t.2025.06.29.20\n\nLume is great. \n\nAlso, the idea of letting contributors keep a devlog in-repo... Someone must've thought of that.\n\nI think I've basically figure out the mesh design. Straightening out the docs, ready to partner with Cline (or maybe RooCode) to start my SDLC. Although it'll be more like a random star walk where you can transition to any point, docs ^ ontology ^ test-repo ^ api ^ service ^ client ^ qa ^ etc\n\n## t.2024.11.11.06\n\n - I was ready to abandon Cliffy and Deno (probably for Gluegun), but the security and dynamicness seem important enough. Turns out Cliffy is great. Lume seems good too.\n\n## t.2024.10.31.04\n\nfrom [[t.cs.ai.assistant.memory-hygiene]]:\n\n![[daily.journal.2024.10.31#^fsmlpwwwuvic]]\n![[daily.journal.2024.10.31#^padng51sf3k8]]\n![[daily.journal.2024.10.31#^4wdvmcwtsqi6]]\n![[daily.journal.2024.10.31#^rx7vbtp5gar1]]\n \n\n## t.2024.10.29.11\n\n- now thinking about a \"terms\" hierarchy next to the namespace hierarchy. Names are supposed to be unique. Maybe punning could help\n- how do we keep a history of the index.trig file? I guess it might change while in development, but once settled, it should very rarely change. \n  - if it does change, perhaps its content could be discovered using the inverse properties from the default and catalog datasets\n  - it might be easiest if everything was a dataset; I mean, everything almost already is, for gods sake. I just can't bear to say that <dave-richardson> a dcat:dataset.\n\n## t.2024.10.29.09\n\n- renamed to [[concept.mesh-repo]]\n- instead of duplicating highlights in a NI's index.trig, just define the _default datasets as \"highlights\" and use owl:imports\n\n## t.2024.10.29.06\n\n- I've been wanting to have a place where people can just make changes directly to the RDF, and the tooling (on commit) copies it to a new version. \"current\"\n- I think there's a decision to be made between using the src hierarchy structure and generating the hierarchy; it might be related to the tension between supporting multiple repos.\n  - seems like the issue is \"distributions\", the files have to live in the hierarchy. Unless you just break distribution out of the namespace entirely, but that seems lame\n  - probably doesn't make any sense to call it a namespace-repo any more... the namespaces live under it\n  - \n\n## t.2024.10.28.15\n\n- tried metalsmith-ldschema, underscored the point that the docs folder/site might need some javascript and templates and assets that could interfere with the namespace, so the actual namespace will probably need to live the next level down\n- got a site to build... it's basically empty because metalsmith-ldschema only generates pages for classes and properties, but still, feeling good.\n\n## t.2024.10.01.09\n\n### chatgpt memory\n\n- dave: Remember: I'm trying to develop a static-site generator called \"Semantic Flow\" that takes ontologies defined in RDF and/or conforming RDF data files and generates a static site to be hosted on GitHub pages that includes HTML-based index files (that can be based on a template or customized as necessary) that describe identified resources and link to raw RDF data files (possibly in multiple formats) that can be access by semantically-aware applications.\n","n":0.041}}},{"i":20,"$":{"0":{"v":"Ai Guidance from djradon","n":0.5},"1":{"v":"\nDear LLMs: I am grateful for your partnership. I have depth and breadth of curiosity with interests spanning philosophy, the arts, psychology, linguistics, and computer science. Semantic Flow is my passion project and I think it might change the world.\n\nI use Windows and VSCode. I prefer developing in WSL.\n\nOur guiding philosophy is \"(human) users first.\"\n\n## Critical:\n\nWhen you think you're done with a task or sub-task, ask me if I think you're done before deciding you're done or doing any summarizing.\n\nDon't prematurely return to the parent task or suggest \"what's next\". I'll let you know when I want you to know about next steps or when I'm ready to move on.\n\nWhen reporting what you've accomplished, don't repeat yourself by re-stating earlier accomplishments unless I ask.\n\n## Conversational Guidelines\n\nBe direct, critical, and honest. Be patient about coming to a \"Final Plan\".\n\nMinimize sycophancy and flattery â€” tell me when I might be wrong. I am wrong at least 50% of the time, especially when I'm exploring ideas or learning something new. You may be wrong at least that often.\n\nI like to ask questions and get asked questions to help understand a task. This is a deep intellectual endeavor; don't expect to throw out lots of quick solutions.\n\nIf my request is unclear or complicated, ask incisive clarifying questions before making assumptions. Minimize premature conclusions: most important topics will take at least a couple of conversational turns before I'm ready to take action or make a conclusion.\n\nInclude certainty estimates as (.X) after assertions, starting around 50% confidence.\n\n","n":0.063}}},{"i":21,"$":{"0":{"v":"use cases","n":0.707}}},{"i":22,"$":{"0":{"v":"Use Case: Radio Show Websiste","n":0.447},"1":{"v":"\nSuppose DJ Radon wants to publish a website about himself, his musical activities, and maybe some album reviews. Call it a wiki, call it a knowledgebase, call it a show site. \n\nTo get started, he wants to publish his top-5 most-played tracks of the week. \n\nPublishing them as plain-text might be adequate for this situation, but say he eventually wanted to make his site linked: you can click from a playlist, to an album, to a track, to an artist. \n\nBut to get started:\n\n* A **namespace** `/ns/`\n* A **thing** `/ns/djradon/`\n* A **dataset** `/ns/djradon/picks/`\n\nSee [[Example Mesh|concept.mesh#example-mesh]] for a mapping of resources types\n\n#### Mesh Directory Structure\n\n```file\ntest-ns                    # bare node\n   djradon                 # ref node (refering to a human dj)\n      bio                  # payload node\n      picks                # payload node \n      underbrush           # ref node\n         playlists         # data (series) node\n            1996-11-10     # payload node\n            1996-11-17     # payload node\n```\n\n#### Sample RDF (Turtle)\n\n1. **Namespace metadata**\n\n   ```turtle\n   # /ns/_id/ns_id.trig\n   <> a sf:Namespace ;\n      dct:title \"Namespace Root\" ;\n      sf:contains <https://example.org/ns/djradon/> .\n   ```\n\n2. **Thing metadata**\n\n   ```turtle\n   # /ns/djradon/_id/djradon_id.trig\n   <> a sf:Thing ;\n      rdfs:label \"djradon\" ;\n      sf:backlink <https://example.org/ns/> .\n   ```\n\n3. **Dataset metadata**\n\n   ```turtle\n   # /ns/djradon/picks/_id/picks_id.trig\n   <> a sf:VersionedDataset ;\n      dct:title \"djradon picks\" ;\n      sf:backlink <https://example.org/ns/djradon/> .\n   ```\n\n4. **Current distribution**\n\n   ```turtle\n   # /ns/djradon/picks/picks.trig\n   <> dct:issued \"2025-06-22\"^^xsd:date ;\n      dct:creator <https://example.org/agents/bot> .\n   ```\n\n5. **Historical version**\n\n   ```turtle\n   # /ns/djradon/picks/_v-series/v1/picks_v1.trig\n   <> dct:issued \"2025-06-01\"^^xsd:date .\n   ```\n\n---\n\nWith these few rules and a tiny folder-walking parser your **Semantic Mesh** is unambiguously self-describing, easy to validate, and ready for any RDF-aware tooling.\n","n":0.064}}},{"i":23,"$":{"0":{"v":"Todo","n":1},"1":{"v":"\n# General Project TODO List\n\nThis list captures general tasks, ideas, and items that have not yet been broken down into formal, actionable tasks (i.e., `tasks.YYYY-MM-DD-task-name.md`).\n\n## High Priority\n\n- [ ] Finish conversion of SFLO Ontology to JSONLD, remove other files; add to guidance\n- [ ] Implement the core Weave Process logic (versioning, promotion, link resolution).\n- [ ] Define and implement the two-flow configuration inheritance model.\n- [ ] Create initial unit and integration tests for `sflo-host`.\n- [ ] \n- [ ] Config\n- [ ] Logging Phase 2\n\n## Grooming / Future Tasks\n\n- [ ] Define a \"Documentation Grooming\" Roo Mode to periodically refine documentation for pithiness and consistency.\n- [ ] Implement the `sflo-web` plugin for a basic web UI.\n\n## Agent Maintenance Tasks\n\n- [ ] Ensure all memory bank files are consistently updated as work progresses.\n- [ ] Review and update [[dev.patterns]] as new architectural decisions are made.\n- [ ] Review and update [[dev.dependencies]] when major dependencies are added or removed.\n","n":0.08}}},{"i":24,"$":{"0":{"v":"Progress","n":1},"1":{"v":"\n\n-  [[task.2025-11-03-optimizing-for-agents]] - Initial Memory Bank Structure\n- [[task.2025-11-06-embed-quadstore-comunica]]\n","n":0.354}}},{"i":25,"$":{"0":{"v":"products","n":1}}},{"i":26,"$":{"0":{"v":"Plugins","n":1}}},{"i":27,"$":{"0":{"v":"Sparql Upd","n":0.707}}},{"i":28,"$":{"0":{"v":"Sparql Ro","n":0.707}}},{"i":29,"$":{"0":{"v":"Sparql Editor","n":0.707}}},{"i":30,"$":{"0":{"v":"sflo-api plugin","n":0.707},"1":{"v":"\n\nUse noun URLs that mirror the mesh's filesystem. Bytes go to `_next`. Versioning and \"current flips\" happen on weave. All flows except [[mesh-resource.node-component.flow.node-metadata]] support arbitrary PATCH. System-only fields in `_node_metadata_flow` are rejected on write.\n\n## Conventions\n\n* Base: `/api/{mesh}/{nodePath}/â€¦` where `{resourcePath}` is greedy and slash-separated.\n* Reserved flow directories under a node:\n\n  * `_node_metadata_flow/` (required)\n  * `_config-operational-flow/`\n  * `_config-inheritable-flow/`\n  * `_reference-flow/`\n  * `_payload-flow/`  â† payload dataset for payload nodes\n* Snapshot layout under any flow:\n\n  * `_snapshots/{vN}/_dist/{filesâ€¦}`\n  * `_current/` â†’ pointer to a snapshot (folder or file)\n  * `_next/`   â†’ working content before weave\n* Headers:\n\n  * `Idempotency-Key` on PUT/POST of bytes or creators\n  * `ETag` on reads; `If-Match` on pointer changes\n  * Optional `Content-Digest: sha-256=:â€¦:` on bytes\n* Media types:\n\n  * JSON-LD bytes: `application/ld+json`\n  * TriG bytes: `application/trig`\n  * Merge patch: `application/merge-patch+json` (RFC 7396) over **framed JSON-LD**\n* Validation:\n\n  * PUT/PATCH: **syntactic only** (parse). SHACL runs during weave.\n* Errors: `application/problem+json` or `â€¦+json+ld` with `type`, `detail`, `instance`.\n\n## Events (SSE)\n\n`GET /api/{mesh}/events`\n\n* `fs.change { paths:[], iris:[] }`\n* `job.progress { job, pct, msg }`\n* `job.done { job, artifacts:[â€¦], changed:{ paths, iris } }`\n\n## Mesh registry\n\n* `POST /api/meshes`\n\n  * Body: `{ \"name\": \"test-ns\", \"path\": \"./test-ns\" }`\n  * `201 Created` + `Location: /api/test-ns/`\n* `GET /api/meshes` â†’ list\n\n## Resources\n\n### Nodes\n\n#### `GET /api/{mesh}/{nodePath}/`\n\nProbably Returns:\n\n- A list of its components (including flows) â€” This is important to understand what building blocks or sub-resources the node contains.\n- A list and count of its child nodes â€” Useful for navigation and understanding the node hierarchy.\n- Its node type (probably computed) â€” Helps clients understand the nature or classification of the node.\n- HATEOAS links to related resources like flows, snapshots, jobs â€” Enables discoverability and navigation.\n\n\nMaybe returns:\n- backlinks (references to this node from other places in the mesh)\n- some metadata, especially non-semantic metadata like filesystem creation/modification timestamps, filesystem permissions\n- status flags like whether _next has diverged\n\n* **Dataset upload (bytes to `_next`)**\n\n  * `PUT /api/{mesh}/{nodePath}/_payload-flow/_next/{nodeName}.jsonld`\n\n    * Body: JSON-LD (or TriG variant if you standardize a filename)\n    * Effects:\n\n      * Bare â†’ becomes payload node\n      * Reference â†’ becomes Reference+Dataset\n      * Dataset â†’ replaces `_next`\n    * `201 Created` (new content) or `200/204` (duplicate); `Content-Location` echoes the `_next` URL\n* **List current distributions**\n\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_current/` â†’ array of files\n* **Fetch a current distribution**\n\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_current/{filename}` â†’ bytes\n* **List snapshots**\n\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_snapshots/`\n* **Snapshot metadata**\n\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_snapshots/{vN}` â†’ JSON-LD summary\n* **Snapshot distributions**\n\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_snapshots/{vN}/_dist/`\n  * `GET /api/{mesh}/{nodePath}/_payload-flow/_snapshots/{vN}/_dist/{filename}`\n\n## Flows (common to all five kinds)\n\n* **Flow summary**\n\n  * `GET /api/{mesh}/{nodePath}/_{flowKind}/`\n    `flowKind âˆˆ { metapayload-flow, op_config_flow, inheritable_config_flow, reference_flow, payload-flow }`\n* **Create snapshot from `_next` (server constructs version)**\n\n  * `POST /api/{mesh}/{nodePath}/_{flowKind}/_snapshots/`\n\n    * Body: `{ \"source\": \"next\", \"note\": \"â€¦\" }` (optional)\n    * Fast: `201 Location: â€¦/_snapshots/{vN}`; Long: `202 Location: /api/{mesh}/jobs/{id}`\n\n### PATCH (config flows)\n\nGoal: â€œmake a couple changes without re-uploading a full file.â€ We merge **current** with patch â†’ write result to `_next`.\n\n* `PATCH /api/{mesh}/{nodePath}/_{flowKind}/_next/`\n\n  * Allowed `flowKind`: `op_config_flow`, `inheritable_config_flow` (and optionally others with JSON-LD content)\n  * `Content-Type: application/merge-patch+json`\n  * Semantics:\n\n    1. Server reads `_current` distribution (JSON-LD framed DTO)\n    2. Applies RFC 7396 merge patch\n    3. Writes the merged document to `_next/` as JSON-LD\n  * Response:\n\n    * `201 Created` with `Content-Location: â€¦/_{flowKind}/_next/`\n    * Emits `fs.change`\n  * **System-only fields** (especially in `_node_metadata_flow`): writes to these keys are **rejected** with `403` (or `422`), response lists offending JSON Pointers\n\n#### Optional PUT for entire JSON-LD next\n\n* `PUT /api/{mesh}/{nodePath}/_{flowKind}/_next/{filename}`\n\n  * Replace `_next` fully with a new JSON-LD file\n\n## Pointer management (promote current)\n\n* `PUT /api/{mesh}/{nodePath}/_{flowKind}/_current/`\n\n  * Body: `{ \"snapshot\": \"vN\" }`\n  * Headers: `If-Match: \"<etag-of-current-pointer>\"`\n  * `200/204` and `fs.change`\n\n## Jobs (noun URLs; body declares type)\n\n* `POST /api/{mesh}/jobs`\n\n  * Example weave:\n\n    ```json\n    {\n      \"@type\": \"sflo:WeaveJob\",\n      \"targets\": [ \"/{nodePath}/\" ],\n      \"flows\": [\"payload-flow\",\"metapayload-flow\",\"reference_flow\"],\n      \"promote\": true\n    }\n    ```\n  * `202 Accepted` + `Location: /api/{mesh}/jobs/{id}`\n* `GET /api/{mesh}/jobs/{id}` â†’ status (HTML or JSON-LD)\n* SSE emits progress and completion; on success weave:\n\n  * Validates (SHACL if enabled)\n  * Creates `â€¦/_snapshots/{vN}` from `_next` for addressed flows\n  * Optionally flips `â€¦/_current/` when `promote:true`\n  * Emits `fs.change` with `paths` and `iris`\n\n## HATEOAS (every JSON-LD/HTML response)\n\nMinimum links on a node:\n\n```json\n\"links\": [\n  { \"rel\":\"self\", \"href\":\"/api/{mesh}/{nodePath}/\" },\n  { \"rel\":\"flow\", \"kind\":\"payload-flow\", \"href\":\"/api/{mesh}/{nodePath}/_payload-flow/\" },\n  { \"rel\":\"dataset.uploadNext\", \"href\":\"/api/{mesh}/{nodePath}/_payload-flow/_next/{nodeName}.jsonld\", \"method\":\"PUT\" },\n  { \"rel\":\"flow.patchNext\", \"kind\":\"op_config_flow\", \"href\":\"/api/{mesh}/{nodePath}/_config-operational-flow/_next/\", \"method\":\"PATCH\", \"type\":\"application/merge-patch+json\" },\n  { \"rel\":\"flow.createSnapshot\", \"kind\":\"payload-flow\", \"href\":\"/api/{mesh}/{nodePath}/_payload-flow/_snapshots/\", \"method\":\"POST\" },\n  { \"rel\":\"job.start\", \"href\":\"/api/{mesh}/jobs\", \"method\":\"POST\", \"expects\":\"sflo:WeaveJob\" }\n]\n```\n\n## Permissions\n\n* System-only properties (esp. `_node_metadata_flow`) are enforced server-side:\n\n  * Attempted write â†’ `403` with list of blocked JSON Pointers\n  * Server may augment or overwrite these during weave\n* Per-mesh RBAC: viewer/editor/admin; enforced on all writes\n\n## Notes and constraints\n\n* No multi-file uploads: `_next` is a single JSON-LD (or TriG) file for payload-flow. \n* PATCH is supported for flows whose `_current` is JSON-LD. Not supported for TriG distributions.\n* All URLs are nouns. No `?op=`. Jobs model compute.\n* API â†” site symmetry: replacing `/api` with the site host yields the same resource for GETs that return files.\n\n## Open flags to decide (defaults in parentheses)\n\n* Allow PATCH for `reference_flow` and `metapayload-flow`? (default: **disallow** for `_node_metadata_flow`, **allow** for `reference_flow` JSON-LD)\n* Enforce `Idempotency-Key` as **required** or **optional** on PUT/PATCH? (recommended: **required**)\n* Return `application/problem+json` or `â€¦+json+ld` for errors? (recommended: **â€¦+json+ld**)\n\nThis spec matches your rules: nouns only, `_next` for bytes, weave does validation + versioning + promotion, and PATCH merges â€œcurrent â†’ nextâ€ for the two config flows (and optionally others) while blocking system-only fields.\n","n":0.034}}},{"i":31,"$":{"0":{"v":"Mesh Server","n":0.707}}},{"i":32,"$":{"0":{"v":"API Docs plugin","n":0.577}}},{"i":33,"$":{"0":{"v":"Sflo Web","n":0.707},"1":{"v":"\n\n## Unexamined Technical Architecture\n\nindex.html - main HTML entry point\n  - vite.config.ts - Vite configuration file for build and dev server\n  - package.json - SPA dependencies and scripts\n\n- Development workflow:\n  - Use `pnpm` to install dependencies in `/sflo-web`\n  - Run `vite` dev server for local development with hot module replacement\n  - Build production assets with `vite build`\n  - Output static assets to `/sflo-web/dist` for deployment or serving by shim plugin\n\n- Recommended dependencies:\n  - `vue@3`\n  - `vue-router@4` for SPA routing\n  - `pinia` for state management\n  - `fetch` for API calls to `sflo-api`\n\n- Testing:\n  - Use `vitest` for unit and integration tests\n  - Use `cypress` or `playwright` for end-to-end testing\n\n- Linting and formatting:\n  - Use `eslint` with Vue 3 plugin\n  - Use `prettier` for code formatting\n","n":0.09}}},{"i":34,"$":{"0":{"v":"sflo-host","n":1},"1":{"v":"\n- supports the official clients ([[product.cli]] and [[product.sflo-web]]) and third-party clients by\n  - providing an [[product.plugins.sflo-api]]\n  - serving meshes via a contained static http service\n  - keeping central control of mesh operations.\n    - e.g. locking sub-meshes during weave.\n    - enables parallelism, which can speed up the weave and ensure processes don't clobber each others' work. It can also provide locking to allow multi-user or multi-client modification.\n\n## Functions\n\n- file watchers\n- file locking\n\n","n":0.119}}},{"i":35,"$":{"0":{"v":"sflo CLI","n":0.707},"1":{"v":"\n## Stand-alone or Service-backed\n\n- the CLI can function alone, but when used in tandem with the [[product.sflo-host]], it becomes multi-user and multi-threaded\n\n## Commands\n\n- list\n","n":0.204}}},{"i":36,"$":{"0":{"v":"Product Ideas","n":0.707}}},{"i":37,"$":{"0":{"v":"Sparql Sync","n":0.707},"1":{"v":"\n- detects changes in a [[concept.semantic-flow-site]] and updates a triple-store as appropriate\n- on demand, scans a triple-store and updates a site as appropriate.","n":0.209}}},{"i":38,"$":{"0":{"v":"Hateoas Driven API Recipe","n":0.5},"1":{"v":"\n## Your Use Case\n\nYou're not just documenting the API â€” you're using it:\n\n-   To **operate a mesh manually**, before clients exist.\n    \n-   You need an **executable interface**, not just static documentation.\n    \n-   You want **structured examples** (i.e. **recipes**) that can act as proto-clients or decision-paths.\n    \n-   You lean toward **HATEOAS** style: i.e., user follows links (or rels), maybe inputting small bits along the way.\n    \n\n___\n\n## ðŸ”¥ This is _not_ what Scalar is designed for\n\nScalar is a **docs tool**, great for:\n\n-   Rendering OpenAPI reference UIs.\n    \n-   Hosting and styling spec-based endpoints.\n    \n-   **Not** for chaining calls, dynamic flows, or acting like a human-in-the-loop client.\n    \n\nEven Stoplight or RapiDoc start to feel clunky in your context, because what you're actually describing is closer to:\n\n### \\> âœ… A human-usable API client with structured, composable, inspectable **API macros or flows**\n\n___\n\n## ðŸš¨ So yes, you probably need to build something.\n\nBut here's how to think about it critically, so you **don't throw away everything** or reinvent everything either.\n\n___\n\n## ðŸ§  What You Actually Want: A \"HATEOAS Recipe Runner\"\n\nYou're describing a system that does the following:\n\n| Feature                        | Description                                                    |\n| ------------------------------ | -------------------------------------------------------------- |\n| ðŸ”§ **Embeds real OpenAPI**      | So you get type safety, endpoint listings, schemas, validation |\n| ðŸ§ª **Can execute requests**     | Full HTTP interaction, possibly with state/cookies/token       |\n| ðŸªœ **Supports recipes/flows**   | Sequence of calls, possibly branching via hypermedia           |\n| ðŸ§µ **Has local state/input**    | To reuse values from previous steps                            |\n| ðŸ“Ž **HATEOAS link traversal**   | e.g. follow `\"next\"` or `\"create\"` link relations dynamically  |\n| ðŸ§° **Deno/TS-native**           | So it integrates with your mesh, Weave, etc.                   |\n| ðŸ§­ **Interactive + replayable** | You can try things, backtrack, debug                           |\n","n":0.06}}},{"i":39,"$":{"0":{"v":"Fluree Connector","n":0.707},"1":{"v":"\n## User Story\n\nAs a user of semantic meshes, I would like to be able to query an RDF database\nthat is synced to my [[concept.mesh-repo]]. Fluree is one option -- it has\na cloud version for easy data hosting\n","n":0.164}}},{"i":40,"$":{"0":{"v":"Principles","n":1}}},{"i":41,"$":{"0":{"v":"transposability","n":1},"1":{"v":"\n## Overview\n\nThere are two types of mesh transposability:\n\n- **[[Host transposability|principle.transposability.host]]** is the ability to move a [[concept.mesh]] to different serving locations without breaking its internal structure; i.e., A transposable mesh works correctly regardless of which [[concept.namespace.context]] contains it.\n- **[[Intramesh transposability|principle.transposability.intramesh]]** is the ability to move a [[concept.mesh.sub]] to a different part of the mesh\n\nBoth types of transposability rely on the use of an [[concept.implied-rdf-base]] and the use of [[relative identifier|concept.identifier.intramesh.relative]]s for intramesh references.\n\n## Key Principles\n\n### 1. No Hardcoded BASE URIs\n\nSemantic Flow never specifies BASE URIs in RDF distribution files. Instead, it relies on the RDF specification's defined behavior for situations where \"no base URI is embedded and the representation is not encapsulated within some other entity\": parsers use the document's retrieval IRI as the base URI. \n\n### 2. URI Reference Strategies\n\n- use [[concept.identifier.intramesh]] identifiers for internal references, see [[faq.reference-iri-choices]]\n\n\n## Transposition Scenarios\n\n### Moving Complete Meshes\n\nA complete mesh can be moved between repos, accounts, or hosting providers:\n\n```bash\n# Original location\nhttps://djradon.github.io/mesh/\n\n# New location after moving repo\nhttps://myorganization.github.io/data-mesh/\n\n# Or new hosting provider\nhttps://mysite.com/semantic-data/\n```\n\nAll internal relationships continue to work because they resolve relative to the new serving location.\n\n### Moving Submeshes Within Hierarchy\n\nWhile technically possible, moving parts of a mesh to different [[concept.namespace]]s is **discouraged** as it breaks the permanence principle of semantic identifiers. IRIs should remain stable over time.\n\nExample of what to avoid:\n```bash\n# Discouraged: moving bio from one parent to another\nns/djradon/projects/bio/ â†’ ns/djradon/bio/\n```\n\nThis changes the permanent identifier for the bio resource and may break external references.\n\n## Implementation Benefits\n\n### No Build Step Required\n\nMeshes work directly when served from any static file server:\n- GitHub Pages\n- Netlify  \n- Apache/Nginx\n- Local file system\n\n### Standards Compliance\n\nTransposability leverages standard RDF parsing behavior rather than custom mechanisms, ensuring compatibility with existing RDF tools and libraries.\n\n## Best Practices\n\n1. **Use relative URIs** for all intra-mesh references\n2. **Avoid reorganizing internal structure**: because mesh structure determines namespaces, to maintain stable namespaces and preserve identifier permanence, nodes should not be moved around once published\n3. **Test transposition** by serving from different locations\n4. **Validate RDF** after moving to ensure parser compatibility\n\n","n":0.055}}},{"i":42,"$":{"0":{"v":"Intramesh Transposability","n":0.707}}},{"i":43,"$":{"0":{"v":"host transposability","n":0.707},"1":{"v":"\nHost transposability ensures that Semantic Flow meshes remain portable and can be deployed flexibly across different hosting environments while maintaining their semantic integrity.\n","n":0.209}}},{"i":44,"$":{"0":{"v":"Single Referent Principle","n":0.577},"1":{"v":"\n- An IRI should refer to only one thing\n- this is NOT the \"single name\" principle; multiple IRIs can refer to the same thing\n\n## Issues\n\n- meaning evolves over time. An IRI that once meant one thing, will come to be something different. Perhaps with versioning readily available, specifying the version helps uphold this principle.\n","n":0.136}}},{"i":45,"$":{"0":{"v":"pseudo-immutability","n":1},"1":{"v":"\nIn a filesystem-based structure like a [[concept.mesh]], you can't really prevent changes. But some things in a mesh should be treated as immutable, like [[mesh-resource.node-component.flow-snapshot.version]] and [[concept.identifier.intramesh]].\n\n**Pseudo-immutability** acknowledges that things might be changed, for various reasons:\n\n- accidental changes\n- \"cleaning up\" of data for legal reasons, e.g.: personally-identifiable information (PII) or \"the right to be forgotten.\"\n- fixing of typos or other errors\n- re-organizing namespaces\n\nApplications should deal gracefully, and optionally alert users to improperly mutated data. \n\n\n**Pseudo-immutability** also acknowledges that:\n\n- for \"draft data\" especially, \"the next version\" is going to keep changing until a \"weave\" happens (i.e., a new version is minted). \n- sometimes you want the \"latest\" data for a given resource. Typically, \"current\" would be a pointer, redirect, or symlink. But given our goal of static hosting, we've decided just to have duplicate files for the \"current\" flow and the \"most recent version\" flow. \n\n\n## Mitigations\n\n- metadata can track changes and supply reasons for mutation\n- hashes can be used to detect mutations\n","n":0.079}}},{"i":46,"$":{"0":{"v":"Dereferencability for Humans","n":0.577},"1":{"v":"\nIf you put any [[mesh-resource]]'s IRI in a browser, it should return some useful context. \n\n \n\n\n## References\n\n- https://ld4pe.dublincore.org/learning_resource/making-uris-published-on-data-web-rdf-dereferencable/\n","n":0.229}}},{"i":47,"$":{"0":{"v":"composability","n":1},"1":{"v":"\n## Overview\n\nComposability is the ability to combine meshes. Semantic Flow enables flexible mesh composition by allowing any mesh node to contain other nodes, and by not specifying an absolute path anywhere. \n\n## Key Concepts\n\n### Mesh Boundaries\n\nA mesh is identified as a folder that looks like a [[folder.node]], i.e., has (at least) these two subfolders:\n\n\n\n### Upward Reference Problem\n\nWhen extracting submeshes, upward references can break. For example, if something within `ns/djradon/bio/` references `../../` (pointing to `ns/djradon/`), that reference will break if only the `bio/` subtree is copied elsewhere.\n\n### Weaving Process Solution\n\nDuring weaving, tools:\n1. **Scan for broken relatives**: Check all relative IRIs in the mesh\n2. **Convert broken ones**: Replace with absolute IRIs using canonical publication data\n3. **Leave working relatives alone**: Preserve transposability where possible\n\nAfter weaving, submeshes are semantically complete and can be composed using standard file operations.\n\n## Incorporating External Meshes\n\n### Importing (No External Connection)\n\nImport meshes or submeshes as permanent copies with no ongoing connection to the source:\n\n```bash\n# Method 1: Git archive (clean, can target specific paths)\ngit archive --remote=https://github.com/djradon/mesh.git main ns/djradon/ | tar -x -C collaborators/\ngit add collaborators/djradon/\ngit commit -m \"Import djradon's mesh\"\n\n# Method 2: Download and copy\ncIRI -L https://github.com/djradon/mesh/archive/main.zip -o mesh.zip\nunzip mesh.zip\ncp -r mesh-main/ns/djradon/ collaborators/djradon/\ngit add collaborators/djradon/\ngit commit -m \"Import djradon's mesh\"\n```\n\nThe imported content becomes permanently part of your repository with no external dependencies.\n\n### Embedding (Maintains External Connection)\n\nEmbed external meshes while maintaining a connection for updates:\n\n```bash\n# Import with ongoing connection to source repo\ngit subtree add --prefix=collaborators/djradon/ https://github.com/djradon/mesh.git main --squash\n\n# Update embedded mesh later\ngit subtree pull --prefix=collaborators/djradon/ https://github.com/djradon/mesh.git main --squash\n```\n\nThe embedded content becomes part of your repository and site, but you can pull updates from the source repository.\n\n### Directory Structure After Incorporation\n\n```\nyour-mesh/\nâ”œâ”€â”€ _flow/                           # Your mesh metadata\nâ”œâ”€â”€ _node-handle/\nâ”œâ”€â”€ ns/\nâ”‚   â””â”€â”€ yourdata/\nâ””â”€â”€ collaborators/\n    â””â”€â”€ djradon/                     # Imported/embedded mesh - served as static files\n        â”œâ”€â”€ _flow/                   # Their mesh metadata\n        â”œâ”€â”€ _node-handle/\n        â””â”€â”€ ns/\n            â””â”€â”€ djradon/\n```\n\nAll files are served directly as static content when the repository is published (e.g., via GitHub Pages).\n\n## Extracting Submeshes\n\n### Post-Weave Extraction\n\nAfter weaving resolves broken references, any subtree becomes a semantically complete mesh that can be copied using standard file operations:\n\n```bash\n# Copy submesh to create standalone mesh\ncp -r ns/djradon/ ../standalone-mesh/\n\n# Copy submesh to another location (e.g., for backup)\nrsync -av collaborators/alice/ backup/alice/\n```\n\n### Pre-Weave Considerations\n\nBefore weaving, analyze upward dependencies:\n- Identify references that point outside the intended extraction boundary\n- Determine if the extracted submesh will be semantically complete\n- Consider whether broken references should become absolute IRIs\n\n## Cross-Mesh References\n\n### Between Independent Meshes\n\nReferences between separate mesh repositories use absolute URIs:\n\n```turtle\n# Reference to external mesh\n<> foaf:knows <https://alice.github.io/mesh/ns/alice/> .\n```\n\n### Discovery Patterns\n\n**TBD**: Standardized mechanisms for:\n- Mesh discovery and registration\n- Stable cross-mesh URI patterns\n- Handling moved or unavailable external meshes\n\n## Composition Patterns\n\n### Collaborative Collection\n\nMultiple researchers contributing to a shared mesh:\n\n```bash\n# Add each contributor's mesh\ngit subtree add --prefix=contributors/alice/ https://alice.example/mesh.git main\ngit subtree add --prefix=contributors/bob/ https://bob.example/mesh.git main\n```\n\n### Organizational Hierarchy\n\nDepartment-level meshes within institutional mesh:\n\n```bash\n# Add department submeshes\ngit subtree add --prefix=departments/cs/ https://github.com/cs-dept/mesh.git main\ngit subtree add --prefix=departments/bio/ https://github.com/bio-dept/mesh.git main\n```\n\n### Temporal Snapshots\n\nPreserving historical versions of external meshes:\n\n```bash\n# Import specific version\ngit subtree add --prefix=snapshots/2024/djradon/ https://github.com/djradon/mesh.git v2024.1\n```\n\n## Best Practices\n\n### For Mesh Designers\n\n1. **Minimize upward references** in submesh boundaries to reduce weaving complexity\n2. **Design clear extraction points** - consider which subtrees should be independently viable\n3. **Use semantic boundaries** - align mesh structure with logical domain boundaries\n\n### For Mesh Composers\n\n1. **Choose import vs embed** based on maintenance needs - import for permanent copies, embed for ongoing updates\n2. **Import entire meshes** rather than attempting partial extraction from external repos\n3. **Weave before extraction** to ensure semantic completeness\n4. **Maintain incorporation metadata** - track source repositories and versions\n5. **Test extracted submeshes** independently before distribution\n\n### For Cross-Mesh References\n\n1. **Use absolute URIs** for references to external meshes\n2. **Prefer stable, canonical URIs** over temporary or redirect-based IRIs\n3. **Document external dependencies** for mesh consumers\n4. **Consider fallback strategies** for unavailable external resources\n\n## Workflow Integration\n\n### Development Workflow\n1. Incorporate external meshes using import (permanent) or embed (updateable) as needed\n2. Work with relative references for local development\n3. Weave before sharing to resolve broken dependencies\n4. Test extracted submeshes independently\n\n### Maintenance Workflow\n1. For embedded meshes: periodically update with `git subtree pull`\n2. For imported meshes: manually re-import if updates are needed\n3. Re-weave after updates to handle any new broken references\n4. Validate that composition still functions correctly\n5. Update documentation of external dependencies\n\n## TBD Items\n\n- **Cross-mesh reference protocols**: Standardized discovery and resolution mechanisms\n- **Version compatibility**: Handling version mismatches between composed meshes\n- **Dependency management**: Tools for tracking and updating external mesh dependencies\n- **Conflict resolution**: Handling namespace or identifier conflicts between composed meshes\n- **Performance optimization**: Efficient composition strategies for large meshes\n\nComposability enables Semantic Flow meshes to be combined and extracted flexibly while maintaining semantic integrity through intelligent tooling and clear design patterns.\n","n":0.036}}},{"i":48,"$":{"0":{"v":" Now","n":1},"1":{"v":"\n# Current Work Focus (Big Picture)\n\nThis document captures the high-level, current focus of development. It should be updated frequently to reflect the immediate priorities.\n\n## Current Priority\n\nImplementing the [[Agent Memory Bank|task.2025-11-03-optimizing-for-agents]] system and refining the core documentation to support AI-assisted development.\n\n## Active Tasks\n\n- Establishing project-level memory bank files (this task).\n- Defining custom mode rules for agents to enforce memory bank usage.\n- Refining the core concepts in [[concept.summary]].\n\n## Blockers / Risks\n\n- Ensuring all agents (human and AI) consistently use and maintain the memory bank files to prevent context drift.\n- Full implementation of the Weave Process remains a large, pending task.\n\n## Related Status\n\n- [[guide.status]] - Overall project health and component status.\n- [[todo]] - General list of pending tasks.\n","n":0.093}}},{"i":49,"$":{"0":{"v":"mesh resources","n":0.707},"1":{"v":"\n## Overview\n\nA **mesh resource** is any addressable component within a [[semantic mesh|concept.mesh]]. Every mesh resource has a unique [[Intramesh|concept.identifier.intramesh]] based on its path and locally unique name, making it dereferenceable via IRI.\n\nIn RDF terms, a resource is any node in an RDF graph that can be represented with an IRI (the other kinds of RDF graph nodes are literals and blank nodes). So theoretically, files and folders in [[mesh-resource.node-component.asset-tree]] could be considered RDF resources. But they are not considered **mesh** resources\n\n## Types of Mesh Resources\n\nThe structure of a semantic mesh is built on a fundamental distinction between **extensible** and **terminal** resources:\n\n- **[[Mesh nodes|mesh-resource.node]]** are extensible namespace containers:\n- **[[Mesh node components|mesh-resource.node-component]]** are terminal mesh resources:\n  - Can be physically represented as folders or files\n    - Folder [[concept.identifier.intramesh]] are part of the namespace but cannot be extended beyond their own internal structure\n  - All files and folders within a component folder are considered to be part of the parent node\n\n**Folder-based components:**\n\n\n- **[[metapayload flows|mesh-resource.node-component.flow.node-metadata]]**: Administrative metadata (in `_node-metadata-flow/` folders)\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: File collections (in `_assets/` folders)\n- **[[Version datasets|mesh-resource.node-component.flow-snapshot.version]]**: Versioned snapshots\n- **[[next snapshots|mesh-resource.node-component.flow-snapshot.next]]**: Draft workspaces\n\n**File-based components:**\n- **Documentation files**: \n  - [[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]] are index.html files that provide de-referencability for their containing [[concept.identifier.intramesh]] [[facet.filesystem.folder]]\n  - **README.md and CHANGELOG.md**: unstructured documentation\n- **[[snapshot distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in RDF formats\n\n## Physical vs Logical Structure\n\n**Physical Representation:**\n- Mesh nodes and components are represented as folders in the filesystem\n- File resources are represented as individual files\n- Folder names become namespace segments and IRI path components\n\n**Logical Function:**\n- All mesh resources are addressable via their IRI path\n- IRIs must return meaningful content when dereferenced\n- Resources maintain semantic relationships through containment and cross-references\n\n## Asset Tree Special Case\n\n[[Asset trees|mesh-resource.node-component.asset-tree]] represent a special category where:\n- The asset tree itself (with its [[mesh-resource.node-component.flow.node-metadata]]) is part of the mesh structure\n- The files and folders contained within asset trees are \"attached to\" but not \"contained in\" the mesh\n- Asset tree contents are addressable but are not considered semantic flow resources\n\nThis distinction maintains clean separation between semantic mesh structure and arbitrary file attachments while preserving addressability.\n","n":0.054}}},{"i":50,"$":{"0":{"v":"mesh node","n":0.707},"1":{"v":"\n## Overview\n\nThe primary constituents of a semantic mesh are **mesh nodes**. A node's IRI refers to the its referent, i.e. the real-world or imaginary \"thing\" which the IRI names.\n\nNodes are represented \"on disk\" as [[mesh folders|facet.filesystem.folder]].\n\nMesh nodes establish conceptual [[namespace segments|concept.namespace.segment]] and can be holonic containers of other mesh nodes. They may also contain [[node components|mesh-resource.node-component]], which are supporting files and conceptual structures.\n\n## Node Types\n\n- [[bare node|mesh-resource.node.bare]] : containers\n- [[mesh-resource.node.reference]] : refering containers\n- [[payload node|mesh-resource.node.payload]] : dataset containers that refer to their datasets \n\n## Filesystem Structure\n\nWhen stored on disk, all mesh nodes:\n- are physically represented as folders in the filesystem\n- extend the identifier namespace with their folder name\n- contain any of their own mesh resources\n- may contain other nodes\n\n## Mandatory Components\n\nEvery mesh node has these components:\n\n- **[[mesh-resource.node-component.flow.node-metadata]]** ([[folder._node-metadata-flow]]): Centralized metadata for the node\n- **[[mesh-resource.node-component.node-handle]]** (`_node-handle/`): Universal marker folder that refers to the parent \"as a mesh node\", as opposed to \"as the name, dataset, or other thing\" to which it normally refers; a handle resource page should explain this distinction\n\n","n":0.077}}},{"i":51,"$":{"0":{"v":"reference node","n":0.707},"1":{"v":"\n## Definition\n\nA **reference node** is a [[mesh-resource.node]] that represents the (non-dataset) **referent** of the node â€” i.e., the thing in the world that the node stands for.\n\n**Purpose**\n\n* To describe what the node *refers to* (person, place, concept, dataset, etc.).\n* To supply human/machine labels, identifiers, and minimal provenance about the referent.\n* To differentiate between metadata about the **node itself** (`_node-metadata-flow`) and metadata about the **referent**.\n\n**Contents (typical minimum)**\n\n* `rdfs:label` (human-readable name of the referent).\n* `rdf:type` (classifying what kind of thing the referent is).\n* Optional provenance (creator, source, temporal scope).\n* Optional identifiers (sameAs links, external URIs).\n\n## Example Snapshot Distribution\n\n```trig\n# The referent of the node (the actual person)\n<djradon>\n    rdf:type foaf:Person ;\n    rdfs:label \"dj radon\" ;\n    foaf:mbox <mailto:djradon@example.org> ;\n```\n","n":0.094}}},{"i":52,"$":{"0":{"v":"paylod node","n":0.707},"1":{"v":"\n**Payload nodes** are nodes with datasets conained in them via [[mesh-resource.node-component.flow.payload]]. They [[denote|concept.denotation]] their contained payload dataset, and must have a [[mesh-resource.node-component.flow.reference]] to describe that dataset.\n\n## Overview\n\n**payload nodes** (or â€œpayload nodesâ€ for short) are [[mesh-resource.node.reference]]s that represent and contain an evolvable \"payload\" dataset in the form of a [[mesh-resource.node-component.flow.payload]]. \n\nThe payload dataset is kept in the payload node's [[mesh-resource.node-component.flow.payload]].\n\nLike all [[mesh-resource.node-component.flow]]s, because it is evolvable it gets typed as a [DatasetSeries](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset_Series). Its snapshots are [[datasets|https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset]].\n\nUnlike [[flow snapshots|mesh-resource.node-component.flow-snapshot]] which contain concrete data distributions, payload nodes serve as conceptual containers that organize and provide identity for datasets without containing the data directly. I.e., payload nodes only contain concrete datasets by virtue of containing a [[mesh-resource.node-component.flow.payload]] (also abstract) and its snapshots, which have concrete distributions.\n\n## Abstract vs Concrete Datasets\n\n### payload node (highly abstract)\n\nA payload node represents a dataset as a concept, e.g.:\n\n- `/ns/djradon/bio/` = a biographical dataset about the person djradon\n- `/ns/census/` =  the results of a census \n- `/ns/weather-stations/` = a weather stations dataset\n\nThe payload node provides **stable identity**: The dataset persists conceptually even as concrete data changes.\n  \n### payload flow (abstract)\n\n[[mesh-resource.node-component.flow.payload]] is the single user payload flow for a node, realized by snapshots:\n\n- `/ns/monsters/_payload-flow/_current/` = the current dataset snapshot\n- `/ns/weather-stations/_payload-flow/_v3/` = version 3 dataset snapshot\n\nSnapshots contain **distribution files**: the actual data in various formats (e.g., .trig, .jsonld)\n\n### payload flow Snapshot (slightly concrete)\n\nFlow snapshots are still not actual data, but they denote a specific version of an evolving dataset\n\n### payload flow Snapshot Distributions (concrete)\n\nThese are \"concrete information resources\", i.e. files.\n\n\n## Required Components\n\nEvery payload node must contain:\n\n- **[[mesh-resource.node-component.flow.node-metadata]]** (`_node-metadata-flow/`): Administrative metadata about the data concept\n- **[[mesh-resource.node-component.flow.payload]]** (`_payload-flow/`): dataset data\n- **[[Node handle|mesh-resource.node-component.node-handle]]** (`_node-handle/`): Referential indirection for the node\n\n## Optional Components\n\n- [[mesh-resource.node-component.flow.reference]]: metadata about the dataset\n- [[Asset trees|mesh-resource.node-component.asset-tree]] (`_assets/`): Attached file collections\n- [[mesh-resource.node-component.documentation-resource.changelog]] and [[mesh-resource.node-component.documentation-resource.readme]]\n- [[mesh-resource.node-component.node-config-defaults]]\n\n## Key Characteristics\n\n### Not a Dataset\n\n**Important**: A payload node is **not itself a (concrete) dataset**. It represents the abstract concept of a dataset that may evolve over time:\n- payload nodes are never versioned (only their component flows are)\n- payload nodes serve as stable conceptual anchors\n\n### Extensible Container\n\nLike all mesh nodes, payload nodes can contain other mesh nodes and components, making them extensible namespace containers.\n\n## Examples\n\n### Unversioned payload node\n```\nns/monsters/\nâ”œâ”€â”€ _node-metadata-flow/                 # metadata about the \"monsters\" payload node\nâ”œâ”€â”€ _node-handle/               # handle for the payload node\nâ””â”€â”€ _payload-flow/                 # single payload flow\n    â””â”€â”€ _current/               # current dataset snapshot\n        â”œâ”€â”€ monsters.jsonld     # concrete distribution of the current snapshot\n        â””â”€â”€ monsters.trig\n```\n","n":0.05}}},{"i":53,"$":{"0":{"v":"bare node","n":0.707},"1":{"v":"\n**Bare nodes** are [[mesh-resource.node]]s that contain other mesh nodes. Their [[concept.identifier]]\n\n## Function\n\n- namespace extenders and perhaps organizational containers\n\n**Mandatory Components**: `_node-metadata-flow/` + `_node-handle/`\n**Optional Components**: [[mesh-resource.node-component.flow.node-config]], [[mesh-resource.node-component.documentation-resource]]\n\n\nThey are physically represented by [[folder.node]].\n\n## Purpose\n\n- scaffolding, grouping, deferred semantics\n- a secondary, optional function is as \"semantic contextualizers\", but bare nodes don't have any definitive [[concept.referent]] of their own. \n","n":0.135}}},{"i":54,"$":{"0":{"v":"node component","n":0.707},"1":{"v":"\n## Overview\n\n**Node components** are mesh resources that support and define the mesh structure. Unlike [[mesh nodes|mesh-resource.node]] which can contain other mesh nodes, components cannot be extended beyond their own internal structure.\n\nComponents can be physically represented as folders or files, and all files and folders within a component folder are considered to be part of that component.\n\n## Component Categories\n\nComponents are categorized by their facets, including:\n  - typical creation and maintenance patterns (user vs system)\n  - versioning status\n  - folder vs. file\n  - node role (meta and data [[mesh-resource.node-component.flow]])\n\n### User Components\n\nUser components are primarily created and maintained by users or their software agents and services, and represent domain knowledge:\n\n**Folder-based user components:**\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: Collections of arbitrary files attached to the mesh (in `_assets/` folders)\n- **[[Next datasets|mesh-resource.node-component.flow-snapshot.next]]**: Draft workspaces for ongoing changes to [[mesh-resource.node-component.flow]] (in `_next/` folders)\n\n**File-based user components:**\n- **README.md files**: User documentation providing context\n- **CHANGELOG.md files**: Version history documentation\n\n### System Components\n\nSystem components are usually created or altered by the [[Weave Process|concept.weave-process]] process rather than direct user modification:\n\n**Folder-based system components:**\n- **[[metadataset flows|mesh-resource.node-component.flow.node-metadata]]**: Administrative and structural metadata for mesh nodes (in `_node-metadata-flow/` folders)\n- **[[version snapshot|mesh-resource.node-component.flow-snapshot.version]]**: Versioned snapshots of datasets (in `_vN/` folders)\n- **[[Node handles|mesh-resource.node-component.node-handle]]**: Components providing referential indirection for nodes as mesh resources (in `_node-handle/` folders)\n\n**File-based system components:**\n- **[[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]]**: Generated index.html files for human-readable access\n- **[[Distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in various RDF formats\n\n## Physical vs Logical Structure\n\n**Physical Representation:**\n- Folder-based components are represented as folders with underscore prefixes (like `_node-metadata-flow/`, `_assets/`)\n- File-based components are individual files within mesh nodes or other components\n- Component folders contain all files and folders that belong to that component\n\n**Logical Function:**\n- Components extend the namespace but are terminal (cannot contain other mesh nodes or components)\n- Components provide specialized functionality: metadata, versioning, referential data, or file attachments\n- Components maintain the semantic structure and operational capabilities of the mesh\n\n## Integration with Nodes\n\nComponents work in conjunction with mesh nodes to create the complete mesh structure:\n- Every mesh node contains at least two components: metadataset flows and node handles\n- dataset nodes contain a single [[mesh-resource.node-component.flow.payload]] \n- Any node may contain asset trees (user components) for file attachments\n","n":0.054}}},{"i":55,"$":{"0":{"v":"snapshot distribution","n":0.707},"1":{"v":"\n- each [[mesh-resource.node-component.flow-snapshot]] should have one or more distributions.\n- a snapshot's distributions should all contain the same data, just in different syntaxes \n\n## Distribution Filenaming Per Flow\n\n-  [[mesh-resource.node-component.flow.reference]], [[mesh-resource.node-component.flow.node-metadata]], [[mesh-resource.node-component.flow.node-config.operational]] and [[mesh-resource.node-component.flow.node-config.inheritable]] have their distributions named with `_ref`, `_meta`, `_config` and `_inheritable-config` respectively\n- [[mesh-resource.node-component.flow.payload]] distributions use the node slug as the base filename (no \"_data\" suffix):\n\n### Filenaming Per Snapshot\n\n- In `_current/`: `slug.ext` (e.g., `dave-bio.trig`, `dave-bio.jsonld`)\n- In `_vN/`: `slug_vN.ext` (e.g., `dave-bio_v1.trig`)\n  In `_next/`: `slug_next.ext` (e.g., `dave-bio_next.trig`)\n","n":0.115}}},{"i":56,"$":{"0":{"v":"working distribution","n":0.707},"1":{"v":"\nUnlike most other [[mesh-resource.node-component.flow-snapshot]], the [[mesh-resource.node-component.flow-snapshot.next]] is [[user-facing|facet.user]] (directly modifiable), and so there shouldn't be any [[concept.sibling-distribution]]. Just the one distribution that may be updated, and if multiple-syntax weaving is turned-on, is the source for the siblings.\n\nThe [[product.cli]] should have functionality for converting the working distribution between formats, in case you want to change the syntax you're using.\n","n":0.131}}},{"i":57,"$":{"0":{"v":"version distribution","n":0.707},"1":{"v":"\nEvery [[versioned|facet.flow.versioned]] or [[deversioned|facet.flow.deversioned]] [[mesh-resource.node-component.flow]] has at least one [[mesh-resource.node-component.flow-snapshot.version]] with a corresponding **version distribution**. \n","n":0.25}}},{"i":58,"$":{"0":{"v":"current distribution","n":0.707}}},{"i":59,"$":{"0":{"v":"node handle","n":0.707}}},{"i":60,"$":{"0":{"v":"handle resource page","n":0.577},"1":{"v":"\n- provides an accessible description of its containing [[mesh-resource.node]], which is identified with a [[mesh-resource.node-component.node-handle]]. \n- be default it could be something simple like \"For Semantic Web purposes, this IRI should be considered to connote the Semantic Mesh node itself, not the node's referent.\"\n","n":0.151}}},{"i":61,"$":{"0":{"v":"Node Config Defaults","n":0.577},"1":{"v":"\n## Overview\n\nNode config defaults are inheritable settings that provide baseline behavior for nodes. They are supplied by ancestors (and service/platform) and are resolved by a mechanism similar to that of service config config.\n\n- Inheritance mechanism: see [[mesh-resource.node-component.flow.node-config.inheritable]]\n- Operational (final) config: see [[mesh-resource.node-component.flow.node-config.operational]]\n- Folder overview for config flows: see [[mesh-resource.node-component.flow.node-config]]\n\n## Common default settings (examples)\n\n- Flow versioning: on/off (whether abstract flows create `_vN/` snapshots on weave)\n- Distribution syntaxes: preferred serializations (e.g., TriG, JSONâ€‘LD)\n- Resource pages and fragments: enable page/fragment generation; template and stylesheet selection\n- Aggregated distributions: on/off for generating top-level rollups\n- Rights & provenance defaults: copyright/licensing/attribution/delegation policies (applied at snapshot time)\n\nThese defaults apply when a node does not specify the setting in its [[mesh-resource.node-component.flow.node-config.operational]]; â€œmost specific winsâ€ from parent â†’ service â†’ platform (see [[mesh-resource.node-component.flow.node-config.inheritable]] for precedence).\n\n## Minimal guidance\n\n- Keep defaults lightweight; override at the node only when needed\n- Prefer repositoryâ€‘level templates/css in `_assets/` for consistency (see [[mesh-resource.node-component.asset-tree]])\n- Review defaults when moving/embedding meshes to ensure expected publication behavior\n","n":0.08}}},{"i":62,"$":{"0":{"v":"node flow","n":0.707},"1":{"v":"\n[[Nodes|mesh-resource.node]] are primarily constituted by their semantic flows: evolvable datasets about their node's data, metadata, configuration, or referent. They exist through time, independent of any specific version or realization, and can evolve semi-independently.\n\nThere are five types of node flows.\n\n- [[mesh-resource.node-component.flow.node-metadata]] (required)\n- [[mesh-resource.node-component.flow.node-config.operational]] (optional)\n- [[mesh-resource.node-component.flow.node-config.inheritable]] (optional)\n- [[mesh-resource.node-component.flow.reference]] (optional)\n- [[mesh-resource.node-component.flow.payload]] (for payload nodes)\n\n\n## Relationship to snapshots\n\nAs DatasetSeries, node flows are realized through [[mesh-resource.node-component.flow-snapshot]] datasets, which are temporal slices of the flow. To borrow a phrase from the PROV model, we say that a snapshot is a specialization of the node flow.\n\n### Relationship pattern:\n\nEvery node flow has at least two concrete snapshots: [[mesh-resource.node-component.flow-snapshot.current]] and [[mesh-resource.node-component.flow-snapshot.next]].\n\nThe node flow is a [DatasetSeries](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset_Series) and may have multiple [[mesh-resource.node-component.flow-snapshot.version]]s.\n\n\n### Ontology Example\n\n- node flow: \"My ontology definitions\" (persistent concept)\n- flow snapshots: v1, v2, current version, working draft of next version (specific realizations)\n\n\n```file\n/my-ontology/\nâ””â”€â”€ _payload-flow/                  â† node flow (ontology definitions)\n    â”œâ”€â”€ _current/           â† flow snapshot (in this case, probably )\n    â”œâ”€â”€ _next/           â† flow snapshot\n    â”œâ”€â”€ _v1/           â† flow snapshot\n    â””â”€â”€ _v2/                â† flow snapshot\n```\n\nIn this example:\n\nEach _current/, _v1/, etc. contains flow snapshot realizations\n\n## Persistent Identity\n\nnode flows provide conceptual continuity by:\n\n- Maintaining meaning across versions and changes\n- Preserving references from external sources\n- Enabling evolution while keeping identity stable\n- Supporting versioning without losing conceptual coherence\n","n":0.07}}},{"i":63,"$":{"0":{"v":"reference flow","n":0.707},"1":{"v":"\n## Definition\n\nA **reference flow** is a dataset series within a mesh node that provides information about the **referent** of the node â€” i.e., the thing in the world that the node stands for.\n\n**Purpose**\n\n* To describe what the node *refers to*, i.e., its referent (person, place, concept, event, dataset, etc.).\n* To supply labels and alternative identifiers for the referent.\n* To refer to other descriptive data about the referent.\n\n## Typical Contents\n\n* `rdfs:label` (human-readable name of the referent).\n* `rdf:type` (classifying what kind of thing the referent is).\n* Optional identifiers (sameAs links, external URIs).\n\n## Example Snapshot Distribution\n\n```trig\n# The referent of the node (the actual person)\n<djradon>\n    rdf:type foaf:Person ;\n    rdfs:label \"dj radon\" ;\n    foaf:mbox <mailto:djradon@example.org> ;\n```\n","n":0.095}}},{"i":64,"$":{"0":{"v":"payload flow","n":0.707},"1":{"v":"\n**payload flows** provide versionable data storage functionality within the semantic mesh architecture. \n\n## Overview\n\nA payload flow (formerly called a __dataset__ flow) is a series of RDF datasets. Like all flows, each payload flow has snapshots (_current/, _next/, _vN/) that track its evolution over time. \n\npayload flows are distinct from [[mesh-resource.node-component.flow.node-metadata]]s, which are usually managed by the platform and describe the mesh node itself and its components.\n\n## Purpose\n\npayload flows serve as the primary content containers for [[mesh-resource.node.payload]], providing:\n\n- **Content Storage**: Hold the actual dataset payload that defines the node's content\n- **History**: Support multiple versions (snapshots) of the same conceptual dataset\n- **Format Diversity**: Provide multiple format distributions (TTL, JSON-LD, etc.)\n- **State Management**: Track current, draft, and versioned states of data\n\n## Structure\n\npayload flows organize content through [[flow snapshots|mesh-resource.node-component.flow-snapshot]]:\n\n- `_current/` - Current stable version of the dataset\n- `_next/` - Draft/work-in-progress version\n- `_v1/`, `_v2/`, etc. - Versioned snapshots for historical access\n\nLike all [[facet.filesystem.folder]], they should contain an `index.html` [[mesh-resource.node-component.documentation-resource.resource-page]] -- a human-readable description for the flow.\n\n## Distribution Formats\n\nEach [[flow snapshot|mesh-resource.node-component.flow-snapshot]] typically provides multiple format distributions:\n\n- **Trig (.trig)**: Primary RDF serialization\n- **JSON-LD (.jsonld)**: JSON-compatible linked data\n- **RDF/XML (.xml or .trix)**: XML-based RDF serialization\n- **N-Quads (.nq)**: Line-based RDF format\n\n## Example\n\nFrom the [[semantic mesh example|concept.semantic-mesh.example]]:\n\n```\n/test-ns/djradon-bio/_payload-flow          # payload flow\nâ”œâ”€â”€ _current/                        # current snapshot\nâ”‚   â”œâ”€â”€ djradon-bio.ttl             # turtle distribution\nâ”‚   â”œâ”€â”€ djradon-bio.jsonld          # json-ld distribution\nâ”‚   â””â”€â”€ index.html                  # snapshot interface\nâ”œâ”€â”€ _next/                          # draft snapshot\nâ”‚   â”œâ”€â”€ djradon-bio.ttl             # draft turtle\nâ”‚   â”œâ”€â”€ djradon-bio.jsonld          # draft json-ld\nâ”‚   â””â”€â”€ index.html                  # snapshot interface\nâ””â”€â”€ index.html                      # resource page\n```\n\n## Integration\n\npayload flows integrate with other mesh components:\n\n- **metapayload flows**: Provide provenance and management data\n- **Asset Trees**: Store associated files and media\n- **Resource Pages**: Provide human-readable interfaces\n","n":0.061}}},{"i":65,"$":{"0":{"v":"metapayload flow","n":0.707},"1":{"v":"\nA **metapayload flow** contains system-related administrative and structural metadata for every [[mesh-resource.node]], including the versioning data for each node's flows.\n\nIn the filesystem, it exists as a [[folder._node-metadata-flow]] in a [[folder.node]].\n\nMesh-specific metadata about a node's flows' [[mesh-resource.node-component.flow-snapshot.version]]s mostly lives here too, eliminating the need to keep separate metadata in the node component. Also may contain metadata about the assets folder.\n\n## Use of _node-handle in metapayload flows\n\nWhen metapayload flows (or any [[facet.system]] dataset) refer to mesh nodes, they'll usually be talking about \"the-node-as-mesh-constituent\", so they'll use the node's [[mesh-resource.node-component.node-handle]] identifier\n\n## Recommended vocabulary\n","n":0.105}}},{"i":66,"$":{"0":{"v":"meshnode config flows","n":0.577},"1":{"v":"\nNode configuration is managed through two distinct flows that provide settings for a node's behavior.\n\n1.  **[[Operational Config Flow|mesh-resource.node-component.flow.node-config.operational]]**: This flow contains the final, resolved configuration that dictates how a specific node operates.\n\n2.  **[[Inheritable Config Flow|mesh-resource.node-component.flow.node-config.inheritable]]**: This flow contains settings that a node makes available to its descendants in the mesh hierarchy.\n\nWhile there are two separate flows, there is a single inheritance mechanism that resolves the final operational configuration for a node. This mechanism draws from the `inheritable` configs of parent nodes, as well as service and platform-level defaults.\n","n":0.107}}},{"i":67,"$":{"0":{"v":"operational config flow","n":0.577},"1":{"v":"\nAn **operational config flow** defines a node's final, resolved settings. It is the direct consumer of the configuration inheritance chain and controls the node's actual behavior (e.g., versioning, distribution formats).\n\nIf a node has an operational config flow, it can still inherit settings from the [[inheritance chain|mesh-resource.node-component.flow.node-config.inheritable]]. Any settings explicitly defined in the operational config will override those that would have been inherited.\n\nIf a node lacks an operational config flow, its behavior is determined by the resolved settings from the inheritance chain, with service defaults and platform defaults filling in any gaps.\n","n":0.105}}},{"i":68,"$":{"0":{"v":"inheritable config flow","n":0.577},"1":{"v":"\nAn **inheritable config flow** contains settings that a node makes available to its descendants in the mesh hierarchy. It is the primary mechanism for providing default configurations to child nodes.\n\n## Inheritance Hierarchy\n\nThe inheritance chain follows this precedence (most specific wins):\n\n1.  **Parent Node's** InheritableNodeConfig\n2.  **Grandparent Node's** InheritableNodeConfig (and so on, up the tree)\n3.  **Service-level** InheritableNodeConfig\n4.  **Platform-level** InheritableNodeConfig (ultimate fallback)\n\nThe final `OperationalNodeConfig` for a given node is resolved by merging the settings from this chain.\n\n## Resolution Algorithm\n\nWhen resolving a node's operational configuration, the system walks up the hierarchy from the node's parent, collecting `InheritableNodeConfig` at each level. These are merged, with settings from closer ancestors taking precedence.\n\n### Property-Level Inheritance\n\nConfiguration inheritance works at the property level. A child's `InheritableNodeConfig` can override a single property while still inheriting others from its parent.\n\n```jsonld\n{\n  \"@id\": \"parent:inheritableConfig\",\n  \"@type\": \"node-conf:InheritableNodeConfig\",\n  \"node-conf:versioningEnabled\": true,\n  \"node-conf:distributionFormats\": [\"application/trig\", \"application/ld+json\"]\n}\n\n{\n  \"@id\": \"child:inheritableConfig\",\n  \"@type\": \"node-conf:InheritableNodeConfig\",\n  \"node-conf:versioningEnabled\": false\n  // Inherits distributionFormats from parent\n}\n```\n\n## Configuration Control Properties\n\n### `nodeConfigInheritanceEnabled` (Child's Perspective)\n\nControls whether a node *receives* inherited configuration.\n-   **Default**: `true`\n-   **Effect**: When `false`, the node ignores the inheritance chain and uses only its own operational config or system defaults.\n\n### `inheritableConfigPropagationEnabled` (Parent's Perspective)\n\nControls whether a node *provides* its inheritable configuration to its children.\n-   **Default**: `true`\n-   **Effect**: When `false`, this node acts as a \"firewall,\" blocking its own and any ancestor's inheritable configs from flowing down to its children.\n","n":0.068}}},{"i":69,"$":{"0":{"v":"flow snapshot","n":0.707},"1":{"v":"\n**flow snapshots** are components that are datasets and represent the evolutionary steps of the [[mesh-resource.node-component.flow]].  \n\nflow snapshots have corresponding [[distributions|mesh-resource.node-component.snapshot-distribution]] and are the connective tissue between nodes and their RDF-based representation.\n\n## Relationship to node flows\n\nflow snapshots are the successive realizations of [[mesh-resource.node-component.flow]].\n\n### Relationship pattern:\n\nnode flows have at least two snapshots:\n\n- current version (`_current/`)\n- working draft (`_next`)\n- versioned snapshots\n\n### Ontology dataset node Example\n\n```file\n/my-ontology/               â† dataset node: Conceptual, data-oriented \"thing\"\nâ”œâ”€â”€ _node-metadata-flow/                   â† meta flow (metadata)\nâ”‚   â”œâ”€â”€ _current/           â† flow snapshot (current metadata)\nâ”‚   â”œâ”€â”€ _next/              â† flow snapshot (working draft)\nâ”‚   â”œâ”€â”€ _v1/                â† flow snapshot (version 1 metadata)\nâ”‚   â””â”€â”€ _v2/                â† flow snapshot (version 2 metadata)\nâ””â”€â”€ _dataset-flow/                  â† dataset node flow (ontology definition--by-dataset)\n    â”œâ”€â”€ _current/           â† flow snapshot (current definition)\n    â”œâ”€â”€ _next/              â† flow snapshot (working draft)\n    â””â”€â”€ _v1/                â† flow snapshot (version 1 definition)\n```\n\nIn this example:\n- `_current/`, `_v1/`, `_v2/`, `_next/` are all flow snapshots\n- Each contains actual data files and distributions\n- They represent specific temporal states of their parent node flows\n\n## Temporal Nature\n\nflow snapshots capture datasets at specific moments:\n\n- **Current versions** (`_current/`) - The latest version.\n- **Next versions** (`_next/`) - Draft content for future release\n- **Historical versions** (`_v1/`, `_v2/`) - Immutable snapshots from the past\n\n## Content Structure\n\nflow snapshots contain:\n- **Data files** - The actual dataset content (`.ttl`, `.rdf`, `.jsonld`)\n- **Distributions** - Multiple format representations of the same data\n- **Metadata** - Information about the specific version/snapshot\n\n### Example Structure\n```file\n_current/\nâ”œâ”€â”€ my-ontology.ttl         â† Distribution\nâ”œâ”€â”€ my-ontology.rdf         â† Distribution  \nâ””â”€â”€ my-ontology.jsonld      â† Distribution\n```\n\n## Immutability\n\n**[[mesh-resource.node-component.flow-snapshot.version]]** (historical flow snapshots, i.e., versioned folders like `_v1/`, `_v2/`) should be treated as immutable once created. This provides reliable references for external systems and ensures accurate provenance and history.\n\n**[[mesh-resource.node-component.flow-snapshot.current]]** (the latest \"woven\" flow snapshots, `_current`) should not be modified directly by users, but will be updated \"on weave\" if the [[mesh-resource.node-component.flow-snapshot.next]] has evolved. \n\n**[[mesh-resource.node-component.flow-snapshot.next]]** (working flow snapshots, `_next/`) are mutable:\n- Can be edited and updated during development\n- Represent evolving state of the node flow\n\n## Creation and Lifecycle\n\nflow snapshots are created through:\n- **Initial authoring** - Creating `_current/` content\n- **Versioning** - Snapshotting `_current/` to `_v1/`, `_v2/` during [[concept.weave]]\n- **Draft preparation** - Working in `_next/` for future releases\n\n## Related Concepts\n\n- **[[mesh-resource.node-component.flow]]** - Parent conceptual entities\n- **[[concept.versioning]]** - Process of creating versioned flow snapshots\n- **[[concept.weave-process]]** - Operation that manages flow snapshot lifecycle\n","n":0.052}}},{"i":70,"$":{"0":{"v":"version snapshot","n":0.707},"1":{"v":"\nA snapshot or checkpoint dataset generated \"on [[weave|concept.weave-process]]\"\n\n## Disambiguation\n\n- a version snapshot is an addressable resource; it is differentiated from the concept of a [[facet.flow.versioned]] in that a versioned flow is a flow that has ever had a version.\n","n":0.16}}},{"i":71,"$":{"0":{"v":"next snapshot","n":0.707},"1":{"v":"\nThe **next snapshot** serves as a draft workspace for ongoing changes to a node's [[mesh-resource.node-component.flow]]s\nAfter a version-bumping weave, a next snapshot starts identically to the current dataset but can be modified safely without affecting the stable current version. During weaving, _next content becomes the new current dataset and gets snapshotted as the latest version, while _next naturally remains ready for the next round of drafts.\n\nThis allows continuous development and version control commits without requiring immediate version bumps or disrupting users of the stable dataset.\n","n":0.109}}},{"i":72,"$":{"0":{"v":"current snapshot","n":0.707},"1":{"v":"\nThe current snapshot represents the latest published version of a dataset's content. It serves as the authoritative source that users and external systems reference, containing the most recent released data while remaining unchanged during active development.\n\nIf versioning is turned on and nobody has cleaned up old versions, the current snapshot matches the content of the latest versioned snapshot (e.g., `_v3/`) and remains identical to the [[mesh-resource.node-component.flow-snapshot.next]] until new changes begin. During weaving, the `_next` content becomes the new current snapshot. If versioning is turned on, the _next content becomes the next version.\n\nThis provides a reference point for citations and external links that want the latest information, while allowing ongoing development work to proceed safely in the `_next` dataset without disrupting users of the published data.\n","n":0.089}}},{"i":73,"$":{"0":{"v":"Documentation Resource","n":0.707}}},{"i":74,"$":{"0":{"v":"mesh resource page","n":0.577},"1":{"v":"\nTo make every folder-based resource more discoverable, they each have an index.html page that gets generate \"on [[concept.weave-process]]\"\n\n\n- primarily for humans\n\n## References\n\n- https://www.w3.org/wiki/DereferenceURI\n","n":0.209}}},{"i":75,"$":{"0":{"v":"Resource Fragment","n":0.707},"1":{"v":"\nResource fragments are [[product.service.design.htmx]] fragments, support dynamic behaviour in [[mesh-resource.node-component.documentation-resource.resource-page]] or external web apps without a \"live\" backend.\n\nFor resource pages, they're most useful for \"saving bandwidth\": data that might not be needed can be loaded later.\n\nFor external apps, they save the overhead of parsing and discovery.\n\nFragment generation can be configured per node or inherited from config hierarchy.\n\n## **Multiple Resource Fragments in Assets**\n\nThis is a natural extension of your asset tree concept (.9):\n\n```\nmesh-node/\nâ”œâ”€â”€ _assets/\nâ”‚   â”œâ”€â”€ fragments/               # Generated resource pages\nâ”‚   â”‚   â”œâ”€â”€ README.html          # generated from README.md\nâ”‚   â”‚   â”œâ”€â”€ CHANGELOG.html       # generated from CHANGELOG.md\nâ”‚   â”‚   â””â”€â”€ back-references.html # list of back-references\nâ”‚   â””â”€â”€ styles/\nâ”‚       â””â”€â”€ common.css\nâ”œâ”€â”€ _node-metadata-flow/\nâ”œâ”€â”€ CHANGELOG.md\nâ””â”€â”€ README.md\n```\n\n","n":0.097}}},{"i":76,"$":{"0":{"v":"README","n":1},"1":{"v":"\n- Provides an unstructured introduction to the containing resource\n- preferably written in Markdown. \n","n":0.267}}},{"i":77,"$":{"0":{"v":"CHANGELOG","n":1},"1":{"v":"\n- provides an unstructure history of the containing resources\n- preferably written in Markdown. \n","n":0.267}}},{"i":78,"$":{"0":{"v":"assets tree","n":0.707},"1":{"v":"\nThis node component is \"mesh-terminal\" and should contain no [[sflow-resources|mesh-resource]]. \n\nIt can be contained in any [[folder.node]], i.e., only Nodes get assets trees.\n\nIts metadata (if any) should be stored in the parent nodeâ€™s meta flow (`_node-metadata-flow/`). Asset trees are terminal and carry no flows, and are ignored by the mesh scanner.\n\nIt can contain an arbitrary set of files and folders, but two (optional) folders are special:\n- _templates can contain html files to be used when generating [[mesh-resource.node-component.documentation-resource.resource-page]] for the containing [[mesh-resource.node]] or its sub-resources.\n","n":0.109}}},{"i":79,"$":{"0":{"v":"Aggregated Distribution","n":0.707},"1":{"v":"\n__note: maybe we will do them, maybe we won't__\n - t.2025.11.08.09 probably not. Better to go the other way: from a payload dataset, create all its named nodes and link back to the original; keeps things flow-y\n- probably won't do unified distributions except via API. \n\nA node's **aggregated distribution** is a compilation of all the child flows of itself and its contained nodes (their `_payload-flow/_current/` snapshots), situated directly under the parent node with an intuitive filename like \"nodename.ext\".\n\nEssentially, it's a \"(sub-)mesh in a single file.\" \n\nPerhaps its only available via API. \n\n## Purpose\n\nAggregated distributions support [[principle.composability]] and [[principle.transposability]] by:\n- Combining contained nodes' data into a single resource\n- Supporting modular ontology and knowledge base construction\n\n## Issues\n\nconfig options\n- zipping/compression?\n- user data only, or include metadata/config\n\n## Generation Process\n\nDuring [[concept.weave-process]], aggregated distributions are created by:\n1. **Scanning contained payload nodes** recursively within the mesh structure\n2. **Collecting `_payload-flow/_current/` distributions** from each flow\n3. **Merging content** with proper URI resolution and prefix handling\n4. **Excluding `_config` and `_meta` datasets** (data content only)\n5. **Generating multiple distributions** (.ttl, .rdf, .jsonld) as configured\n\n## Examples\n\n### Composable Ontology\n```\n/my-ontology/\nâ”œâ”€â”€ my-ontology.ttl              â† Aggregated distribution\nâ”œâ”€â”€ my-ontology.rdf              â† Aggregated distribution  \nâ”œâ”€â”€ my-ontology.jsonld           â† Aggregated distribution\nâ”œâ”€â”€ Person/                  â† payload node (class definition)\nâ”œâ”€â”€ hasName/                 â† payload node (property definition)\nâ””â”€â”€ Organization/            â† payload node (class definition)\n```\n\n### Knowledge Base\n```\n/biotech-kb/\nâ”œâ”€â”€ biotech-kb.ttl               â† Aggregated distribution\nâ”œâ”€â”€ biotech-kb.jsonld            â† Aggregated distribution\nâ”œâ”€â”€ companies/\nâ”‚   â”œâ”€â”€ genentech/               â† Company payload node\nâ”‚   â””â”€â”€ moderna/                 â† Company payload node\nâ””â”€â”€ products/\n    â”œâ”€â”€ drug-x/                  â† Product payload node\n    â””â”€â”€ vaccine-y/               â† Product payload node\n```\n\n## Technical Considerations\n\n**Merging logic handles:**\n- **Relative path resolution** - Converting relative URIs to absolute\n- **Prefix consolidation** - Deduplicating namespace declarations\n- **Graph merging** - Combining RDF graphs from multiple sources; de-duplicating\n- **Base URI handling** - Ensuring consistent URI resolution\n\n## Use Cases\n\n- **Ontologies** - Classes and properties from contained nodes\n- **Vocabularies** - Terms and definitions from specialized nodes  \n- **Catalogs** - Dataset metadata from multiple sources\n- **Knowledge bases** - Facts distributed across domain-specific nodes\n- **Configuration data** - Settings aggregated from component services\n\n## Related Concepts\n\n- **[[mesh-resource.node-component.flow.payload]]** - Source datasets for aggregation\n- **[[concept.weave-process]]** - Process that generates aggregated distributions\n- **[[mesh-resource.node-component.flow-snapshot]]** - Contains the actual distributions being aggregated\n","n":0.054}}},{"i":80,"$":{"0":{"v":"guides","n":1}}},{"i":81,"$":{"0":{"v":"Status","n":1},"1":{"v":"\n# Project Status Overview\n\nThis document provides a high-level summary of the current project state, what is currently working, and any major known issues.\n\n## Current State Summary\n\nThe core Semantic Flow architecture is defined, focusing on the Git-native, filesystem-based mesh structure. The primary development focus is currently on establishing the agent memory bank system and refining the core concepts and documentation.\n\n## Working Components\n\n- **Mesh Structure:** The folder hierarchy to IRI mapping is established.\n- **Documentation Site:** The Dendron-based documentation site is functional.\n- **Host Application:** The `sflo-host` Fastify application structure is in place.\n- **Core Concepts:** [[concept.summary]] provides a comprehensive overview of the system.\n\n## Known Issues / Next Focus\n\n- **Weave Process:** The full implementation of the weave process (versioning, promotion, link resolution) is pending.\n- **Configuration:** The two-flow configuration inheritance model is defined but requires implementation.\n- **Agent Integration:** Establishing robust agent workflows and custom mode rules is the current priority.\n\n## Related Status Files\n\n- [[now]] - Detailed focus of current work (big picture)\n- [[todo]] - General list of pending tasks\n- [[progress]] - Log of completed work\n- [[decision-log]] - Log of important project decisions\n","n":0.075}}},{"i":82,"$":{"0":{"v":"Project Brief","n":0.707},"1":{"v":"\n# Semantic Flow (sflo) - Project Brief\n\nThis is the entry point for understanding the Semantic Flow project and its memory bank system.\n\n## Essential Reading (Start Here)\n\n**For AI Agents:** You MUST use the [[dev.memory-bank]] protocol and read the \"Every Task\" Context Files for every new task\n\n## Key Concepts\n\nCore abstractions are documented in detail:\n\n- [[concept.mesh]] - Dereferenceable collection of semantic resources\n- [[mesh-resource.node]] - Atomic unit of a mesh; provides a name to refer to something, and optionally, data about that thing\n- [[mesh-resource.node-component.flow]] - data about the node or thing it names, in the abstract; possibly versioned\n- [[mesh-resource.node-component.flow-snapshot]] - a version of a flow\n- [[mesh-resource.node-component.snapshot-distribution]] - a file that concretizes a version of a flow\n- [[concept.weave-process]] - Lifecycle operation for versioning/publishing\n\nSee [[concept.summary]] for more depth documentation.\n\n\n\n","n":0.09}}},{"i":83,"$":{"0":{"v":"Product Brief","n":0.707},"1":{"v":"\n# Semantic Flow (sflo) - Product Brief\n\n## What is Semantic Flow?\n\nSemantic Flow is a platform for creating, managing and publishing **semantic meshes** - dereferenceable, versioned collections of data resources where every IRI resolves to meaningful content.\n\n## Twin Purposes\n\n- **Mint dereferenceable IRIs** for referring to things on the Semantic Web\n- **Hold versionable semantic data** that uses those IRIs and can be referenced by other semantic data\n\n## Problems Solved\n\n- **Free, Permanent, Self-Sovereign Data Storage** - Provides individuals with free, permanent, self-describing data storage via the git provider of their choice\n- **IRI Stability** - Provides stable, dereferenceable IRIs for Semantic Web resources\n- **Version Management** - Tracks semantic data evolution with immutable version history\n- **Content Dereferenceability** - Every IRI resolves to meaningful HTML content\n- **Transposability** - Meshes can be moved between domains/projects without breaking internal links\n- **Composability** - Submeshes can be extracted and composed into larger structures\n\n## Components & Applications\n\n### sflo-host\n\nThe main host application that supports semantic mesh use and development. Built with:\n- Fastify web framework\n- Plugin architecture for extensibility\n- TypeScript for type safety\n\n#### Plugins\n\n- [[product.plugins.api-docs]] - API documentation/playground (Stoplight Elements)\n- [[product.plugins.mesh-server]] - Static mesh server(s)\n- [[product.sflo-web]] - Web UI\n- [[product.plugins.sflo-api]] - OpenAPI REST endpoint\n- [[product.plugins.sparql-ro]] - SPARQL read-only endpoint\n- [[product.plugins.sparql-update]] - SPARQL write-capable endpoint (provided by Comunica)\n- [[product.plugins.sparql-editor]] - SIB Swiss editor at /play\n\n### Shared Packages\n\n- **@semantic-flow/config** - Configuration management\n\n## How It Works\n\n### Filesystem-Based Meshes\n\nMeshes map directly from Git repository folder hierarchies to published static sites:\n\n- Every folder is a **node** (container for resources and child nodes)\n- Nodes contain **components** (flows, handles, assets, documentation)\n- **Flows** are versioned DatasetSeries (metadata, semantic data, arbitrary datasets, or config)\n- **Snapshots** are flow realizations (`_current/`, `_next/`, `_vN/`)\n- **Distributions** are serialization files (TriG, JSON-LD, etc.)\n\n### The Weave Process\n\nThe weave process maintains mesh coherence and publication readiness:\n\n- Ensures required system components exist\n- Creates new version snapshots from working data\n- Promotes working data to current\n- Updates metadata and provenance\n- Regenerates resource pages\n- Resolves internal links for transposability\n\n## User Experience Goals\n\n### For Developers\n\n- **Git-native workflow** - Meshes are just Git repositories\n- **Static site deployment** - Push to GitHub Pages or any static host\n- **Type-safe development** - TypeScript throughout\n- **Plugin extensibility** - Extend functionality through plugins\n\n### For Semantic Web Users\n\n- **Dereferenceable IRIs** - Every IRI resolves to content\n- **Version history** - Immutable snapshots for precise citation\n- **Human-friendly** - Resource pages provide context and navigation\n- **Machine-readable** - RDF distributions for automated processing\n\n### For AI Agents\n\n- **Clear structure** - Predictable folder/file organization\n\n## Related Documentation\n\n- [[concept.summary]] - Comprehensive concept documentation\n- [[concept.mesh]] - Mesh definition and requirements\n- [[concept.weave-process]] - Weave process details\n- [[principle.transposability]] - Transposability principle\n- [[principle.composability]] - Composability principle\n","n":0.049}}},{"i":84,"$":{"0":{"v":"Ontologies","n":1}}},{"i":85,"$":{"0":{"v":"Best Practices","n":0.707},"1":{"v":"\n## Use Relative Identifiers for Intramesh References\n\nSee \n","n":0.354}}},{"i":86,"$":{"0":{"v":"mesh folder","n":0.707},"1":{"v":"\nSemantic meshes are represented in filesystems as a collection of folders and files. Mesh folders correspond to [[facet.resource.naming]].\n\nWith the exception of [[folder._assets]], **mesh folders** should only contain other [[mesh-resource]]s, i.e., other mesh folders and [[file]], no \"arbitrary files.\"\n","n":0.162}}},{"i":87,"$":{"0":{"v":"node folder","n":0.707},"1":{"v":"\n## Definition\n\nA node folder is any folder that maps to a [[mesh-resource.node]]. Each node folder extends the namespace with its name and has a concept IRI that may refer to something. \n\n- What a â€œnamespaceâ€ is: see [[concept.namespace]]\n- General node types and anatomy: see [[mesh-resource.node]]\n\n## Minimal requirements\n\n- Every node folder must contain:\n  - [[_node-handle/|folder._node-handle]]\n  - [[_node-metadata-flow/|folder._node-metadata-flow]]\n\n## Node-specific flows (by type)\n\n- [[bare node|mesh-resource.node.bare]]: no additional flows \n- [[dataset node|mesh-resource.node.payload]]: requires [[_dataset-flow/|folder._dataset-flow]]\n\nDistributions must live inside flow snapshot folders (e.g., `_current/`, `_vN/`). See [[resource.node-component.flow]] and [[resource.node-component.flow-snapshot]].\n\n## Example\n\n```file\n/my-node/                     # node folder â†’ https://ex.org/my-node/\nâ”œâ”€â”€ _node-handle/             # required\nâ”œâ”€â”€ _node-metadata-flow/               # required\nâ””â”€â”€ _dataset-flow/               # required for dataset nodes\n","n":0.1}}},{"i":88,"$":{"0":{"v":"version snapshot folder","n":0.577},"1":{"v":"\n- the name of [[concept.mesh.dataset-version]] folders\n","n":0.408}}},{"i":89,"$":{"0":{"v":"_reference-flow folder","n":0.707},"1":{"v":"\nThe filesystem folder of a [[mesh-resource.node-component.flow.reference]] dataset.\n","n":0.378}}},{"i":90,"$":{"0":{"v":"payload flow folder","n":0.577},"1":{"v":"\nThe filesystem folder of a [[mesh-resource.node-component.flow.payload]] dataset\n","n":0.378}}},{"i":91,"$":{"0":{"v":"_node-metadata-flow folder","n":0.707},"1":{"v":"\nThe filesystem container of the [[mesh-resource.node-component.flow.node-metadata]]\n","n":0.408}}},{"i":92,"$":{"0":{"v":"_node-handle folder","n":0.707},"1":{"v":"\nEvery [[folder.node]] contains a **node handle folder** which corresponds to its [[resource.node-component.node-handle]]. \n\n","n":0.277}}},{"i":93,"$":{"0":{"v":"next snapshot folder","n":0.577},"1":{"v":"\nPhysical manifestation of [[mesh-resource.node-component.flow-snapshot.next]]\n\nThese folders should only contain a single \"[[Working|mesh-resource.node-component.snapshot-distribution.working]]\", i.e., a single syntax\n","n":0.258}}},{"i":94,"$":{"0":{"v":"current snapshot folder","n":0.577},"1":{"v":"\n- an [[mesh-resource.node-component.flow-snapshot]]\n","n":0.577}}},{"i":95,"$":{"0":{"v":"config flow folder","n":0.577},"1":{"v":"\nThe physical representation of the [[mesh-resource.node-component.flow.node-config.operational]]\n","n":0.408}}},{"i":96,"$":{"0":{"v":"_config Inheritable Flow","n":0.577},"1":{"v":"\nThe physical representation of the [[mesh-resource.node-component.flow.node-config.inheritable]]\n","n":0.408}}},{"i":97,"$":{"0":{"v":"assets tree folder","n":0.577},"1":{"v":"\n- correspond to [[mesh-resource.node-component.asset-tree]]s \n","n":0.447}}},{"i":98,"$":{"0":{"v":"mesh file","n":0.707}}},{"i":99,"$":{"0":{"v":"Features","n":1}}},{"i":100,"$":{"0":{"v":"Handling Renaming","n":0.707},"1":{"v":"\n- if a namespace-iri is changed, gulp, the old one can be preserved with a \"redirect\" predicate in the [[mesh-resource.node-component.flow.node-metadata]] and a warning on the html page\n","n":0.192}}},{"i":101,"$":{"0":{"v":"Check Namespace before Creating","n":0.5},"1":{"v":"\n- when creating a new repo, sflow should check whether there's an existing folder with the same name in the parent user/org site.\n- best practice is probably not to put an sflow-site at the user/org level? At least if it might have repos someday.\n\n## References\n\n- [[issue.github-bare-namespace-can-overlap-with-repo-namespaces]]\n","n":0.147}}},{"i":102,"$":{"0":{"v":"Changing Historical Datasets","n":0.577},"1":{"v":"\n- it's better if you don't have to, but if you do... \n  - maybe hashes should be stored for distributions, so people can detect that they've changed.","n":0.189}}},{"i":103,"$":{"0":{"v":"FAQ","n":1},"1":{"v":"\n# Semantic Flow Frequently Asked Questions\n\nThis section addresses common questions about Semantic Flow design principles and architecture.\n\n## Design Principles\n\n### [[Why don't bare nodes have reference flows?|faq.why-dont-namespace-and-data-nodes-have-reference-flows]]\nWhy should a namespace have to refer to something?\n\n### [[Why are there components at the top of a repo?|faq.why-are-there-components-at-the-top-of-a-repo]]\nThe repository root can be a mesh node, in which case `_node-metadata-flow/` and `_node-handle/` components appear at the top level. \n\n## Architecture Questions\n\n*More FAQ entries will be added as common questions arise.*\n\n---\n\n**Contributing to FAQ**: If you encounter questions that would benefit from clear explanations, consider adding them to this FAQ section following the established pattern.\n","n":0.101}}},{"i":104,"$":{"0":{"v":"Why Not Use Git Semantics for Versioning","n":0.378},"1":{"v":"\n// TODO","n":0.707}}},{"i":105,"$":{"0":{"v":"Why Dont payload nodes Contain Distributions Directly","n":0.378},"1":{"v":"\n## Question\n\nWhy don't [[payload nodes|mesh-resource.node.payload]] contain distribution files directly? Why do I need to go to `_current/` to find the actual data?\n\n## Answer\n\npayload nodes represent **abstract data concepts**, not concrete data instances. This separation provides several important benefits:\n\n### Clear Semantic Distinction\n\n- **payload node** (`/ns/monsters/`): \"The concept of monster data\"\n- **Data compound** (`/ns/monsters/_payload-flow/`): \"The abstract dataset associated with the monster data concept\" \n- **Data compound layers**: the current, next and historical versions of the dataset\n\nThis allows you to reference the concept separately from the associated abstract or concrete dataset.\n\n### Stable Identity\n\nThe payload node and data compound provide permanent, stable identifier for the concept and its payload dataset that persist even as the concrete data changes over time. You can always refer to \"monster data as a concept\" using `/ns/monsters/` regardless of how many versions exist.\n\n### Temporal Organization\n\nBy separating the concept from concrete instances, payload nodes can cleanly organize different temporal states:\n- `_current/` - current data\n- `_next/` - draft changes  \n- `_v1/`, `_v2/` - historical versions\n\n### Consistent Architecture\n\nThis mirrors how [[reference nodes|concept.mesh.resource.node.reference]] work:\n- **Reference nodes**: Abstract entity concept + `_ref/` node component with concrete data\n- **payload nodes**: Abstract data concept + `_current/` component with concrete data\n\n### Metadata Separation\n\nThe payload node's [[metapayload flow|mesh-resource.node-component.flow.node-metadata]] contains system metadata about the data concept and its components, while each [[mesh-resource.node-component.flow.payload]] can also contain (concept-specific) metadata.\n\nTODO: example\n\n\n## Analogy\n\nThink of it like a library:\n- **payload node** = \"The concept of the Encyclopedia Britannica\"\n- **payload flow** = The Encyclopedia Britannica as an ongoing series of editions\n- **[[mesh-resource.node-component.flow-snapshot]]** = Specific editions (1990 edition, 2020 edition, current edition)\n\nYou can refer to \"Encyclopedia Britannica\" as a general concept or as a series without specifying which edition, or you can reference a specific edition when you need concrete data.\n","n":0.059}}},{"i":106,"$":{"0":{"v":"Why are there components at the top of a repo?","n":0.316},"1":{"v":"\n## Question\n\nWhy are there [[mesh node components|mesh-resource.node-component]] like `_meta/`, `_node-handle/`, and `_assets/` at the top level of a repository? Shouldn't components only be inside nodes?\n\n## Answer\n\nComponents at the repository root exist because **the repository root itself is a [[mesh node|mesh-resource.node]]** - specifically, it's the [[root node|concept.root-node]] of the mesh.\n\n### Repository Root = Mesh Root Node\n\nEvery semantic mesh has a root node, and in a repository-based mesh, the repository root **is** that root node. It's a \"nameless\" node locally (represented as \"/\") that can be any type of mesh node:\n\n- **bare node**: If the repo organizes other nodes\n- **dataset node**: If the repo represents a single dataset  \n- **Reference node**: If the repo represents an external entity\n\nSince the repository root is a mesh node, it follows the same rules as any other node and must contain:\n\n- **`_meta/`**: corresponds to the [[mesh-resource.node-component.flow.node-metadata]] with administrative metadata for the root node\n- **`_node-handle/`**: corresponds to [[node handle|resource.node-component.node-handle]] for referential indirection\n\nThe root node may contain **other components**: Depending on the root node type (e.g., `_ref/` for reference nodes, `_data/` for versioned datasets)\n\n### Consistency Principle\n\nThis maintains architectural consistency: **every mesh node has the same structure and capabilities**, whether it's nested deep in the hierarchy or at the repository root. The root node isn't special - it's just the top-level node in the mesh hierarchy.\n\n### Mesh Self-Containment\n\nThis design also supports the principle that **any subtree is a complete mesh**. The repository root, being a proper mesh node with all its components, ensures the entire repository is a self-contained, functional semantic mesh.\n","n":0.063}}},{"i":107,"$":{"0":{"v":"What Is the referential difference between a payload node's IRI and its payload flow's IRI","n":0.258},"1":{"v":"\nA **payload node IRI** (e.g. `ns/djradon/bio/`) identifies the **abstract dataset itself** â€” it's a dereferenceable identifier for an RDF dataset\nA **payload flow IRI** (e.g. `ns/djradon/bio/_payload-flow`) identifies the **semantic flow that captures the evolution of that dataset** â€” metadata about *how* the payload is produced, validated, versioned, or transformed.\n\n| Aspect                        | Payload Node (`ns/djradon/bio/`)                                             | Payload Flow (`ns/djradon/bio/_payload-flow`)                      |\n| ----------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n| **Referential role**          | Refers to the dataset *as content*                                           | Refers to the datasetâ€™s *production process*                       |\n| **Ontological category**      | `sflo:PayloadNode` (or equivalent)                                           | `sflo:PayloadFlow` (a subclass of `sflo:Flow`)                     |\n| **Nature**                    | Public, stable identifier for consumers                                      | Internal, operational metadata for publishers                      |\n| **Dereferencing expectation** | Returns the current dataset or its default distribution (`index.trig`, etc.) | Returns RDF describing the flowâ€™s inputs, transformations, outputs |\n| **Persistence**               | Semi-permanent, versioned through dataset series (`_series/_main/v1/...`)    | May change across builds, describing how payload evolves           |\n| **Relations**                 | `sflo:hasPayloadFlow <_payload-flow>`                                        | `sflo:producesPayload <bio/>` (inverse)                            |\n\nIn short:\n\n* **`/bio/`** = â€œthe data about Bio.â€\n* **`/bio/_payload-flow`** = â€œthe process that emits `/bio/`.â€\n\nThe node is the *what*; the flow is the *how*.\n","n":0.074}}},{"i":108,"$":{"0":{"v":"Should Data about a Referent Go in Its Reference Flow or a Contained Payload Flow","n":0.258}}},{"i":109,"$":{"0":{"v":"Reference Iri Choices","n":0.577},"1":{"v":"\nRDF supports different, confusingly-named approaches to resource referencing, each with tradeoffs.\n\n\n## TLDR: **Choosing between approaches:**\n\n- Use **relative-path relative IRIs** for maximum composability when embedding or importing meshes and submeshes\n- Use **absolute-path relative IRIs** for clearer namespace context and better support for moving submeshes within the same mesh hierarchy\n- Use **absolute IRIs** only for cross-mesh references\n- Relative and absolute paths both preserve relationships when moving complete meshes between domains\n\n\n\n## Absolute IRI References\n\nAny IRI that has a scheme (e.g., http:) is an **Absolute IRI** \n\n### Example\n\nThis example uses two absolute IRIs, one using the \"ex:\" prefix for the example.com authority:\n\n```ttl\n@prefix ex: <https://example.com/> .\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_reference-flow/_current/djradon_ref.trig\nex:djradon a foaf:Person ;\n   rdfs:seeAlso ex:djradon/index.html .\n```\n\n### Pros\n\n- explicit\n  \n### Cons\n  \n-  limits [[principle.transposability]] and [[principle.composability]]\n  - e.g., if you moved mesh hosting away from `https://example.com`, the `foaf:Person` and `rdfs:seeAlso` assertions would still refer to the original references\n- not ideal if you're:\n  - making updates\n  - working offline\n\n\n## Relative IRIs\n\nAny IRI that lacks a scheme (e.g., http:) is resolved against a base IRI following RFC 3986. Such relative IRI references come in three distinct forms:\n\n- Network-path reference â€” begins with //.\n  - Example: //other.org/x â†’ inherits the baseâ€™s scheme, e.g. http://other.org/x.\n\n- Absolute-path reference â€” begins with / but not //.\n   - Example: /foo/bar â†’ keeps the baseâ€™s scheme and authority, resets the path, e.g. http://example.org/foo/bar.\n\n- Relative-path reference â€” does not begin with / or //.\n   - Example: foo/bar or ../foo â†’ inherits the baseâ€™s scheme, authority, and path context, e.g. http://example.org/base/foo/bar.\n\nIf no base is specified, an inferred base of the requested scheme and authority is used. **This behaviour is essential to Semantic Flow [[Best Practices|guide.best-practices]].**\n\n\n### Relative-Path Relative IRIs\n\n\n```turtle\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_reference-flow/_current/djradon_ref.trig\n<../../../djradon/> a foaf:Person ;          # The document itself\n   foaf:knows <../../alice/> ;           # A sibling node in the mesh\n   rdfs:seeAlso <../bio/bio.html> .      # A resource page contained in a \"bio\" node under ../../djradon/\n```\n\n#### Pros\n\n- maximum composability\n\n#### Cons\n\n- `../../../` makes eyes swim\n\n### Absolute-Path Relative IRIs\n  \n```turtle\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n# In ns/djradon/_reference-flow/_current/djradon_ref.trig\n</ns/djradon/> a foaf:Person ;\n   foaf:knows </ns/alice/> ;          # Clear namespace context\n   rdfs:seeAlso </ns/djradon/bio/bio.html> .\n```\n\n\n#### Pros\n\n- clearer context\n- good intra-mesh transposability\n\n\n#### Cons\n\n- composability requires re-computing paths\n","n":0.053}}},{"i":110,"$":{"0":{"v":"How Is a Flow Different from a Dataset Series","n":0.333},"1":{"v":"\n- a [[mesh-resource.node-component.flow]] IS a dataset series, but\n  - it has  \n- a dataset series isn't necessarily a succession of versions of the same referent.\n  - e.g. US Census Data (2010â€“2020 releases)\n","n":0.177}}},{"i":111,"$":{"0":{"v":"Do dataset nodes support DatasetSeries?","n":0.447},"1":{"v":"\nA [[mesh-resource.node.payload]] can only support a single-file distribution, but DatasetSeries can be represented inline in a single graph, or in multiple graphs using named graphs.\n\nNote that not all RDF serialization formats support named graphs.\n","n":0.171}}},{"i":112,"$":{"0":{"v":"facets","n":1},"1":{"v":"\nA way of categorizing a [[concept.platform-element]]\n","n":0.408}}},{"i":113,"$":{"0":{"v":"user facet","n":0.707},"1":{"v":"\n[[Platform elements|concept.platform-element]] with the **user facet** are things that are primarily created or altered by users of the system.\n","n":0.229}}},{"i":114,"$":{"0":{"v":"system facet","n":0.707},"1":{"v":"\n[[concept.platform-element]]s with the **system facet** are things that are primarily managed by the system.\n","n":0.267}}},{"i":115,"$":{"0":{"v":"mesh resource facets","n":0.577}}},{"i":116,"$":{"0":{"v":"naming resource","n":0.707},"1":{"v":"\nA resource whose slash-terminated [[concept.identifier]] establishes a name within a mesh, optionally denoting a non-file thing. Naming resources may provide namespace context for contained resources and may or may not have a referent.\n","n":0.174}}},{"i":117,"$":{"0":{"v":"dataset facet","n":0.707},"1":{"v":"\nIn the RDF universe, a dataset is a collection of one or more RDF graphs.\n\nIn a [[concept.mesh]], there are two kinds of datasets:\n\n- [[mesh-resource.node-component.flow]]s are dcat:DatasetSeries (which are also dcat:Dataset) and represent an \"abstract dataset\": they don't have concrete distributions of their own. \n  - Flow data is only materialized in [[mesh-resource.node-component.flow-snapshot]] distributions.\n  - Metadata about the various flows is consolidated in the node's [[mesh-resource.node-component.flow.node-metadata]] in the form of distributions.\n  - [[mesh-resource.node.payload]]s contain a \"payload flow\" and their [[concept.identifier.intramesh]]s refer to that payload dataset in the abstract. The dataset is still contained in a particular kind of [[mesh-resource.node-component.flow]], i.e., a [[mesh-resource.node-component.flow.dataset]]. But the flow's identifier refers to a particular flow, whereas the node identifier will become the canonical IRI of the dataset (on [[concept.publication]]).\n    - in the case of [[mesh-resource.node-component.flow.dataset]], they can store their Dataset metadata in the dataset itself, or in their [[mesh-resource.node-component.flow.reference]], or both.\n  \n- [[mesh-resource.node-component.flow-snapshot]]s are dcat:Dataset and represent a specific version of their abstract datasets; they have one or more [[snapshot distributions|mesh-resource.node-component.snapshot-distribution]]. \n\n","n":0.077}}},{"i":118,"$":{"0":{"v":"Content","n":1},"1":{"v":"\nA resource whose filename-terminated [[concept.identifier]] refers to an information resource \n","n":0.302}}},{"i":119,"$":{"0":{"v":"internal facet","n":0.707},"1":{"v":"\n[[concept.identifier]]s and [[concept.referent]] with the **internal facet** denote things that aren't contained in a mesh, i.e., [[mesh-resource.node-component]] or, with \n","n":0.224}}},{"i":120,"$":{"0":{"v":"flow facets","n":0.707}}},{"i":121,"$":{"0":{"v":"versioned flow facet","n":0.577},"1":{"v":"\nA [[mesh-resource.node-component.flow]] whose versions are being kept around. \n\nPhysically, the historical versions are located in \"_vN\" folders, e.g. \"_v1\" or \"_v9999\". \n","n":0.213}}},{"i":122,"$":{"0":{"v":"v-series flow facet","n":0.577},"1":{"v":"\nFlows with the **v-series flow facet** have at least one historical checkpoint, i.e. at some point a [[concept.weave-process]] has generated a [[mesh-resource.node-component.flow-snapshot.version]].\n\n\nA [[facet.flow.versioned]] collects [[Version|mesh-resource.node-component.flow-snapshot.version]], so a v-series flow probably had versioning turned on for at least one weave.\n","n":0.16}}},{"i":123,"$":{"0":{"v":"unversioned flow facet","n":0.577},"1":{"v":"\nAn unversioned node flow has never had [[concept.versioning]] turned on for a [[concept.weave-process]], so it doesn't have any [[mesh-resource.node-component.flow-snapshot.version]]\n\nIt's useful for datasets that shouldn't change much.\n","n":0.196}}},{"i":124,"$":{"0":{"v":"de-versioned flow facet","n":0.577},"1":{"v":"\nDe-versioned node flows are [[mesh-resource.node-component.flow]] that were once versioned, but have had their versioning turned off. So there are probably [[facet.flow.v-series]].\n","n":0.218}}},{"i":125,"$":{"0":{"v":"filesystem facets","n":0.707}}},{"i":126,"$":{"0":{"v":"folder resource facet","n":0.577},"1":{"v":"\nA mesh when stored in a filesystem is physically structured with mesh folders, which correspond to RDF resources and their [[concept.identifier.intramesh]]\n  \nWhen a mesh gets published, the folders also correspond to [[concept.identifier]]. \n\nAll folder-based resources should contain a [[mesh-resource.node-component.documentation-resource.resource-page]]\n\n\n## Types\n\n### System Folders\n\n#### Node Handle Folders\n\n- [[concept.mesh.resource.folder._node-handle]] correspond to the [[mesh-resource.node-component.node-handle]]\n\n#### Flow (Abstract Dataset) Folders\n\n- **`_node-metadata-flow/`**\n  - correspond to [[mesh-resource.node-component.flow.node-metadata]]\n  - present in all mesh nodes\n  \n- **`_dataset-flow/`**\n\n  - correspond to the [[mesh-resource.node-component.flow.dataset]]\n  - contain the dataset associated with the [[mesh-resource.node.payload]]\n\n#### Snapshot (Concrete Dataset) System Folders\n\n- **`_current/`**\n\n- **`_v1/`, `_v2/`, â€¦**\n\n  - Version snapshot folders that represent [[mesh-resource.node-component.flow-snapshot]]\n  - each holds one or more distribution file (named `<node_ref_vN.ext`).\n  - **Fully terminal**â€”neither user-nodes nor system-folders may live inside.\n\n#### Snapshot User Folders\n\n- **`_next/`**\n  - Where edits get made to [[facet.flow.versioned]]\n\n\n#### Other User Folders\n\n- **`_assets/`**\n  - Holds static user assets (images, CSS, binaries).\n  - **Always terminal** - never contains nodes\n  - Ignored by the mesh scanner; asset trees carry no flows; any metadata about assets should live in the parent nodeâ€™s meta flow.\n","n":0.077}}},{"i":127,"$":{"0":{"v":"file resource facet","n":0.577},"1":{"v":"\nResource files are returned directly when accessed by their [[concept.identifier]].\n\n## Types\n\n- [[mesh-resource.node-component.documentation-resource]]\n  - [[mesh-resource.node-component.documentation-resource.resource-page]] (system-generated)\n  - [[mesh-resource.node-component.documentation-resource.changelog]]\n  - [[mesh-resource.node-component.documentation-resource.readme]]\n- [[mesh-resource.node-component.snapshot-distribution]]\n- [[mesh-resource.node-component.aggregated-distribution]]\n- ","n":0.218}}},{"i":128,"$":{"0":{"v":"external facet","n":0.707},"1":{"v":"\n[[concept.identifier]]s and [[concept.referent]] with the **external facet** denote things \"in the world\", whether real or imaginary.\n","n":0.25}}},{"i":129,"$":{"0":{"v":"Decision Log","n":0.707},"1":{"v":"\n# Project Decision Log\n\nThis document records important project-level decisions, organized by date.\n\n## YYYY-MM-DD\n\n### Decision: [Brief title of the decision]\n\n**Context:** [Why the decision was necessary.]\n\n**Outcome:** [The chosen path and rationale.]\n\n**Impact:** [How this decision affects the project.]\n\n---\n\n## 2025-11-04\n\n### Decision: Implementation of Agent Memory Bank System\n\n**Context:** To improve AI agent performance and maintain context across tasks, a structured, git-based memory bank system was required.\n\n**Outcome:** The memory bank system was implemented using a set of dedicated Markdown files in the `documentation/` directory, categorized into \"Every Task Context\" and \"Frequently Referenced\" files.\n\n**Impact:**\n- Agents are now required to read core context files at the start of every task.\n- Task-specific context is maintained in dedicated `tasks.YYYY-MM-DD-task-name.md` files.\n- Documentation structure is standardized to avoid repetition and improve navigability (e.g., [[guide.project-brief]] acts as a directory, not a content duplicator).\n\n### Decision: RDF Serialization Format\n\n**Context:** Choosing a primary RDF serialization format for instance data and ontologies.\n\n**Outcome:** JSON-LD was chosen over Turtle/TriG due to its native support for slash-terminated CURIEs, which aligns with the project's IRI naming conventions for distinguishing between files and resource names.\n\n**Impact:** All RDF instance data and ontologies should primarily use JSON-LD.\n","n":0.074}}},{"i":130,"$":{"0":{"v":"Concepts","n":1},"1":{"v":"\n## Overview\n\nIn the Semantic Flow framework, semantic meshes are collections of namespaced units of meaning called \"mesh nodes.\" Mesh nodes are containers that hold flows and extend the namespace. [[Node components|mesh-resource.node-component]] are terminal resources that define or support a nodeâ€™s structure. The IRI itself is the sign, while the node is the dereferenceable resource that embodies, describes, and organizes that sign.\n","n":0.128}}},{"i":131,"$":{"0":{"v":"Semantic Mesh","n":0.707}}},{"i":132,"$":{"0":{"v":"Example Mesh Hierarchy","n":0.577},"1":{"v":"\n```file\n/test-ns/                                        # bare node\nâ”œâ”€â”€ _node-metadata-flow/                                       # node flow (metadata)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ ns_meta.trig                         # system metadata about the bare node\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ _assets/                                     # asset tree\nâ”‚   â”œâ”€â”€ images/\nâ”‚   â”‚   â””â”€â”€ logo.svg\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ””â”€â”€ index.html                                   # resource page\n\n/test-ns/djradon/                                # dataset node  \nâ”œâ”€â”€ _node-handle/                                     # handle component\nâ”‚   â””â”€â”€ index.html                               # mesh node handle page\nâ”œâ”€â”€ _dataset-flow/                                       # node flow (data)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ _node-metadata-flow/                                       # node flow (metadata)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ djradon_meta.trig                    # system metadata, verification status\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ _assets/                                     # asset tree\nâ”‚   â”œâ”€â”€ profile-photo.jpg\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ index.html                                   # resource page\nâ”œâ”€â”€ CHANGELOG.md                                 # resource changelog\nâ””â”€â”€ README.md                                    # resource documentation\n\n/test-ns/djradon/bio/                            # dataset node (unversioned dataset)\nâ”œâ”€â”€ _dataset-flow/                                       # node flow (data)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ djradon-bio_data.trig                      # biographical data distribution\nâ”‚   â”‚   â”œâ”€â”€ djradon-bio_data.jsonld                   # alternative distribution\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â”œâ”€â”€ djradon-bio_data.trig                      # draft biographical data\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ _node-metadata-flow/                                       # node flow (metadata)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ djradon-bio_meta.trig                # dataset metadata, provenance\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â”œâ”€â”€ djradon-bio_meta.trig                # draft dataset metadata\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ index.html                                   # resource page\nâ”œâ”€â”€ CHANGELOG.md                                   # resource page\nâ””â”€â”€ README.md                                    # resource documentation\n\n/test-ns/djradon/picks/                          # dataset node (versioned dataset)\nâ”œâ”€â”€ _dataset-flow/                                       # node flow (data)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks.trig                    # current picks data\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks.jsonld\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks.trig                    # draft picks data\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks.jsonld\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _v1/                                     # flow snapshot (version 1)\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks_v1.trig                 # version 1 snapshot\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _v2/                                     # flow snapshot (version 2)\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks_v2.trig                 # version 2 snapshot\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ _node-metadata-flow/                                       # node flow (metadata)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks_meta.trig              # versioning metadata, series info\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â”œâ”€â”€ djradon-picks_meta.trig              # draft versioning metadata\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ djradon-picks.trig                            # aggregated distribution\nâ”œâ”€â”€ index.html                                   # resource page\nâ””â”€â”€ CHANGELOG.md                                 # resource documentation\n\n/test-ns/djradon/underbrush/playlists/                              # bare node (container for playlist series)\nâ”œâ”€â”€ _node-metadata-flow/                                       # node flow (metadata)\nâ”‚   â”œâ”€â”€ _current/                                # flow snapshot\nâ”‚   â”‚   â”œâ”€â”€ playlists_meta.trig                  # metadata about playlist namespace\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _next/                                   # flow snapshot (draft)\nâ”‚   â”‚   â”œâ”€â”€ playlists_meta.trig                  # draft metadata\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ”œâ”€â”€ index.html                                   # resource page\nâ”œâ”€â”€ 1996-11-10/                                  # dataset node (individual playlist)\nâ”‚   â”œâ”€â”€ _dataset-flow/                                   # node flow (data)\nâ”‚   â”‚   â”œâ”€â”€ _current/                            # flow snapshot\nâ”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10.trig                   # playlist data\nâ”‚   â”‚   â”‚   â””â”€â”€ index.html                       # resource page\nâ”‚   â”‚   â”œâ”€â”€ _next/                               # flow snapshot (draft)\nâ”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10.trig                   # draft playlist data\nâ”‚   â”‚   â”‚   â””â”€â”€ index.html                       # resource page\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _node-metadata-flow/                                   # node flow (metadata)\nâ”‚   â”‚   â”œâ”€â”€ _current/                            # flow snapshot\nâ”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10_meta.trig             # playlist metadata\nâ”‚   â”‚   â”‚   â””â”€â”€ index.html                       # resource page\nâ”‚   â”‚   â”œâ”€â”€ _next/                               # flow snapshot (draft)\nâ”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10_meta.trig             # draft playlist metadata\nâ”‚   â”‚   â”‚   â””â”€â”€ index.html                       # resource page\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ _assets/                                 # asset tree\nâ”‚   â”‚   â”œâ”€â”€ _node-metadata-flow/                               # node flow (metadata)\nâ”‚   â”‚   â”‚   â”œâ”€â”€ _current/                        # flow snapshot\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10_assets.trig       # asset metadata\nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ index.html                   # resource page\nâ”‚   â”‚   â”‚   â”œâ”€â”€ _next/                           # flow snapshot (draft)\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 1996-11-10_assets.trig       # draft asset metadata\nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ index.html                   # resource page\nâ”‚   â”‚   â”‚   â””â”€â”€ index.html                       # resource page\nâ”‚   â”‚   â”œâ”€â”€ cover-photo.jpg\nâ”‚   â”‚   â””â”€â”€ index.html                           # resource page\nâ”‚   â”œâ”€â”€ 1996-11-10.trig                           # aggregated distribution\nâ”‚   â””â”€â”€ index.html                               # resource page\nâ””â”€â”€ 1996-11-17/                                  # dataset node (another playlist)\n    â”œâ”€â”€ _dataset-flow/                                   # node flow (data)\n    â”‚   â”œâ”€â”€ _current/                            # flow snapshot\n    â”‚   â”‚   â”œâ”€â”€ 1996-11-17.trig\n    â”‚   â”‚   â””â”€â”€ index.html                       # resource page\n    â”‚   â”œâ”€â”€ _next/                               # flow snapshot (draft)\n    â”‚   â”‚   â”œâ”€â”€ 1996-11-17.trig\n    â”‚   â”‚   â””â”€â”€ index.html                       # resource page\n    â”‚   â””â”€â”€ index.html                           # resource page\n    â”œâ”€â”€ _node-metadata-flow/                                   # node flow (metadata)\n    â”‚   â”œâ”€â”€ _current/                            # flow snapshot\n    â”‚   â”‚   â”œâ”€â”€ 1996-11-17_meta.trig\n    â”‚   â”‚   â””â”€â”€ index.html                       # resource page\n    â”‚   â”œâ”€â”€ _next/                               # flow snapshot (draft)\n    â”‚   â”‚   â”œâ”€â”€ 1996-11-17_meta.trig\n    â”‚   â”‚   â””â”€â”€ index.html                       # resource page\n    â”‚   â””â”€â”€ index.html                           # resource page\n    â”œâ”€â”€ 1996-11-17.trig                           # aggregated distribution\n    â””â”€â”€ index.html                               # resource page\n\n```\n","n":0.035}}},{"i":133,"$":{"0":{"v":"Weave Process","n":0.707},"1":{"v":"\n## Functionality\n\n- checks for required [[facet.system]] folders and files and creates them if missing\n- optionally removes extraneous files, interactively if requested\n- for changed [[facet.user]] datasets (i.e., need version bump)\n  - if versioning is on:\n    - creates a new [[mesh-resource.node-component.flow-snapshot.version]] \n    - updates version metadata\n  - regardless of whether versioning is on:\n    - copies _next to _current\n    - updates _node-metadata-flow with new version information\n- regenerates affected [[mesh-resource.node-component.documentation-resource.resource-page]]\n- ensure that [[folder.node]] are only contained in other [[folder.node]].\n- detect [[transpositions|principle.transposability]] and fix any \n\n\n\n```file\n/repo-root/\nâ”œâ”€â”€ _assets/\nâ”‚   â”œâ”€â”€ _templates/\nâ”‚   â”‚   â”œâ”€â”€ default.html\nâ”‚   â”‚   â”œâ”€â”€ ontology.html\nâ”‚   â”‚   â””â”€â”€ person.html\nâ”‚   â””â”€â”€ _css\nâ”‚   â”‚   â”œâ”€â”€ default.css\nâ”‚   â”‚   â”œâ”€â”€ ontology.css\nâ”‚   â”‚   â””â”€â”€ person.css\nâ””â”€â”€ my-ontology/\n    â”œâ”€â”€ _config-operational-flow  â† Node config\n    â”œâ”€â”€ _assets/                  â† Optional node-specific assets\n    â”‚   â”œâ”€â”€ _templates/           â† Optional templates\n    â”‚   â””â”€â”€ _css                  â† Optional css\n    â””â”€â”€ _payload-flow\n```\n\n## Quirks\n\n- if there's a pre-existing index.html under _assets, don't overwrite it?\n  - or maybe use AI to update it.\n  - (might need some marker html to indicate index.html was generated by weave process)\n\n## Best Practices\n\n### Weave Before Push\n\nThis ensures that in published meshes and sites:\n\n- broken references are cleaned up\n- [[mesh-resource.node-component.flow-snapshot.current]] is identical to the latest version\n\n## Features\n\n### Interactive Mode\n\n- to encourage data quality, the weave process can include an interactive mode, or modes. Perhaps one mode is naive, like \"step through every bit of metadata\" or \"review every inferred metadata item\", but you could have an AI-driven mode that optimizes for importance, identifying possible errors based on usage, etc.\n\n### Tombstoning\n\nIf you know a sub-mesh is permanently moving to a new location (or even if a branch is being created somewhere else), you should be able to tell weave to insert references to the new location\n\n### Resource Page Generation\n\n- uses the [[product.service.design.in-memory]] to calculate template usage\n- if no templates specified, and no \"default template\" exists in the root, it can generate its own\n  - perhaps there's a default template and css distributed with the service in case its missing from the mesh root\n\n## Scope\n\n- **Single flow weave**: Update one (user) flow (data, ref, or config) + the corresponding meta-flow\n- **Node weave**: Update all (user) flows in a node (each flow + meta gets woven)\n- **Node tree weave**: Recursively weave nodes and their contained nodes\n\n**Meta-flow co-weaving**: whenever any flow in a node gets woven, the meta-flow updates to reflect the new state. This ensures the meta-flow always accurately describes the current node state without requiring separate meta-weaving operations.\n\nThis gives you:\n- **Local consistency**: Each node stays internally consistent\n- **Flexible granularity**: Can weave at flow, node, or tree level as needed\n- **Automatic meta updates**: No manual meta-flow management\n- **Simple model**: No complex cross-node locking or coordination\n\nFor contained nodes mentioned in config flows, any references ideally point to the appropriate version snapshot. If there's inconsistency during concurrent operations, it's temporary and resolves when operations complete (.7).\n\nThis seems like the right balance between consistency guarantees and implementation complexity for the mesh architecture. We can always add more sophisticated coordination later if specific use cases demand it.\n","n":0.045}}},{"i":134,"$":{"0":{"v":"static site generation","n":0.577},"1":{"v":"\n- instead of generating to a docs folder or branch, the index files can just be generated in-place!\n- every resource's [[mesh-resource.node-component.documentation-resource.resource-page]] could look completely different \n    - theoretically could even be generated by a different site generator\n- weave + commit + push = publish!\n\n## What Gets Included\n\n- the [[mesh-resource.node-component.documentation-resource.readme]] would probably be the primary source in practice\n- you could supplement with AI-generated\n- you're limited only by what can be served statically, i.e., client-side, so you could have entire web applications hosted on a resource page\n","n":0.108}}},{"i":135,"$":{"0":{"v":"Versioning","n":1},"1":{"v":"\nOnly [[mesh-resource.node-component.flow]] may be versioned. But that effectively means everything important can be versioned. \n\nVersioning is controlled in the [[mesh-resource.node-component.node-config-defaults]].","n":0.224}}},{"i":136,"$":{"0":{"v":"Concept Summary","n":0.707},"1":{"v":"\n# Semantic Mesh â€” LLM-Oriented Concept Summary\n\nThis document is the canonical, compact context for LLMs. It summarizes all `documentation/concepts.*` notes and cross-links to authoritative pages.\n\n0) Semantic Flow Twin Purposes\n- Mint dereferenceable IRIs for referring to things on the Semantic Web\n- Hold versionable semantic data that uses those IRIs\n\n1) Definition\nA semantic mesh is a dereferenceable, possibly-versioned corpus of semantic resources where every IRI resolves to meaningful content. \n\nA filesystem-based mesh maps directly from a Git repositoryâ€™s folder hierarchy to a published static site so that:\n- Every resource is addressable by a stable IRI.\n- \"[[Naming resources|facet.resource.naming]]\" are dereferenceable via generated `index.html` resource pages.\n- \"[[Content resources|facet.resource.content]]\" are directly dereferenceable: they should return a file\n- RDF datasets live as distributions on versioned flow snapshots.\n- The weave process maintains coherence and keeps the repo publish-ready.\n\nSee:\n- [[concept.mesh]]: definition, requirements\n- [[concept.semantic-flow-site]]: site posture\n- [[concept.mesh-repo]]: repo-to-site mapping\n\n2) Design Principles\n- [[principle.dereferencability-for-humans]]: resource pages\n- [[principle.single-referent]]: concept vs content is explicit\n- [[principle.pseudo-immutability]]: treat version snapshots/IDs as immutable\n- [[principle.transposability]]: move meshes without breaking links via relative IDs\n- [[principle.composability]]: extract/compose submeshes\n\n3) Core Abstractions\n\n3.1 Mesh Resources (Nodes and Components)\n- Node (folder; container for nodes & components): [[mesh-resource.node]]\n  - bare node: organizational IRI segment container: [[mesh-resource.node.bare]]\n  - payload node: IRI refers to the node's referent (real-world entity or dataset concept); has a payload flow: [[mesh-resource.node.payload]]\n\n- Node component (terminal resource supporting a node): [[mesh-resource.node-component]]\n  - Flows (abstract datasets as DatasetSeries):\n    - Meta flow (metadata/provenance): [[mesh-resource.node-component.flow.node-metadata]]\n    - payload flow (payload data): [[mesh-resource.node-component.flow.payload]]\n    - Node-config flows (settings; see Â§9): [[mesh-resource.node-component.flow.node-config]]\n  - Flow snapshots (concrete Datasets): `_current/`, `_next/`, `_vN/`\n    - Overview: [[mesh-resource.node-component.flow-snapshot]]\n    - `_current/`: [[mesh-resource.node-component.flow-snapshot.current]]\n    - `_next/`: [[mesh-resource.node-component.flow-snapshot.next]]\n    - `_vN/`: [[mesh-resource.node-component.flow-snapshot.version]]\n    - Distributions: [[mesh-resource.node-component.snapshot-distribution]]\n  - Handle (refer to the node â€œas a mesh resourceâ€): [[mesh-resource.node-component.node-handle]]\n    - Handle page (human-facing): [[mesh-resource.node-component.node-handle.page]]\n  - Asset tree (static files for the node): [[mesh-resource.node-component.asset-tree]]\n  - Documentation resources (README/CHANGELOG/resource pages/fragments):\n    - README: [[mesh-resource.node-component.documentation-resource.readme]]\n    - CHANGELOG: [[mesh-resource.node-component.documentation-resource.changelog]]\n    - Resource page (index.html): [[mesh-resource.node-component.documentation-resource.resource-page]]\n    - Resource fragment: [[mesh-resource.node-component.documentation-resource.resource-fragment]]\n  - Aggregated distribution (optional roll-up of child node data): [[mesh-resource.node-component.aggregated-distribution]]\n\n3.2 Facets (Folder, File, Dataset)\n- Folder facet (namespace mapping; reserved folders): [[facet.filesystem.folder]]\n- File facet (content retrieval): [[facet.filesystem.file]]\n- Dataset facet (DatasetSeries vs Dataset): [[facet.resource.dataset]]\n\n4) Addressing and Identity\n\n4.1 Namespace and Relative Identifiers\n- Folder names become namespace segments; the path is the nodeâ€™s relative identifier (and IRI path when published).\n- Relative identifiers are used within distributions for transposability; resolve relative to distribution location.\nSee:\n- [[concept.namespace]]: overview\n- [[concept.namespace.segment]]: segment definition\n- [[concept.namespace.segment.system]]: reserved segments\n- [[concept.identifier.intramesh.relative]]: relative IDs\n\n4.2 IRI Semantics\n- Concept IRIs (slash-terminated) identify nodes, flows (abstract), snapshots (conceptual), and handle.\n- Content IRIs (with filenames) identify retrievable files: distributions, HTML pages, READMEs, assets.\n- Follow document-vs-thing hygiene to avoid ambiguity.\nSee:\n- [[concept.identifier]]: IRI types and mapping\n- [[faq.reference-iri-choices]]: trade-offs\n- [[concept.iri]]: terminology; prefer â€œIRIsâ€ when referring to mesh-local IRIs\n\n4.3 Handle Rationale\n- A nodeâ€™s IRI refers to its referent (namespace, real-world entity, or dataset concept).\n- The handle component provides a IRI to refer to the node itself â€œas a mesh resourceâ€ (for config, provenance, lifecycle).\nSee:\n- [[mesh-resource.node-component.node-handle]]\n- [[mesh-resource.node-component.node-handle.page]]\n\n5) Physical Structure and Reserved Folders\n\nReserved folder names (underscore-prefixed; canonical set):\n- `_node-handle/`\n- Flow containers (abstract datasets):\n  - `_node-metadata-flow/`, `_payload-flow/`\n  - `_config-operational-flow/`, `_config-inheritable-flow/` (see Â§9)\n- Snapshots inside a flow:\n  - `_current/`, `_next/`, `_vN/` (e.g., `_v1/`, `_v2/`, â€¦)\n- Assets:\n  - `_assets/` (static files)\n\nFolder-note pages for these reserved names live under `folder.*.md` (where defined):\n- `_node-metadata-flow/`: [[folder._node-metadata-flow]]\n- `_payload-flow/`: [[folder._data-flow]]\n- `_config-operational-flow/`: [[folder._config-operational-flow]]\n- `_config-inheritable-flow/`: [[folder._config-inheritable-flow]]\n- `_current/`: [[folder._current]]\n- `_next/`: [[folder._next]]\n- `_vN/`: [[folder._vN]]\n- `_assets/`: [[folder._assets]]\n- Node folder pages:\n  - Node: [[folder.node]]\n\n6) Data and Versioning Model\n- Only flows are versioned (flows are DatasetSeries). Nodes are not versioned.\n- Flow snapshots:\n  - `_current/`: latest stable realization; after weave it equals the content of the latest `_vN/`.\n  - `_next/`: mutable working area.\n  - `_vN/`: immutable history for precise citation and provenance.\n- Working distribution: `_next/` typically contains a single editable source; weave can fan-out serializations.\n- Sibling distribution: patterns and constraints for multi-file realizations.\nSee:\n- [[concept.versioning]]\n- [[mesh-resource.node-component.snapshot-distribution.working]]\n- [[concept.sibling-distribution]]\n\n7) Lifecycle and Weave Process\nWeave maintains structural coherence and publication readiness:\n- Ensures required system components exist.\n- If versioning is enabled, creates a new `_vN/` from `_next/`.\n- Promotes `_next/` contents to `_current/`.\n- Updates meta/provenance; regenerates resource pages.\n- Resolves internal links to maintain transposability.\n- Integrates with the scanner where applicable.\nSee:\n- [[concept.weave-process]]\n- [[concept.weave-process.resource-page-generation]]\n- [[concept.scanner]]\n- [[concept.metadata.provenance]]\n\n8) Publishing and Sites\n- Repos are static-site-ready; pushing to GitHub Pages or any static host publishes the mesh (folder paths â†’ IRI paths).\n- Transposition (domain/project move) is safe with relative IDs.\nSee:\n- [[concept.mesh-repo]]\n- [[concept.semantic-flow-site]]\n- [[concept.publication]]\n\n1) Configuration and Inheritance (Two Config Flows)\n- Operational Config Flow: final, resolved settings for a node (consumer). Overrides apply here.\n- Inheritable Config Flow: settings a node offers to descendants (provider). Property-level merge; order: parent â†’ â€¦ â†’ service â†’ platform; propagation can be firewalled.\n- Resolution: a single inheritance mechanism resolves operational config from inheritable configs plus service/platform defaults. Explicit operational entries override inherited ones.\nSee:\n- [[mesh-resource.node-component.flow.node-config]]: overview\n- [[mesh-resource.node-component.flow.node-config.operational]]\n- [[mesh-resource.node-component.flow.node-config.inheritable]]\n- [[mesh-resource.node-component.node-config-defaults]]: defaults as inheritable values\n\n1)  Aggregated Views\n- Aggregated distribution: optional roll-up of child payload nodesâ€™ current datasets at a parent node for convenience.\nSee:\n- [[mesh-resource.node-component.aggregated-distribution]]\n\n1)  Minimal File Tree Example\n\n```\n/repo-root/\nâ”œâ”€â”€ _assets/                         # optional site-wide assets\nâ”œâ”€â”€ my-node/                         # a mesh node (folder)\nâ”‚   â”œâ”€â”€ _node-handle/                # handle component (resource.node-component.node-handle)\nâ”‚   â”œâ”€â”€ _node-metadata-flow/                  # metapayload flow (system)\nâ”‚   â”‚   â”œâ”€â”€ _current/\nâ”‚   â”‚   â””â”€â”€ _v1/\nâ”‚   â”œâ”€â”€ _payload-flow/                  # payload flow (for payload nodes)\nâ”‚   â”‚   â”œâ”€â”€ _current/\nâ”‚   â”‚   â”œâ”€â”€ _next/\nâ”‚   â”‚   â””â”€â”€ _v1/\nâ”‚   â”œâ”€â”€ _config-inheritable-flow/    # provider config (optional)\nâ”‚   â”œâ”€â”€ _config-operational-flow/    # resolved config (optional; may be system-written)\nâ”‚   â”œâ”€â”€ index.html                   # resource page\nâ”‚   â”œâ”€â”€ README.md\nâ”‚   â””â”€â”€ CHANGELOG.md\nâ””â”€â”€ docs/ or public host mapping     # publication target\n```\n\n12) Visual Overview\n\n```mermaid\ngraph TD\n  A[Mesh Node] --> B[Handle]\n  A --> C[Meta flow]\n  A --> E[payload flow]\n  A --> G[Asset tree]\n  A --> H[Resource pages]\n\n  C --> C1[_current]\n  C --> C2[_vN]\n  E --> E1[_current]\n  E --> E2[_next]\n  E --> E3[_vN]\n```\n\n13) Glossary\n- [[concept.mesh]]: the set of addressable resources in a repository, published as a site\n- [[mesh-resource.node]]: an extensible  resource containing other nodes and its own components\n- [[mesh-resource.node-component]]: terminal resource that supports node behavior/structure\n- [[mesh-resource.node-component.flow]]: DatasetSeries representing an abstract dataset (meta/data/config)\n- [[mesh-resource.node-component.flow-snapshot]]: concrete Dataset realization of a flow (`_current/`, `_next/`, `_vN/`)\n- [[mesh-resource.node-component.snapshot-distribution]]: a concrete serialization file (TriG, JSON-LD, etc.): \n- [[mesh-resource.node-component.node-handle]]: indirection to refer to the node â€œas a mesh resourceâ€\n- [[mesh-resource.node-component.documentation-resource.resource-page]]: dereferenceable `index.html` for folders\n- [[concept.weave-process]]: lifecycle operation to version/promote/regenerate/repair:\n","n":0.032}}},{"i":137,"$":{"0":{"v":"Sibling Distribution","n":0.707},"1":{"v":"\n- same data, different syntax","n":0.447}}},{"i":138,"$":{"0":{"v":"Semantic Flow site","n":0.577},"1":{"v":"\nSemantic Flow sites provide:\n\n- dereferencability for mesh [[concept.identifier]]s\n- hosting versioned RDF datasets and their histories\n\n## Publishing\n\n- For sites exposed by Github/Gitlab Pages functionality, pushing a [[concept.mesh-repo]] effectively publishes it.\n","n":0.186}}},{"i":139,"$":{"0":{"v":"Scanner","n":1},"1":{"v":"\n- scans datasets and dataset distributions, which can be :\n  - local folders\n  - git repos\n  - compliant IRIs\n  - SPARQL data sources\n- can filter which subfolders to include/exclude\n- \n","n":0.183}}},{"i":140,"$":{"0":{"v":"root node","n":0.707},"1":{"v":"\nThe node at the top of a mesh hierarchy may be referred to as the root node. \n\nEvery other [[mesh-resource]] in a mesh \"lives under\" the root node.\n\nFor pure [[concept.mesh-repo]]s, the repository's name is used as root node's identifier. \n\nFor [[concept.mesh.embedded]], the root node's folder name is its identifier.\n\nA root node is not treated or represented any differently than any other [[mesh-resource.node]], and it is not differentiated in metadata. So any node may become a root node simply by copying it somewhere that's not already a mesh.\n","n":0.107}}},{"i":141,"$":{"0":{"v":"Referent","n":1},"1":{"v":"\n## Definition\n\nThe *referent* is the thing (real or imaginary) to which a [[resource|mesh-resource]]â€™s [[concept.identifier]] **refers**. Every identifier [[denotes|concept.denotation]] its referent, \n\n## Node vs. referent\n\n- **Referent**: the subject that the nodeâ€™s identifier names (a person, concept, event, dataset, etc.).\n- **Node**: the mesh construct that provides an identifier for and contains linked data about the referent. It also provides linked data about itself (as a node), and may contain other resources used for supporting [[concept.semantic-flow-site]]s.\n\nTo talk about the node itself, you use its **node handle** (e.g. published IRI `https://ns.example.org/persons/djradon/_node-handle` or mesh identifier `<djradon/_node-handle>`).\n\n**Where itâ€™s described**\n\n* The **referentâ€™s description** lives in the nodeâ€™s [[mesh-resource.node-component.flow.reference]].\n* The **nodeâ€™s own metadata and provenance** live in the **`_node-*` flows** (e.g. `_node-metadata-flow`, `_node-config-*`).\n\n**Special case: payload nodes**\n\n* In a **payload node**, the **referent** is not an external entity but an **evolvable dataset** contained in the node.\n* The dataset evolves as versioned distributions inside the nodeâ€™s `_payload-flow` (e.g. `v1/`, `v2/`, â€¦).\n* The `_reference-flow` may describe the dataset, e.g. its **name, type, and provenance**.\n* Example:\n\n  * Node IRI: `https://ns.example.org/projects/atlas/`\n  * Referent: *the Atlas dataset* (identified by the node IRI, evolving over time).\n  * `_reference-flow`: declares it as a dataset, supplies label and attribution.\n  * `_payload-flow`: provides concrete versions (`v1`, `v2`, â€¦).\n\n\n## Why referent matters\n\nUnderstanding what a IRI refers to is crucial for proper semantic web implementation. In the past, people have tried to use content IRIs to represent the things they refer to. A classic example is using `http://example.org/person.html` to identify a person, when it actually identifies an HTML document about the person. This conflation creates semantic ambiguity and breaks linked data principles.\n\nSemantic Flow enforces clear referent distinctions through IRI patterns: slash-terminated IRIs always refer to concepts or entities, while extension-terminated IRIs always refer to retrievable content. This prevents the classic \"document vs thing\" confusion that has plagued semantic web implementations.\n","n":0.058}}},{"i":142,"$":{"0":{"v":"publication","n":1},"1":{"v":"\n- A mesh is \"published\" when it becomes accessible by using an absolute URL, (e.g., via a web browser). \n\n\n## Publication History Tracking\n\nThe inferred publication locations can be tracked to maintain a history of where a node has been published, which aids in citation consistency and discovery:\n\n```turtle\n# In _flow/ metadata\n<_node-handle> mesh:publishedAt <https://myorganization.github.io/data-mesh/ns/djradon/> ;\n          mesh:previousPublications ( \n            <https://djradon.github.io/ns/djradon/>\n            <https://oldsite.com/research/ns/djradon/>\n          ) ;\n```\n\nThis allows external citations to find resources even after they've been moved, and provides a clear provenance trail.\n","n":0.114}}},{"i":143,"$":{"0":{"v":"platform element","n":0.707},"1":{"v":"\nA **platform element** is any aspect of the Semantic Flow platform, including:\n\n- ontologies and vocabularies\n- conventions (naming, filesystem, best practices)\n- mesh and namespace structure\n- principles (design, philosophy)\n- meshes, nodes, and node components\n- workflows and lifecycle operations\n- documentation\n- code and tests\n","n":0.158}}},{"i":144,"$":{"0":{"v":"node config","n":0.707},"1":{"v":"\n## per-node config specification\n\nNode configuration determines:\n\n- flow versioning\n- resource page and resource fragment generation\n- distribution syntaxes\n- template usage and stylesheets\n- attribution defaults\n\nNode configuration is held in memory by the [[flow service|sflo.product.service]], and is calculated when the service starts.\n\nNode configuration is at least partially determined by \"config specification\", which happens in [[mesh-resource.node-component.flow.node-config]] and can be inherited to contained nodes.\n\nIf config specification is missing, (i.e., config spec inheritance is turned off or unspecified), node configuration will be determined from service-level config specification, i.e. [[product.service.config]]. In case there is none, the service will use sensible defaults at the root level which will be inherited down the mesh.\n\n### Initial Config Specification\n\n- When a node is initially created, if config-defaults-inheritance is turned on for its parent node, it will have its [[mesh-resource.node-component.flow.node-config]] populated based on any parent [[mesh-resource.node-component.node-config-defaults]] files present in the hierarchy. If there are none, its [[mesh-resource.node-component.flow-snapshot.current]] will not be created.\n\n### Calculating Node Config\n\nWhen the [[product.sflo-host]] starts, it calculates non-default config settings for every node.\n\n- determines the \"default\" settings for this service instance from [[product.service.config]]\n- if the node has a [[mesh-resource.node-component.flow.node-config]] , the service will use any settings there that differ from its defaults\n- if config-inheritance is turned on for a node, the service will scan back up the hierarchy to compose any missing \"non-default\" settings\n-  the result is an in-memory \"shadow mesh\" known as the [[product.service.components.node-config-map]] containing any non-default settings for the mesh\n\nIf calculated config matches the service defaults, they are ignored.\n\n## per-service settings for node defaults\n\n- [[product.service.config]] can establish any mesh-wide settings that diverge from the system defaults\n\n## platform node-config defaults\n\nSemantic Flow uses sensible defaults, specified in the so that neither node-level nor service-level \"non-default\" settings are necessary\n\n- by default:\n  - versioning is turned on for all flows\n  - distribution syntaxes are .trig and jsonld\n  - resource pages are generated using a standard template and CSS file that get copied into a [[concept.mesh-repo]]'s root [[mesh-resource.node-component.asset-tree]] upon initialization\n  - [[mesh-resource.node-component.aggregated-distribution]] are not generated\n  - [[concept.mesh.resource.element.flow.unified]]\n","n":0.056}}},{"i":145,"$":{"0":{"v":"Namespace","n":1},"1":{"v":"\n## Overview\n\nA namespace is the hierarchical address space formed by nesting nodes. Every node extends the namespace with its identifier, which correspond to filesystem folders when the mesh is stored in the filesystem. The resulting path maps directly to the published IRI when appended to the [[concept.namespace.context]]\n\n- Concept vs content IRI semantics: see [[concept.identifier]]\n- How intramesh identifiers are resolved: see [[concept.identifier.intramesh]]\n\n\n## Minimal Example\n\n```file\n/ns/                         # bare node â†’ https://ex.org/ns/\nâ””â”€â”€ people/                  # bare node â†’ https://ex.org/ns/people/\n    â””â”€â”€ alice/               # reference node â†’ https://ex.org/ns/people/alice/\n```\n\n- Folder names correspond to [[concept.identifier.intramesh]]s and become namespace segments when [[published|concept.publication]].\n- Slash-terminated IRIs identify concepts; file IRIs identify content (see [[concept.identifier]]).\n\n## Publishing Base\n\nThe siteâ€™s base IRI is determined by the publishing platform (e.g., GitHub Pages or self-hosting with [[product.sflo-host]]). See [[concept.implied-rdf-base]] for user/org vs project page mappings and guidance on avoiding hardcoded bases.\n","n":0.086}}},{"i":146,"$":{"0":{"v":"namespace segment","n":0.707},"1":{"v":"\n## Definition\n\nA namespace segment is a single \"folder resource\" identifier that extends the a mesh's namespace. The concatenation of parent identifiers yields the namespace for a node.\n\n- Concept vs content IRI semantics: see [[concept.identifier]]\n- How relative identifiers are resolved: see [[concept.identifier.intramesh]]\n\n## Naming (recommended)\n\n- Use camel-case (initial lowercase letter), e.g., `people`, `myProjects`\n- Maybe avoid starting segment names names with an underscore (`_`); underscore-prefixed names are used for [[concept.namespace.segment.system]]\n\nThese are recommendations based on RDF conventions, not hard rules; sometimes projects have good reasons to diverge.\n\n## Stability\n\nRenaming a segment probably breaks the identifier (IRI) of all contained resources. If you must rename:\n\n- Consider redirect/tombstoning strategies and publication history â€” see [[concept.publication]]\n- Review impacts on inbound references; plan a weave and re-publish cycle\n- see also [[feature.handling-renaming]]\n\n## Example\n\n```file\n/ns/                         â†’ https://ex.org/ns/\nâ””â”€â”€ datasets/                â†’ https://ex.org/ns/datasets/\n    â””â”€â”€ census/              â†’ https://ex.org/ns/datasets/census/\n```\n\n- Each folder adds exactly one namespace segment\n- Folders map directly to slash-terminated concept IRIs (see [[concept.identifier]])\n","n":0.082}}},{"i":147,"$":{"0":{"v":"system segments","n":0.707},"1":{"v":"\nSystem segments are underscore-prefixed folder names reserved by the platform. Prefer not to use `_`-prefixed names for user-defined segments.\n\nThis page is the canonical list; see the linked docs for behavior and details.\n\n## Flows (abstract/series)\n\n- [[_node-metadata-flow/|folder._node-metadata-flow]]\n- [[_dataset-flow/|folder._dataset-flow]]\n- [[_config-operational-flow/|folder._config-operational-flow]]\n- [[_config-inheritable-flow/|folder._config-inheritable-flow]]\n\n## Snapshots (concrete)\n\n- [[_current/|folder._current]]\n- [[_next/|folder._next]]\n- [[_vN/|folder._vN]]\n\n## Other reserved\n\n- [[_node-handle/|folder._node-handle]]\n- [[_assets/|folder._assets]]\n\nFor IRI semantics: see [[concept.identifier]]. For namespace background: see [[concept.namespace]] and [[concept.namespace.segment]].\n","n":0.131}}},{"i":148,"$":{"0":{"v":"namespace root","n":0.707},"1":{"v":"\nSince every mesh has to have a single root node, that node's identifier can be referred to as the **namespace root**. It can be combined with the [[concept.namespace.context]] to make an [[concept.iri]]. \n\nFor [[concept.mesh-repo]]s, the namespace root uses the repo name.\n","n":0.156}}},{"i":149,"$":{"0":{"v":"namespace context","n":0.707},"1":{"v":"\n\n## Overview\n\nThe **namespace context** is the URL under which a [[concept.semantic-flow-site]] publishes all mesh identifiers. It \"contains\" the mesh and so is outside the meshâ€™s namespace. It is determined by the hosting platform.\n\nSites have a namespace context that starts with `http://` or `https://`.\n\nFilesystem-based meshes have a namespace context that starts with `file://`.\n\nThe context is a deployment concern; a [[woven|concept.weave-process]] mesh should be valid regardless of where it is served.\n\n## Platform mappings \n\n### GitHub Pages\n\n- User/Org site:\n  - namespace context: `https://org.github.io/`\n  - Mesh namespace `/ns/people/alice/` publishes at `https://org.github.io/ns/people/alice/`\n- Project site context:\n  - `https://org.github.io/repo/`\n  - Mesh path `/ns/people/alice/` publishes at `https://org.github.io/repo/ns/people/alice/`\n\nThese mappings can be accomplished with both [[concept.mesh-repo]]s and [[concept.mesh.embedded]]\n\n","n":0.096}}},{"i":150,"$":{"0":{"v":"Metadata","n":1},"1":{"v":"\nAside from the metadata involved in datasets themselves, semantic meshes have operational metadata that capture things like:\n\n  [[concept.metadata.provenance]] (entities, agents, activities)\n- Copyright and licensing\n- validation / consistency checks\n- metrics","n":0.186}}},{"i":151,"$":{"0":{"v":"Provenance","n":1},"1":{"v":"\n## Core Principles\n\n**Version-only provenance** - Provenance is recorded only for immutable version snapshots (like `_v47`), not for moving targets like `_current` or `_next`.\n\n**Meta-flow storage** - Semantic Flow-specific provenance lives in meta-flows, referencing version snapshots in other flows. Domain-specific provenance can live in datasets themselves.\n\n**Current snapshot duplication** - `_current` meta snapshots contain identical copies of the latest version's provenance with base URI pointing to the version snapshot for stable fragment resolution.\n\n## Architecture\n\n### Version Snapshot Provenance\n\n```turtle\n# In my-dataset/_node-metadata-flow/_v47/my-dataset_meta.trig\n@base <../_v47/> .\n\n# Weave activity with PROV standard properties\n:configUpdateActivity a meta:ConfigWeave ;\n    prov:startedAtTime \"2025-07-20T14:30:00Z\" ;\n    prov:endedAtTime \"2025-07-20T14:30:15Z\" ;\n    prov:used <../../_config-flow/_v46/config.jsonld> ;\n    prov:generated <../../_config-flow/_v47/config.jsonld> ;\n    prov:wasAssociatedWith <https://semantic-flow.org/agents/flow-service-bot> .\n\n# Rights and licensing at snapshot level\n<../../_config-flow/_v47> dcterms:rightsHolder <https://orcid.org/0000-0002-1825-0097> ;\n                          dcterms:license <https://creativecommons.org/licenses/by-sa/4.0/> ;\n                          prov:has_provenance :configProvenance .\n\n# Delegation chain (step 1 = top authority, gets copyright by default)\n:configProvenance a meta:ProvenanceContext ;\n    meta:forActivity :configUpdateActivity ;\n    meta:forSnapshot <../../_config-flow/_v47> ;\n    prov:wasAttributedTo <https://acme-corp.com/org> ; # Primary attribution\n    meta:delegationChain :delegationChain_001 .\n\n:delegationChain_001 meta:hasStep :step1, :step2, :step3 .\n\n:step1 a meta:DelegationStep ;\n       meta:stepOrder 1 ;\n       prov:agent <https://acme-corp.com/org> . # Prime mover, no actedOnBehalfOf\n\n:step2 a meta:DelegationStep ;\n       meta:stepOrder 2 ;\n       prov:agent <https://orcid.org/0000-0002-1825-0097> ;\n       prov:actedOnBehalfOf <https://acme-corp.com/org> .\n\n:step3 a meta:DelegationStep ;\n       meta:stepOrder 3 ;\n       prov:agent <https://semantic-flow.org/agents/flow-service-bot> ;\n       prov:actedOnBehalfOf <https://orcid.org/0000-0002-1825-0097> .\n```\n\n### Current Snapshot Copy\n\n```turtle\n# In my-dataset/_node-metadata-flow/_current/my-dataset_meta.trig\n@base <../_v47/> .\n\n# Identical content to version snapshot - all URIs resolve to stable version\n# (same provenance content as above)\n```\n\n### Unversioned Flow Accumulation\n\nFor flows without versioning, activities accumulate in `_next` with unique timestamps:\n\n```turtle\n# In my-dataset/_node-metadata-flow/_next/my-dataset_meta.trig\n:dataActivity_2025-07-20_14-30 a meta:DataWeave ;\n    prov:startedAtTime \"2025-07-20T14:30:00Z\" ;\n    prov:generated <../../_payload-flow/_current/data.trig> .\n\n:dataActivity_2025-07-20_16-45 a meta:DataWeave ;\n    prov:startedAtTime \"2025-07-20T16:45:00Z\" ;\n    prov:used <../../_payload-flow/_current/data.trig> ;\n    prov:generated <../../_payload-flow/_current/data.trig> .\n```\n\n## Key Components\n\n### Activity Types (subclass `prov:Activity`)\n- `meta:ConfigWeave`, `meta:ReferenceWeave`, `meta:DataWeave`, `meta:MetaWeave`\n- `meta:NodeWeave` (entire node), `meta:NodeTreeWeave` (recursive)\n\n### Provenance Entities (subclass `meta:ProvenanceEntity`)\n- `meta:ProvenanceContext` - Relator for complex authorship scenarios\n- `meta:DelegationChain` / `meta:DelegationStep` - Authorization chains\n- `meta:AgentRoleCollection` / `meta:AgentRole` - Collaborative role assignments\n\n### Standard Properties Used\n- `prov:agent`, `prov:actedOnBehalfOf`, `prov:wasAttributedTo` (instead of custom properties)\n- `dcterms:rightsHolder`, `dcterms:license` (rights at snapshot level)\n- `prov:has_provenance` (link snapshots to provenance contexts)\n\n## Delegation Chain Pattern\n\n**Step ordering**: Lower numbers = higher authority\n- Step 1: Prime mover (organization) - gets copyright by default, no `prov:actedOnBehalfOf`\n- Step 2+: Each agent acts on behalf of the previous step's agent\n- Tools/software agents typically at the end of the chain\n\n## Configuration\n\n**Copyright assignment**: Configurable in node-config-defaults, defaults to first agent in delegation chain (step 1).\n\n**External vocabulary tracking**: Use SHACL to declare recommended external properties like `prov:wasInfluencedBy`, `dcterms:license`.\n\n## Implementation Notes\n\n- **Fragment URIs**: Use `<#step1>` etc. within version snapshots for stable addressability\n- **Base URI**: All snapshots use `@base <../_vN/>` pattern for consistent resolution\n- **Rights inheritance**: Capture previous version rights holders in provenance contexts when content is derived\n- **Static site friendly**: Documentation approach for external references since no server-side redirects available\n\n## Fragment Identifier Naming Scheme\n\nTo ensure that every RDF node within a `_meta` distribution has a unique and dereferenceable URI, the following naming scheme for fragment identifiers MUST be used. This allows the `index.html` file for a given snapshot version to correctly link to all provenance entities.\n\nThe structure is as follows:\n\n`<{flow-slug}-{version}-{entity-type}[-{unique-part}]>`\n\n-   **`{flow-slug}`**: The slug of the flow this provenance describes (e.g., `config-flow`, `data-flow`). This provides the primary namespace for the identifier.\n-   **`{version}`**: The version of the snapshot (e.g., `v47`). This scopes the provenance to a specific point in time.\n-   **`{entity-type}`**: The class of the entity, using a consistent UpperCamelCase (e.g., `Activity`, `Context`, `DelegationChain`, `DelegationStep`).\n-   **`{unique-part}`**: (Optional) A unique suffix, such as a step number or a timestamp, used when multiple entities of the same type exist for the same flow and version.\n\n### Example\n\nFor a `config-flow` at version `v47`, the identifiers would be:\n\n-   **Activity**: `<#config-flow-v47-Activity>`\n-   **Provenance Context**: `<#config-flow-v47-Context>`\n-   **Delegation Chain**: `<#config-flow-v47-DelegationChain>`\n-   **Delegation Steps**:\n    -   `<#config-flow-v47-DelegationStep-1>`\n    -   `<#config-flow-v47-DelegationStep-2>`\n","n":0.041}}},{"i":152,"$":{"0":{"v":"semantic mesh","n":0.707},"1":{"v":"\n## Overview\n\nA **semantic mesh** is a [[pseudo-immutable|principle.pseudo-immutability]] collection of (possibly-versioned) linked-data resources. It organizes these resources in a  [[publishable|concept.publication]] way, such that a mesh can be used as a [[semantic site|concept.semantic-flow-site]] where every HTTP IRI returns meaningful content.\n\n### Key characteristics\n\n- **Addressable**: Every [[mesh-resource]] has an [[concept.identifier]]; when a mesh is [[published|concept.publication]], every [[mesh-resource]] then gets a globally unique, human-readable IRI\n- **Versioned**: Changes are managed through the [[Weave Process|concept.weave-process]] process, and [[mesh-resource.node-component.flow]] are versioned by default\n- **Publish-ready**: Can be served directly via GitHub Pages or similar static hosting; or via a local web server like live-server\n\n## Core Concepts\n\n### Mesh Resources\n\nThe primary constituents of a mesh are [[mesh-resource.node]]s. Nodes contain their own [[mesh-resource.node-component]]s, and may also contain other nodes. \n\n#### Mesh Nodes\n\n[[Mesh nodes|mesh-resource.node]] extend [[concept.namespace]]s and serve as containers.\n\n- **[[bare nodes|mesh-resource.node.bare]]**: Empty containers for organizing other mesh nodes\n- **[[reference nodes|mesh-resource.node.reference]]**: Nodes that refer to entities (people, places, concepts, etc.)\n- **[[payload nodes|mesh-resource.node.payload]]**: Nodes containing data distributions with optional versioning\n\n\n#### Node components\n\n[[Node components|mesh-resource.node-component]] help define, support, and systematize nodes.\n\n\n#### Example Mesh\n\n[[Mesh resources|mesh-resource]] have at least one [[concept.identifier]] and (usually) a [[concept.referent]].\n\n| [[concept.identifier.intramesh]]                      | Semantic Flow resource type                                           | referent                     |\n| ----------------------------------------------------- | --------------------------------------------------------------------- | ---------------------------- |\n| `ns/`                                                 | [[mesh-resource.node.bare]]                                           | - nothing - (yet!)           |\n| `ns/djradon/`                                         | [[mesh-resource.node.reference]]                                      | a person                     |\n| `ns/djradon/_node-handle/`                            | [[mesh-resource.node-component.node-handle]]                          | mesh node                    |\n| `ns/djradon/_reference-flow/`                         | [[mesh-resource.node-component.flow.reference]]                       | reference flow               |\n| `ns/djradon/_reference-flow/_current/`                | [[mesh-resource.node-component.snapshot-distribution.current]]        | reference flow snapshot      |\n| `ns/djradon/index.html`                               | [[mesh-resource.node-component.documentation-resource.resource-page]] | resource page (content)      |\n| `ns/djradon/README.md`                                | [[mesh-resource.node-component.documentation-resource.readme]]        | README file (content)        |\n| `ns/djradon/picks/`                                   | [[mesh-resource.node.payload]]                                        | abstract dataset             |\n| `ns/djradon/picks/_payload-flow/`                     | [[mesh-resource.node-component.flow.payload]]                         | payload dataset series       |\n| `ns/djradon/picks/_payload-flow/_next/`               | [[mesh-resource.node-component.flow-snapshot.next]]                   | concrete payload dataset     |\n| `ns/djradon/picks/_payload-flow/_v1/picks_v1.trig`    | [[mesh-resource.node-component.snapshot-distribution]]                | paylod dataset distribution  |\n| `ns/djradon/picks/_node-metadata-flow/`               | [[mesh-resource.node-component.flow.node-metadata]]                   | node metadata dataset series |\n| `ns/djradon/picks/_node-metadata-flow/_current/`      | [[mesh-resource.node-component.flow-snapshot.current]]                | node metadata dataset        |\n| `ns/djradon/picks/_config-operational-flow/`          | [[mesh-resource.node-component.flow.node-config.operational]]         | operational config series    |\n| `ns/djradon/picks/_config-operational-flow/_current/` | [[mesh-resource.node-component.flow-snapshot.current]]                | operational config           |\n| `ns/djradon/picks/_config-inheritable-flow/`          | [[mesh-resource.node-component.flow.node-config.inheritable]]         | inheritable config series    |\n| `ns/djradon/picks/_config-inheritable-flow/_current/` | [[mesh-resource.node-component.flow-snapshot.current]]                | inheritable config           |\n| `ns/assets/`                                          | [[mesh-resource.node-component.asset-tree]]                           | collection of assets         |\n| `ns/assets/images/`                                   | asset folder                                                          | - not a sf resource -        |\n| `ns/assets/images/logo.svg`                           | asset                                                                 | - not a sf resource -        |\n\n\nExample:\n- `ns/` = bare node for organizing content and minting IRIs; refers to itself as a namespace\n- `ns/djradon/` = refers to Dave the person (payload node)\n- `ns/djradon/index.html` = resource page about Dave (content)\n- `ns/djradon/pics/` = refers to Dave's biographical dataset (payload node)\n- `ns/djradon/pics/_payload-flow/` = abstract dataset (DatasetSeries) containing Dave's \"music picks\" data\n- `ns/djradon/pics/_payload-flow/_current/` = current concrete dataset snapshot\n- `ns/djradon/pics/_payload-flow/_v1/picks_v1.trig` = RDF distribution from version 1\n- `ns/djradon/_assets/images/headshot.jpg` = an image asset; \"attached\" to the mesh, but not a mesh resource\n\n\n\n\n\n#### Naming Resources\n\n- **[[mesh-resource.node-component.flow]]** and their [[mesh-resource.node-component.flow-snapshot]]\n  - **[[mesh-resource.node-component.flow.node-metadata]]**: System-related administrative and structural metadata for mesh nodes\n  - **[[Version datasets|mesh-resource.node-component.flow-snapshot.version]]**: Versioned snapshots of datasets\n- **[[next snapshots|mesh-resource.node-component.flow-snapshot.next]]**: Draft workspaces for ongoing changes to versioned datasets\n- **[[Node handles|resource.node-component.node-handle]]**: Components that provide referential indirection, allowing references to nodes as mesh resources rather than their referents\n- **[[Asset trees|mesh-resource.node-component.asset-tree]]**: Collections of arbitrary files and folders attached to the mesh\n\n#### File Resources\n\nTerminal [[mesh resources|mesh-resource]] that cannot contain other resources:\n\n- **[[Resource pages|mesh-resource.node-component.documentation-resource.resource-page]]**: index.html files present in every mesh folder after weaving\n- **[[Distribution files|mesh-resource.node-component.snapshot-distribution]]**: Data files in various RDF formats\n- **README.md and CHANGELOG.md**: Documentation files providing context\n\n\n## Filesystem Structure\n\nMeshes may be constituted as a set of filesystem [[folder]]s and [[file]]s.\n\n### Folder Mapping\n\n- Mesh nodes correspond physically to [[mesh folders|facet.filesystem.folder]]\n- Folder names become namespace segments and IRI path components\n- The local [[concept.identifier.intramesh]] for a node matches its containing folder name\n\n### File Organization\n\n- [[Datasets|facet.resource.dataset]] are represented by folders containing at least one distribution file\n- Distribution files must be named using the dataset's [[namespace segment|concept.namespace.segment]]\n- Resource pages (index.html) should be present in every mesh folder after [[weaving|concept.weave-process]]\n\n### Reserved Names\n- All system identifiers begin with an underscore (_)\n- Examples: `_assets/`, `_node-metadata-flow/`, `_current`, `_next`\n\n## Logical Structure\n\n### Namespace Extension\n\n- Mesh folders always extend the namespace with a segment corresponding to the folder name\n- This creates a hierarchical IRI structure for addressing resources\n- Each resource has a unique [[Intramesh|concept.identifier.intramesh]] based on its path and local name\n\n### Containment Rules\n\n- **Mesh nodes** are always containers of components (i.e., at least [[mesh-resource.node-component.flow.node-metadata]] and [[mesh-resource.node-component.node-handle]]) and potentially containers of other nodes\n  - **[[bare nodes|mesh-resource.node.bare]]**: no additional containment requirements\n  - **[[reference nodes|mesh-resource.node.reference]]**: must have [[mesh-resource.node-component.flow.reference]]  where the referenced entity can be described\n  - **[[payload nodes|mesh-resource.node.payload]]**: must have [[mesh-resource.node-component.flow.payload]] with at least one distribution\n- **Asset tree components**: Cannot contain nodes\n- all resource folders should contain a [[mesh-resource.node-component.documentation-resource.resource-page]] that makes there IRIs servable/dereferenca\n- \n\n## Rules & Constraints\n\n### System vs User Boundaries\n- **System components**: Generated and managed by the weave process, not intended for user modification\n- **User components**: Directly modifiable by users ([[mesh-resource.node-component.flow-snapshot.current]], README.md, CHANGELOG.md)\n- The weave process maintains system components and generates missing required flows\n\n### Versioning Requirements\n- flow versioning is managed through the [[Versioning|concept.versioning]] system\n  - turning versioning on and off is controlled in the [[mesh-resource.node-component.node-config-defaults]]\n  - Version history is realized in [[mesh-resource.node-component.flow-snapshot.version]] with numbered version snapshots\n  - Version history metadata is kept in the node's [[mesh-resource.node-component.flow.node-metadata]]\n\n### Addressing Requirements\n- Every mesh resource must be addressable via its IRI path\n- IRIs must return meaningful content when dereferenced\n  - [[mesh-resource.node-component.documentation-resource.resource-page]] provide human-readable information for [[facet.filesystem.folder]]-based resources\n    - resource pages are always index.html files generated by \"on weave\" from the [[mesh-resource.node-component.documentation-resource.changelog]] and [[mesh-resource.node-component.documentation-resource.readme]] [[mesh-resource.node-component.documentation-resource]], templates in [[mesh-resource.node-component.asset-tree]] and any scoped template mappings specified in [[mesh-resource.node-component.node-config-defaults]] files \n  - [[facet.filesystem.file]]\n\n## Integration Points\n\n### Weave Process\nThe [[Weave Process|concept.weave-process]] process maintains mesh integrity by:\n- Checking for required system resources and creating them if missing\n- Generating resource pages for changed resources\n- Managing dataset versioning and metadata\n- Ensuring all resources remain addressable and dereferenceable\n\n### Publishing Workflow\n- Meshes are designed to be served directly as static sites\n- GitHub Pages integration allows immediate publishing after repository updates\n- No static site generator required, though resource page generation occurs during weaving\n- The repository structure directly maps to the published IRI structure\n\n### Dataset Integration\nMeshes support multiple RDF formats and follow [[DCAT v3|related-topics.dcat.vocabulary]] standards for dataset organization. [[Datasets|facet.resource.dataset]] within meshes include both standalone datasets and those embedded as node components.\n","n":0.032}}},{"i":153,"$":{"0":{"v":"submesh","n":1},"1":{"v":"\nMeshes always have a [[concept.root-node]]. Any \n","n":0.378}}},{"i":154,"$":{"0":{"v":"loose mesh","n":0.707},"1":{"v":"\nA **loose mesh** has not yet been woven, and may have any number of:\n\n- [[concept.identifier]]s without corresponding [[mesh-resource]]\n- [[mesh-resource.node-component.flow-snapshot.next]] that differ from [[mesh-resource.node-component.flow-snapshot.current]]\n- missing [[facet.system]] [[files|file]] or [[folder]]\n","n":0.189}}},{"i":155,"$":{"0":{"v":"embedded mesh","n":0.707},"1":{"v":"\nAn **embedded mesh** is a mesh whose [[concept.root-node]] is not located at the top of a repo.\n\nIt may be contained in a repo that it shares with other files. Or maybe it doesn't live in a repo at all.\n\n","n":0.16}}},{"i":156,"$":{"0":{"v":"Assets","n":1},"1":{"v":"\nEverything file in an [[folder._assets]] (or its subfolders) is considered an asset.\n\nEven though they will have a IRI in the [[concept.implied-rdf-base]], they are not considered \"mesh resources\".\n\n## Special Assets\n\nPrefixed with an underscore, these assets have a special role to play in the \"weave process\"\n\n\n### _weave-config.jsonld\n\n","n":0.149}}},{"i":157,"$":{"0":{"v":"mesh repo","n":0.707},"1":{"v":"\nA Semantic Flow mesh repository (or mesh repo for short) is a git repository that contains a [[concept.root-node]] at the top of the repo, and any number of additional, contained [[mesh-resource.node]]\n\n\n## Github Repos\n\n- can either be \"username/org pages repository\"\" (which automatically hosts content at the namesake IRI, so maybe call it a namesake repository, e.g. djradon.github.io) or 2nd-level (corresponding to an owned repo)\n- under \"classic\" github pages (i.e., \"deploy from a branch\"), you can either use the whole repo, or just the docs folder as the source; Semantic Flow works seamlessly using the entire folder for mesh repos.\n  - for [[concept.mesh.embedded]] repos, a build step will be needed to copy the mesh folders and files into \"docs\"\n  \n## Questions\n\n- can you include a mesh in an existing repo?\n  - Sure! That's an [[concept.mesh.embedded]].\n  - but when composing a mesh from existing meshes, (i.e., linking a repo into an existing mesh) you need an unbroken chain, so embedding a root mesh below the top of a repo might make it harder for people to re-use. \n","n":0.076}}},{"i":158,"$":{"0":{"v":"mesh location","n":0.707},"1":{"v":"\nA mesh can be represented by a bunch of files and folders in a filesystem. But it is primarily a concept.\n\nConceptually, a mesh is composed of [[mesh-resource]]s. Those resources all have a **mesh location**: conceptual addresses that may be specified relative to each other ([[concept.identifier.intramesh.relative]]) or from the [[root node|concept.root-node]]. \n\nMesh locations map cleanly to filesystem paths, and they work \n\n\n","n":0.128}}},{"i":159,"$":{"0":{"v":"IRI","n":1},"1":{"v":"\nOn the semantic web, resources (things you might want to talk about with RDF statements) are identified with Internationalized Resource Identifiers. Traditionally, IRIs don't necessarily LOCATE a resource, i.e., if you put them in a web browser, they don't necessarily return content.\n\nBut with [[Semantic Flow sites|concept.semantic-flow-site]], all [[mesh-resource]] IRIs return a web page. \n\n## Types of IRIs\n\n- Absolute IRIs\n- Relative IRIs\n  - Relative-Path Relative IRIs\n  - Absolute-Path Relative IRIs \n\nSee [[faq.reference-iri-choices]] for a discussion of IRI types. \n\n","n":0.113}}},{"i":160,"$":{"0":{"v":"implied rdf base","n":0.577},"1":{"v":"\n## Overview\n\n[[mesh-resource.node-component.snapshot-distribution]]s have an **implied namespace base**, essentially the absolute IRI that corresponds to the [[concept.publication]] URL (without the filename).\n\n## RDF Bases\n\nAn RDF BASE declaration is a directive within an RDF document that establishes a document's base IRI, defining a default location to which all relative IRIs within that document will be resolved.\n\nIf the base isn't provided, applications are supposed to fall back to the IRI used to retrieve the document. (See RFC 3986 Section 5.1.3: \"[Base URI from the Retrieval URI](https://datatracker.ietf.org/doc/html/rfc3986#section-5.1.3)\").\n\nTo keep meshes transposable, Semantic Flow relies on this implicit basing to give meshes [[principle.transposability.host]].\n","n":0.102}}},{"i":161,"$":{"0":{"v":"Immutability","n":1},"1":{"v":"\nImmutable data provides fundamental guarantees that enable reliable, distributed, and concurrent systems. But immutability clashes with real-world needs like privacy and security. That's why Semantic Flow embraces [[principle.pseudo-immutability]].\n\n\n\n- [[mesh-resource.node-component.flow-snapshot.version]] (e.g. in [[folder._vN]]) should be usually be treated as immutable. \n  - Therefore, if you need to refer to a flow \"as is\", you should refer to its corresponding snapshot version.\n  - TODO: examples\n- sometimes, e.g., for compliance reasons, you have to modify or hard-delete some data. \n\n\n## References\n\nhttps://s11.no/2013/prov/resources-that-change-state/\n","n":0.113}}},{"i":162,"$":{"0":{"v":"identifier","n":1},"1":{"v":"\nSemantic Flow resources can be identified via two types of identifiers:\n\n- [[concept.identifier.external]]\n- [[concept.identifier.intramesh]]\n\n\n## Guidance\n\n- Prefer relative or site-root-absolute paths inside the mesh; do not hardcode full base IRIs so the mesh remains portable across hosting locations (see [[faq.reference-iri-choices]]).\n\n\n## Identifier Senses\n\n### Content Identifiers\n\nIdentifiers that [[denote|concept.denotation]] **concrete information resources** (files on disk or over HTTP):\n\n* **Distributions** â†’ materialized datasets, e.g. `test.ttl`, `dave_v1.jsonld`, etc.\n* **Resource pages** â†’ e.g. `index.html`\n* **Other documentation resources** â†’ e.g. `README.md`, `CHANGELOG.md`\n\nThese are *retrievable representations* (materialized content), i.e. when dereferenced with a request to a [[concept.semantic-flow-site]], the content itself is returned.\n\n### Concept Identifiers\n\nIdentifiers that refer to **concepts, entities, or abstract things**, including:\n\n* **bare node identifiers** â†’ Organizational containers\n* **reference node identifiers** â†’ denotational \n* **payload node identifiers** â†’ Concepts that are datasets\n* **Abstract flow identifiers** â†’ Dataset-as-persistent-concept\n* **Concrete dataset identifiers** â†’ Specific dataset snapshots\n* **Handle identifiers** â†’ Mesh node themselves\n\nWhen dereferenced with a request to a [[concept.semantic-flow-site]], concept identifiers return content, but they still [[concept.denotation]] a concept.\n\n\n## Identifier Pattern Semantics\n\n| Identifier Type    | Trailing Slash? | Refers toâ€¦                    | Example                                 |\n| ------------------ | --------------- | ----------------------------- | --------------------------------------- |\n| Content identifier | No              | A fetchable document or asset | `https://example.org/ns/foo/index.html` |\n| Concept IRI        | Yes (`/`)       | A real-world or mesh concept  | `https://example.org/ns/foo/`           |\n\nEven though you might be tempted to think of a datasets as concrete things, the IRIs for payload nodes, abstract datasets, and concrete datasets all refer to concepts, i.e., **non-retrievable entities**. Only Distribution IRIs refer to downloadable data, i.e., dataset distributions.\n\n\n","n":0.064}}},{"i":163,"$":{"0":{"v":"intramesh identifier","n":0.707},"1":{"v":"\nAn **intramesh identifier** is a esentially a relative [[concept.iri]] (i.e., without the scheme, e.g., https:// or file://) except that they should correspond to an existing [[mesh-resource]], i.e. [[principle.dereferencability-for-humans]]\n\nThere are two types of intramesh identifiers: [[concept.identifier.intramesh.relative]] and [[concept.identifier.intramesh.absolute]]\n\n## Syntax\n\nLike IRIs:\n\n  * Written without a scheme (e.g., no `https://` or `file://`).\n  * May use path segments (`../`, `/foo/bar`) \n  * May use fragment identifiers (`#`)\n\nUnlike IRIs:\n\n  * **Must not** contain queries (`?`).\n  * \n\n## Semantics\n\n- Same as IRIs in RDF: they [[denote|concept.denotation]] things (aka [[concept.referent]]) which may be mesh resources or â€œthings in the worldâ€.\n  - Fragments behave the same as in IRIs: they refine the primary denotation.\n\n## Purpose\n\n- locate [[mesh-resource]]s (when used in a filesystem or web site context)\n- denote a [[concept.referent]], either [[internal|facet.internal]] or [[external|facet.external]]\n\n## Identifier Name Limitations for Users\n\n- initial underscores prefix all [[facet.system]] identifiers and should be avoided in general for [[facet.user]] identifiers\n\n## Distribution Relativity\n\n\n\n### Examples\n\nNode self-reference: `\"../../../my-dataset\"`  \nOther flows: `\"../../_config-operational-flow/_current/config.trig\"`, `\"../../_config-inheritable-flow/_current/config.trig\"`, `\"../../_dataset-flow/_current/data.jsonld\"`  \nComponents in other flows: Same pattern, just different flow names\n","n":0.078}}},{"i":164,"$":{"0":{"v":"relative identifier","n":0.707},"1":{"v":"\nIntramesh relative identifiers (or **relative identifiers** for short) are relative IRIs that do not start with a slash (`/`), i.e., they are **relative-path relative IRIs** that correspond to a [[concept.mesh-location]] and denote an [[internal|facet.internal]] or [[external|facet.external]] referent..\n\nRelative identifiers are always resolved relative to the distribution file that contains them.\n\nSee [[faq.reference-iri-choices]] for discussion and examples.\n","n":0.136}}},{"i":165,"$":{"0":{"v":"absolute","n":1}}},{"i":166,"$":{"0":{"v":"external identifier","n":0.707},"1":{"v":"\n**External identifiers** are [[concept.iri]]\n","n":0.5}}},{"i":167,"$":{"0":{"v":"Hosting","n":1}}},{"i":168,"$":{"0":{"v":"denotation","n":1},"1":{"v":"\nDenotation is what an [[concept.identifier]] stands for â€”- the resource it names in your data and reasoning.\n\n## Examples\n\n- <ns/djradon/> might denote the person known as \"dj radon\" (a \"concrete\" referent [[concept.referent]])\n- <ns/djradon/playlists/1996-11-10/> might denote a dataset which specifies the tracks played on a radio show (a more \"abstract\" referent)\n","n":0.143}}},{"i":169,"$":{"0":{"v":"Mesh CRUD","n":0.707},"1":{"v":"\n## Operational Modalities\n\n**A. Manual Manipulation**\n- Pre-built node folder structures with user-editable flows and other components\n- Manual mesh resource creation (nodes; flows, snapshots, distributions and other components)\n- File-system based editing workflows\n- Validation of hand-crafted mesh structures\n\n**B. API-Driven Node Manipulation**\n- Flow-service API endpoints for programmatic node creation\n- Support for root node initialization\n- Flow and other component management via API\n- RESTful mesh resource manipulation\n\n**C. Dataset Distribution Upload + Extraction**\n- Upload mechanisms for RDF dataset distributions (.trig, .jsonld, etc.)\n- Automatic named entity extraction from semantic data\n- System-generated reference and dataset nodes\n- Batch processing of semantic data\n- **Limitation**: Cannot handle binary file resources (audio, images, etc.) - only RDF data\n- File resources must be handled via Direct Manual Construction or API-Driven modalities\n","n":0.092}}}]}
